# Dask Array å¯¦æˆ°ï¼šå¾æ°£è±¡è³‡æ–™åˆ° ML Pipeline

# Dask Array å¯¦æˆ°ï¼šå¾æ°£è±¡è³‡æ–™åˆ° ML Pipeline

---

## èª²ç¨‹ç°¡ä»‹

<aside>
ğŸ“¢

### æœ¬å·¥ä½œåŠæ˜¯ã€ŒPython å¤§æ•¸æ“šè™•ç†ä¸‰éƒ¨æ›²ã€çš„ç¬¬äºŒéƒ¨ï¼šN-D Array è™•ç†ç¯‡

ç•¶ä½ çš„æ°£è±¡è³‡æ–™å¾ GB æˆé•·åˆ° TB ç´šåˆ¥ï¼Œå‚³çµ±çš„ NetCDF + Xarray å–®æ©Ÿè™•ç†æ–¹å¼å°‡æœƒé­é‡ç“¶é ¸ã€‚æœ¬èª²ç¨‹å°‡å¸¶ä½ ä½¿ç”¨ Zarr + Dask + Xarray çš„ç¾ä»£åŒ–å·¥ä½œæµç¨‹ï¼Œå¯¦ç¾çœŸæ­£çš„ out-of-core åˆ†æï¼Œä¸¦å°‡è³‡æ–™ç„¡ç¸«æ¥å…¥ PyTorch é€²è¡Œæ©Ÿå™¨å­¸ç¿’ã€‚

</aside>

---

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬èª²ç¨‹å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

1. **åˆ†æè¶…éè¨˜æ†¶é«”å®¹é‡çš„è³‡æ–™**ï¼šä½¿ç”¨ Xarray + Dask è™•ç† TB ç´šæ°£è±¡è³‡æ–™è€Œä¸æœƒ OOM
2. **å„ªåŒ–å„²å­˜æ ¼å¼**ï¼šç†è§£ Zarr çš„å„ªå‹¢ä¸¦å°‡ NetCDF è½‰æ›æˆå„ªåŒ–çš„ Zarr store
3. **å»ºç«‹ ML è³‡æ–™ Pipeline**ï¼šä½¿ç”¨ xbatcher å°‡å¤§å‹ç§‘å­¸è³‡æ–™ç„¡ç¸«æ¥å…¥ PyTorch
4. **ç§‘å­¸åŒ–é©—è­‰**ï¼šä½¿ç”¨ xskillscore é€²è¡Œä¿ç•™ç©ºé–“è³‡è¨Šçš„æ¨¡å‹é©—è­‰

---

## å…ˆå‚™çŸ¥è­˜

æœ¬èª²ç¨‹å‡è¨­ä½ å·²ç¶“å…·å‚™ï¼š

- âœ… Python åŸºç¤èªæ³•
- âœ… NumPy åŸºæœ¬æ“ä½œ
- âœ… ç†è§£ Dask æ ¸å¿ƒæ¦‚å¿µï¼ˆLazy Evaluation, Task Graphsï¼‰
- âœ… ç†è§£ GIL, Processes vs.Â Threadsï¼ˆå·²å®Œæˆ Part 1 èª²ç¨‹ï¼‰

---

## èª²ç¨‹æ¶æ§‹

```
Part 0: ç’°å¢ƒè¨­å®š
â”œâ”€â”€ uv å°ˆæ¡ˆç®¡ç†
â”œâ”€â”€ Jupyter Kernel è¨­å®š
â””â”€â”€ Dask Dashboard å•Ÿå‹•

Part 1: Zarr èˆ‡ Xarray åŸºç¤
â”œâ”€â”€ ç‚ºä»€éº¼éœ€è¦ Zarrï¼Ÿ
â”œâ”€â”€ Zarr vs NetCDF å°ç…§
â””â”€â”€ å¯¦ä½œï¼šè®€å–èˆ‡æ¢ç´¢
		â””â”€â”€ æ™‚ç©ºåˆ‡ç‰‡

Part 2: æ™‚ç©ºè³‡æ–™è™•ç†
â”œâ”€â”€ å¤šæª”æ¡ˆè®€å–
â”œâ”€â”€ é‡æ¡æ¨£&é‡æŠ•å½±
â”œâ”€â”€ è¨ˆç®—èˆ‡å„²å­˜å„ªåŒ– 
â””â”€â”€ rechunker å­˜æˆ zarr

Part 3: ML Pipeline 
â”œâ”€â”€ ä»»å‹™å®šç¾©ï¼šå°æµåˆ†é¡
â”œâ”€â”€ xbatcher æ‰¹æ¬¡åˆ‡å‰²
â”œâ”€â”€ PyTorch DataLoader æ©‹æ¥
â”œâ”€â”€ è¨“ç·´æµç¨‹ç¤ºç¯„
â”œâ”€â”€ é æ¸¬çµæœ â†’ Xarray
â””â”€â”€ xskillscore ç©ºé–“é©—è­‰
```

---

# Part 0: ç’°å¢ƒè¨­å®š

## 0.1 ä½¿ç”¨ uv å»ºç«‹å°ˆæ¡ˆç’°å¢ƒ

> ğŸ’¡ ç‚ºä»€éº¼ç”¨ uvï¼Ÿ
> 
> 
> `uv` æ˜¯æ–°ä¸€ä»£çš„ Python å¥—ä»¶ç®¡ç†å·¥å…·ï¼Œæ¯”å‚³çµ±çš„ pip + virtualenv å¿« 10-100 å€ï¼Œä¸”èƒ½è‡ªå‹•è™•ç†ç›¸ä¾æ€§è¡çªã€‚
> 

### å®‰è£ uv

```bash
# å¦‚æœé‚„æ²’å®‰è£ 
uvcurl -LsSf https://astral.sh/uv/install.sh | sh # é©—è­‰å®‰è£uv --version
```

### åˆå§‹åŒ–å°ˆæ¡ˆ

```bash
git clone https://github.com/Isongzhe/python-parallel-data-processing.git
cd python-parallel-data-processing
```

### å®‰è£ç›¸ä¾å¥—ä»¶ & æª¢æŸ¥ç’°

```bash
uv sync 
uv run python -c "import xarray, dask, zarr; print('Environment OK!')"
```

## 0.2 VSCode Jupyter Kernel è¨­å®š

### æ­¥é©Ÿ 1: å»ºç«‹ Jupyter Kernel

```bash
# ä½¿ç”¨ uv å»ºç«‹ kernel
uv add --dev ipykernel
uv run python -m ipykernel install --user --name dask-workshop --display-name "python-parallel-data-processing"
```

### æ­¥é©Ÿ 2: åœ¨ VSCode é¸æ“‡ Kernel

1. æ‰“é–‹ VSCode
2. å»ºç«‹æ–°çš„ `.ipynb` æª”æ¡ˆ
3. é»æ“Šå³ä¸Šè§’ã€Œé¸æ“‡æ ¸å¿ƒã€
4. é¸æ“‡ `python-parallel-data-processing`

### æ­¥é©Ÿ 3: æ¸¬è©¦é€£ç·š

åœ¨ Notebook ä¸­åŸ·è¡Œï¼š

```python
# æ¸¬è©¦ Cellimport xarray as xr
import dask
import zarr
print(f"Xarray version: {xr.__version__}")
print(f"Dask version: {dask.__version__}")
print(f"Zarr version: {zarr.__version__}")
# ç¢ºèª Zarr < 3.0assert int(zarr.__version__.split('.')[0]) < 3, "è«‹ç¢ºèª Zarr ç‰ˆæœ¬ < 3.0"print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆï¼")
```

## 0.3 å•Ÿå‹• Dask Dashboard

> ğŸ’¡ Dask Dashboard æ˜¯ä»€éº¼ï¼Ÿ
> 
> 
> Dashboard æ˜¯ä¸€å€‹ç¶²é ä»‹é¢ï¼Œå¯ä»¥å³æ™‚è§€å¯Ÿï¼š
> - Task Graphï¼ˆä»»å‹™ä¾è³´é—œä¿‚ï¼‰
> - Task Streamï¼ˆä»»å‹™åŸ·è¡Œæ™‚é–“ç·šï¼‰
> - Memory Usageï¼ˆè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼‰
> - Workers Statusï¼ˆå·¥ä½œç¨‹åºç‹€æ…‹ï¼‰
> 

### å•Ÿå‹• Client

åœ¨ Jupyter Notebook ä¸­åŸ·è¡Œï¼š

```python
from dask.distributed import Client
# å•Ÿå‹•æœ¬åœ° Clusterclient = Client()
# é¡¯ç¤º Dashboard é€£çµprint(client)
print(f"Dashboard: {client.dashboard_link}")
```

ä½ æœƒçœ‹åˆ°é¡ä¼¼çš„è¼¸å‡ºï¼š

```
<Client: 'tcp://127.0.0.1:xxxxx' processes=8 threads=16>
Dashboard: http://127.0.0.1:8787/status
```

### æŸ¥çœ‹ Dashboard

1. **æœ¬æ©Ÿç’°å¢ƒ**ï¼šç›´æ¥é–‹å•Ÿç€è¦½å™¨è¨ªå• `http://127.0.0.1:8787/status`
2. **é ç«¯ Serverï¼ˆSSHï¼‰**ï¼šéœ€è¦è¨­å®š Port Forwarding
    
    ```bash
    # åœ¨æœ¬æ©ŸåŸ·è¡Œssh -L 8787:localhost:8787 user@remote-server
    # ç„¶å¾Œåœ¨æœ¬æ©Ÿç€è¦½å™¨è¨ªå•# http://localhost:8787/status
    ```
    
3. **VSCode Remote SSH**ï¼šVSCode æœƒè‡ªå‹•è½‰ç™¼ portï¼Œç›´æ¥é»æ“Šé€£çµå³å¯

### Dashboard é‡è¦é é¢

- **Status**: ç¸½è¦½
- **Task Stream**: å³æ™‚ä»»å‹™åŸ·è¡Œï¼ˆæœ€å¸¸ç”¨ï¼‰
- **Progress**: é€²åº¦æ¢
- **Graph**: ä»»å‹™ç›¸ä¾åœ–
- **System**: CPU/Memory ä½¿ç”¨ç‡

# Part 1: Zarr èˆ‡ Xarray åŸºç¤

<aside>
ğŸ“–

**å°ç…§**: /w1/notebooks/01-data-loading-basics.ipynb 

</aside>

## 1.1 ç‚ºä»€éº¼éœ€è¦ Zarrï¼Ÿ

### å‚³çµ± NetCDF çš„ç—›é»

[Cloud-Optimized HDF/NetCDF â€“ Cloud-Optimized Geospatial Formats Guide](https://guide.cloudnativegeo.org/cloud-optimized-netcdf4-hdf5/)

<aside>
ğŸ“–

NetCDF-4 æ˜¯ä¸€ç¨®æª”æ¡ˆæ ¼å¼ï¼Œå®ƒ**ä½¿ç”¨ HDF5 ä½œç‚ºåº•å±¤çš„å„²å­˜æ ¼å¼**ã€‚

</aside>

âŒ å¤šæª”æ¡ˆ**è®€å–é€Ÿåº¦æ…¢**

- **å…ƒè³‡æ–™æ•£ä½ˆ**ï¼šå…ƒè³‡æ–™å¯èƒ½æ•£å¸ƒåœ¨æª”æ¡ˆçš„å„å€‹å€å¡Šï¼Œéœ€è¦é€²è¡Œå¤šæ¬¡ I/O è®€å–æ‰èƒ½æ‹¼æ¹Šå‡ºå®Œæ•´çš„æª”æ¡ˆçµæ§‹åœ–ã€‚ç•¶ä½ å˜—è©¦ç”¨Â `xarray.open_mfdataset()`Â ä¸€æ¬¡é–‹å•Ÿæ•¸åƒå€‹ NetCDF æª”æ¡ˆæ™‚ï¼Œç¨‹å¼æœƒé€ä¸€è®€å–æ¯å€‹æª”æ¡ˆï¼Œå°‹æ‰¾ä¸¦è§£æé€™äº›æ•£å¸ƒçš„å…ƒè³‡æ–™ï¼Œå°è‡´æ¥µå¤§çš„å»¶é²ã€‚
- **å…ƒè³‡æ–™é–**ï¼šHDF5 ç‚ºäº†ç¢ºä¿æª”æ¡ˆçš„ä¸€è‡´æ€§ï¼Œåœ¨é€²è¡Œä¿®æ”¹æ™‚æœƒå•Ÿå‹•é–å®šæ©Ÿåˆ¶ã€‚é€™ç¨®é–å®šå°æ–¼ä¸¦è¡Œå¯«å…¥éå¸¸ä¸åˆ©ã€‚åœ¨Â `open_mfdataset`Â é€™ç¨®å¤šæª”ä¸²æ¥æƒ…å¢ƒä¸­ï¼Œå³ä½¿æ˜¯è®€å–ï¼Œå¾ŒçºŒçš„è¨ˆç®—ä¹Ÿå¯èƒ½å› åº•å±¤çš„ HDF5 æª”æ¡ˆå­˜å–è€Œç›¸äº’ç­‰å¾…ã€‚

```python
# è™•ç†å¤šæª”æ¡ˆè¶…ç´šä¹…
ds = xr.open_mfdataset('*.nc')  # æƒææ‰€æœ‰ metadata

# NetCDF æœ‰ metadata lockï¼Œç„¡æ³•çœŸæ­£å¹³è¡Œ
# å¤šå€‹ process åŒæ™‚è®€å– â†’ äº’ç›¸ç­‰å¾…
```

âŒ **é›²ç«¯ä¸å‹å–„ (ex. Himawri** https://noaa-himawari9.s3.amazonaws.com/index.html#AHI-L2-FLDK-Clouds/2025/10/10/0000/)

- æ€§**éé›²ç«¯åŸç”Ÿ**ï¼šé€™ç¨®è¨­è¨ˆå°æ–¼å‚³çµ±çš„æª”æ¡ˆç³»çµ±ï¼ˆæœ¬åœ°ç¡¬ç¢Ÿï¼‰å¾ˆæœ‰æ•ˆï¼Œä½†åœ¨é›²ç«¯å„²å­˜ï¼ˆå¦‚ S3ï¼‰ä¸­å»æ•ˆç‡ä½ä¸‹ã€‚åœ¨é›²ç«¯ï¼Œæ¯æ¬¡è®€å–éƒ½éœ€è¦ç™¼å‡º HTTP Range Requestã€‚å¦‚æœå…ƒè³‡æ–™æ•£å¸ƒåœ¨æª”æ¡ˆä¸­ï¼Œè®€å–å…ƒè³‡æ–™å°±å¿…é ˆç™¼å‡ºå¤šæ¬¡è«‹æ±‚ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ä¸‹è¼‰ã€‚

```python
# å¦‚æœè³‡æ–™åœ¨ S3/GCS ä¸Š...

# NetCDF éœ€è¦ä¸‹è¼‰æ•´å€‹æª”æ¡ˆæ‰èƒ½è®€å–
# Zarr 
xr.open_zarr('gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr')
```

---

### Zarr çš„è§£æ±ºæ–¹æ¡ˆ

**ç›®éŒ„çµæ§‹ï¼Œæ¯å€‹ chunk æ˜¯ç¨ç«‹æª”æ¡ˆ**

```
era5.zarr/
â”œâ”€â”€ .zattrs              # å…¨åŸŸå±¬æ€§
â”œâ”€â”€ .zgroup              # ç¾¤çµ„è³‡è¨Š
â”œâ”€â”€ temperature/
â”‚   â”œâ”€â”€ .zarray          # é™£åˆ— metadata
â”‚   â”œâ”€â”€ .zattrs          # è®Šæ•¸å±¬æ€§
â”‚   â”œâ”€â”€ 0.0.0            # chunk æª”æ¡ˆ
â”‚   â”œâ”€â”€ 0.0.1
â”‚   â”œâ”€â”€ 0.1.0
â”‚   â””â”€â”€ ...
â””â”€â”€ precipitation/
    â””â”€â”€ ...
```

**çœŸæ­£çš„å¹³è¡Œè®€å¯«**

- æ²’æœ‰ metadata lock
- æ¯å€‹ worker è®€å–ä¸åŒçš„ chunk æª”æ¡ˆ
- å¯ä»¥åŒæ™‚å¯«å…¥ä¸åŒçš„ chunk

**é›²ç«¯åŸç”Ÿè¨­è¨ˆ**

- åªä¸‹è¼‰éœ€è¦çš„ chunk
- æ”¯æ´ S3, GCS, Azure Blob
- HTTP Range Request å‹å–„

<aside>
ğŸ“–

ç¾åœ¨å¤šå€‹æ©Ÿæ§‹æœƒæŠŠ TB ç´šçš„è³‡æ–™æ”¾åˆ° GCP / AWS s3 å†æä¾›å‡ºå»ï¼Œä¾‹å¦‚ï¼š[WeatherBench ERA5](https://weatherbench2.readthedocs.io/en/latest/data-guide.html#era5) 

</aside>

## 1.2 Zarr vs NetCDF å®Œæ•´å°ç…§

| ç‰¹æ€§ | NetCDF (HDF5) | Zarr | èªªæ˜ |
| --- | --- | --- | --- |
| **æª”æ¡ˆçµæ§‹** | å–®ä¸€æª”æ¡ˆ | ç›®éŒ„ + å¤šå€‹æª”æ¡ˆ | Zarr æ¯å€‹ chunk ç¨ç«‹ |
| **Metadata** | é›†ä¸­å¼ï¼ˆæœ‰ lockï¼‰ | åˆ†æ•£å¼ï¼ˆç„¡ lockï¼‰ | Zarr å¯å¹³è¡Œè®€å¯« |
| **å¹³è¡Œè®€å–** | å—é™ | å®Œå…¨æ”¯æ´ | NetCDF æœ‰ç«¶çˆ­å•é¡Œ |
| **å¹³è¡Œå¯«å…¥** | ä¸æ”¯æ´ | å®Œå…¨æ”¯æ´ | Zarr å¯åŒæ™‚å¯«ä¸åŒ chunk |
| **é›²ç«¯å„²å­˜** | éœ€å®Œæ•´ä¸‹è¼‰ | åªä¸‹è¼‰éœ€è¦çš„ chunk | Zarr ç¯€çœé »å¯¬ |
| **å£“ç¸®é¸é …** | æœ‰é™ï¼ˆzlib, gzipï¼‰ | è±å¯Œï¼ˆBlosc, Zstd, LZ4â€¦ï¼‰ | Zarr å£“ç¸®æ›´å¿«æ›´å¥½ |
| **è¿½åŠ è³‡æ–™** | å›°é›£ | å®¹æ˜“ | Zarr ç›´æ¥æ–°å¢ chunk |
| **ç”Ÿæ…‹ç³»æ”¯æ´** | æˆç†Ÿï¼ˆå¹¾åå¹´ï¼‰ | æ–°èˆˆï¼ˆå¿«é€Ÿæˆé•·ï¼‰ | NetCDF ä»æ˜¯ä¸»æµæ ¼å¼ |

[A Comparison of HDF5, Zarr, and netCDF4 in Performing Common I/O Operations](https://arxiv.org/abs/2207.09503)

<aside>
âš ï¸

### ç‚ºä»€éº¼å¿…é ˆç”¨ Zarr < 3.0ï¼Ÿ

**Zarr 2.xï¼ˆæ¨è–¦ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼‰**

```
era5.zarr/
â”œâ”€â”€ .zattrs          # JSON æ ¼å¼
â”œâ”€â”€ .zgroup
â”œâ”€â”€ temperature/
â”‚   â”œâ”€â”€ .zarray      # é™£åˆ— metadata
â”‚   â””â”€â”€ 0.0.0        # chunk å‘½åï¼šç¶­åº¦ç´¢å¼•
```

**Zarr 3.xï¼ˆ2024ï¼Œä»åœ¨ç©©å®šä¸­)** 

```
era5.zarr/
â”œâ”€â”€ zarr.json        # æ–°çš„ metadata æ ¼å¼
â”œâ”€â”€ temperature/
â”‚   â”œâ”€â”€ zarr.json    # çµ±ä¸€æ ¼å¼
â”‚   â””â”€â”€ c/0/0/0      # æ–°çš„ chunk å‘½åï¼šc/ å‰ç¶´
```

**ä¸»è¦è®Šæ›´**

| é …ç›® | Zarr 2.x | Zarr 3.x |
| --- | --- | --- |
| Metadata æ ¼å¼ | `.zarray`, `.zattrs` | çµ±ä¸€çš„ `zarr.json` |
| Chunk å‘½å | `0.1.2` | `c/0/1/2` |
| Storage API | `store[key]` | æ–°çš„ abstract API |
| å£“ç¸®å™¨ | `numcodecs` | å¯æ’æ‹”çš„ codec pipeline |
</aside>

---

## 1.3 Part1 - å¯¦ä½œï¼šè®€å–èˆ‡æ¢ç´¢ ERA5 è³‡æ–™

<aside>

**ä¾†æº**ï¼šERA5 Reanalysis
**ç©ºé–“ç¯„åœ**ï¼š10Â°N-40Â°N, 100Â°E-140Â°Eï¼ˆæ±äº-è¥¿å¤ªå¹³æ´‹ï¼‰
**æ™‚é–“ç¯„åœ**ï¼š2019-2023ï¼ˆ5 å¹´ï¼‰
**è®Šæ•¸**ï¼šæº«åº¦ã€æ¿•åº¦ã€é¢¨å ´ã€å°æµåƒæ•¸ã€é™æ°´ç­‰

</aside>

### é‡é» 1: è®€å–è³‡æ–™

[Reading and writing files](https://docs.xarray.dev/en/stable/user-guide/io.html)

<aside>
ğŸ‘‰

å…ˆè¼‰ Engine ä¾†è§£æä½ çš„è³‡æ–™æ ¼å¼ï¼Œä¸ç„¶é–‹ä¸èµ·ä¾† (ex. **zarr / h5netcdf )**

[xarray.open_dataset](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html)

</aside>

```python
import xarray as xr
import dask

# open h5 / netcdf 
ds = xr.open_dataset(..., engine = 'h5netcdf', chunk='auto')

# open zarr with dask 
ds_dask_lazy = xr.open_zarr(...)
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š

```
<xarray.Dataset>
Dimensions:  (time: 8760, latitude: 121, longitude: 161, level: 13)
Coordinates:
  * time       (time) datetime64[ns] 2019-01-01 ... 2019-12-31T23:00:00
  * latitude   (latitude) float32 40.0 39.75 39.5 ... 10.5 10.25 10.0
  * longitude  (longitude) float32 100.0 100.25 100.5 ... 139.5 139.75 140.0
  * level      (level) int32 1000 975 950 925 ... 500 400 300 200
  
Data variables:
    temperature  (time, level, latitude, longitude) float32 dask.array<...>
    specific_humidity  (time, level, latitude, longitude) float32 dask.array<...>
    total_precipitation  (time, latitude, longitude) float32 dask.array<...>
    ...
```

**é‡é»è§€å¯Ÿ**

1. **`dask.array<...>`**ï¼šé€™è¡¨ç¤ºè³‡æ–™é‚„æ²’æœ‰çœŸæ­£è®€å…¥è¨˜æ†¶é«”ï¼ˆlazyï¼‰
2. **Dimensions**ï¼š4 å€‹ç¶­åº¦ï¼ˆæ™‚é–“ã€ç·¯åº¦ã€ç¶“åº¦ã€æ°£å£“å±¤ï¼‰
3. **Coordinates**ï¼šæ¯å€‹ç¶­åº¦éƒ½æœ‰åº§æ¨™å€¼

<aside>
ğŸ‘‰

æˆ‘éƒ½æœƒç›¡å¯èƒ½ä½¿ç”¨ `intake-xarray` ä¾†è®€å–æª”æ¡ˆ: 

- é›†ä¸­ç®¡ç†ï¼šæ‰€æœ‰è³‡æ–™ä¾†æºå®šç¾©åœ¨ `catalog.yaml`
- æè¿°æ€§ï¼šæ¯å€‹è³‡æ–™é›†æœ‰ descriptionï¼Œæ–¹ä¾¿åœ˜éšŠå”ä½œ
- å¯æ”œæ€§ï¼šæ›ç’°å¢ƒåªéœ€ä¿®æ”¹ catalogï¼Œcode ä¸ç”¨å‹•

```markdown
# è·¯å¾‘å¯«æ­»
ds_2019 = xr.open_zarr('.../era5_2019_10N40N_100E140E.zarr')

# intake 
catalog = intake.open_catalog('catalog.yaml')
ds = catalog.era5_2019_raw.to_dask() # catalog.dataset_name 
```

</aside>

---

### é‡é» 2: Chunking çš„é‡è¦æ€§ !!!

<aside>

è‰¯å¥½çš„ chunkingï¼š
âœ… æ¯å€‹ chunk å¤§å°é©ä¸­ï¼ˆ10-100 MBï¼‰
âœ… ç¬¦åˆä½ çš„è®€å–æ¨¡å¼ï¼ˆæ™‚é–“åºåˆ—ï¼Ÿç©ºé–“åˆ‡ç‰‡ï¼Ÿï¼‰
âœ… å¹³è¡¡ä»»å‹™æ•¸é‡èˆ‡å‚³è¼¸é–‹éŠ·

ä¸è‰¯çš„ chunkingï¼š
âŒ å¤ªå°ï¼šç”¢ç”Ÿéå¤šä»»å‹™ï¼Œèª¿åº¦é–‹éŠ·å¤§
âŒ å¤ªå¤§ï¼šè¨˜æ†¶é«”å£“åŠ›å¤§ï¼Œå¹³è¡Œåº¦ä½
âŒ ä¸ç¬¦åˆè®€å–æ¨¡å¼ï¼šéœ€è¦è®€å–å¤§é‡ç„¡ç”¨è³‡æ–™

</aside>

```python
# æŸ¥çœ‹ chunks
print(ds.chunks)
# æŸ¥çœ‹å–®ä¸€è®Šæ•¸çš„ chunks
print(ds['temperature'].chunks)

**è¼¸å‡ºç¯„ä¾‹**ï¼š
Frozen({'time': (24, 24, 24, ...),
        'latitude': (50, 50, 21),
        'longitude': (50, 50, 50, 11),
        'level': (13,)})
        
é€™è¡¨ç¤ºï¼š
- æ™‚é–“ç¶­åº¦ï¼šæ¯ 24 å°æ™‚ä¸€å€‹ chunkï¼ˆä¸€å¤©ï¼‰
- ç·¯åº¦ï¼šæ¯ 50 å€‹æ ¼é»ä¸€å€‹ chunk
- ç¶“åº¦ï¼šæ¯ 50 å€‹æ ¼é»ä¸€å€‹ chunk
- æ°£å£“å±¤ï¼šå…¨éƒ¨åœ¨ä¸€èµ·ï¼ˆ13 å±¤ï¼‰
```

<aside>
ğŸ“–

è©²å¦‚ä½•é¸æ“‡ good chunk sizes å¯ä»¥åƒè€ƒé€™å…©ç¯‡æ–‡ç« ï¼š

https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes

https://docs.dask.org/en/latest/array-chunks.html

</aside>

### é‡é» 3: Xarray Data Structures

[Xarray in 45 minutes](https://tutorial.xarray.dev/overview/xarray-in-45-min.html)

<aside>
ğŸ“–

Xarray provides two main data structures:

1. [**`DataArrays`**](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#dataarray)Â that wrap underlying data containers (e.g. numpy arrays) and contain associated metadata
2. [**`Datasets`**](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#dataset)Â that are dictionary-like containers of DataArrays

DataArrays contain underlying arrays and associated metadata:

1. Name
2. Dimension names
3. Coordinate variables
4. and arbitrary attributes.
</aside>

---

# Part2 - å¯¦ä½œ: æ™‚ç©ºè³‡æ–™è™•ç†

## 2.1 å¤šæª”æ¡ˆè®€å–èˆ‡åˆä½µ

### æƒ…å¢ƒï¼šè®€å–å¤šå¹´è³‡æ–™

æˆ‘å€‘æœ‰ 2019-2023 å…± 5 å¹´çš„è³‡æ–™ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è®€å–ï¼Ÿ

```python
import glob
import xarray as xr
# æ–¹æ³• 1: ä½¿ç”¨ glob pattern
zarr_files = sorted(glob.glob('/home/sungche/NAS/dataset/era5/era5_*.zarr'))
print(f"æ‰¾åˆ° {len(zarr_files)} å€‹æª”æ¡ˆ")

# æ–¹æ³• 2: ä½¿ç”¨ xr.open_mfdatasetï¼ˆæ¨è–¦ï¼‰
ds_multi = xr.open_mfdataset(
    zarr_files,
    engine='zarr',
    parallel=True,  # å¹³è¡Œè®€å– metadata    
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}  # çµ±ä¸€ chunking
)
print(f"åˆä½µå¾Œçš„æ™‚é–“ç¯„åœï¼š{ds_multi.time[0].values} åˆ° {ds_multi.time[-1].values}")
print(f"ç¸½è³‡æ–™é»æ•¸ï¼š{len(ds_multi.time)}")
```

---

### é‡è¦åƒæ•¸èªªæ˜

### `parallel=True`

```python
# parallel=Trueï¼šå¹³è¡Œè®€å–å„æª”æ¡ˆçš„ metadata # é€Ÿåº¦å¿«ï¼Œä½†è¦ç¢ºä¿æª”æ¡ˆçµæ§‹ä¸€è‡´
# parallel=Falseï¼šåºåˆ—è®€å–ï¼ˆé è¨­ï¼‰# è¼ƒæ…¢ï¼Œä½†æ›´å®‰å…¨
```

### `chunks` åƒæ•¸

```python
# æƒ…æ³ 1: è®“ Xarray è‡ªå‹•æ±ºå®šï¼ˆä½¿ç”¨æª”æ¡ˆåŸæœ¬çš„ chunksï¼‰
ds = xr.open_mfdataset(files, engine='zarr', chunks='auto')

# æƒ…æ³ 2: æ˜ç¢ºæŒ‡å®šï¼ˆæ¨è–¦ï¼Œç¢ºä¿ä¸€è‡´æ€§ï¼‰
ds = xr.open_mfdataset(
    files,
    engine='zarr',
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}
)
# æƒ…æ³ 3: è®€å…¥è¨˜æ†¶é«”ï¼ˆå°æª”æ¡ˆï¼‰
ds = xr.open_mfdataset(files, engine='zarr', chunks=None)
```

---

### å„²å­˜æˆ Zarr

### åŸºæœ¬å„²å­˜

```python
# å„²å­˜è·å¹³è³‡æ–™
output_path = './taiwan_summer_anomaly.zarr'
anomaly.to_zarr(
    output_path,
    mode='w',              # 'w' = è¦†å¯«, 'a' = è¿½åŠ     consolidated=True      # åˆä½µ metadataï¼ˆé‡è¦ï¼åŠ é€Ÿè®€å–ï¼‰)
print(f"âœ… å·²å„²å­˜åˆ°ï¼š{output_path}")
```

### é€²éšï¼šå£“ç¸®è¨­å®š

```python
import zarr
# è¨­å®šå£“ç¸®ï¼ˆæ¨è–¦ï¼šBlosc + Zstdï¼‰
encoding = {}
for var in anomaly.data_vars:
    encoding[var] = {
        'compressor': zarr.Blosc(
            cname='zstd',    # å£“ç¸®æ¼”ç®—æ³•ï¼šzstdï¼ˆå¹³è¡¡é€Ÿåº¦èˆ‡å£“ç¸®ç‡ï¼‰            
            clevel=3,        # å£“ç¸®ç­‰ç´šï¼š1-9ï¼ˆ3 æ˜¯å¥½çš„å¹³è¡¡é»ï¼‰            
            shuffle=2        # Bit-shuffleï¼ˆå°æµ®é»æ•¸æœ‰æ•ˆï¼‰        
        )
    }
# å„²å­˜
anomaly.to_zarr(
    output_path,
    mode='w',
    consolidated=True,
    encoding=encoding
)
# æ¯”è¼ƒå¤§å°import os
import subprocess
# åŸå§‹å¤§å°ï¼ˆæœªå£“ç¸®ï¼‰raw_size = anomaly.nbytes / 1e9print(f"åŸå§‹å¤§å°ï¼š{raw_size:.2f} GB")
# å£“ç¸®å¾Œå¤§å°compressed_size = float(subprocess.check_output(['du', '-sb', output_path]).split()[0]) / 1e9print(f"å£“ç¸®å¾Œå¤§å°ï¼š{compressed_size:.2f} GB")
print(f"å£“ç¸®ç‡ï¼š{raw_size / compressed_size:.2f}x")
```

### å£“ç¸®æ¼”ç®—æ³•é¸æ“‡

| å£“ç¸®å™¨ | é€Ÿåº¦ | å£“ç¸®ç‡ | é©ç”¨æƒ…å¢ƒ |
| --- | --- | --- | --- |
| **Blosc-Zstd** | å¿« | é«˜ | **æ¨è–¦**ï¼šé€šç”¨ |
| **Blosc-LZ4** | æ¥µå¿« | ä¸­ | éœ€è¦æ¥µå¿«çš„ I/O |
| **Blosc-GZIP** | æ…¢ | é«˜ | é•·æœŸå„²å­˜ã€é »å¯¬æœ‰é™ |
| **ç„¡å£“ç¸®** | æœ€å¿« | ç„¡ | è‡¨æ™‚æª”æ¡ˆ |

---

### é‡æ–°è®€å–é©—è­‰

```python
import time
# æ¸¬è©¦è®€å–é€Ÿåº¦
start = time.time()
ds_reload = xr.open_zarr(output_path)
elapsed = time.time() - start
print(f"âœ… è®€å–å®Œæˆï¼è€—æ™‚ï¼š{elapsed:.4f} ç§’ï¼ˆå¹¾ä¹ç¬é–“ï¼‰")
print(ds_reload)
# é©—è­‰è³‡æ–™æ­£ç¢ºæ€§
assert ds_reload['temperature'].shape == anomaly['temperature'].shape
print("âœ… è³‡æ–™é©—è­‰é€šé")
```

---

## 2.4 Dashboard æ•ˆèƒ½åˆ†æ

### å¯¦é©— 1: ä¸åŒ Chunk Size çš„å½±éŸ¿

```python
import time
# æº–å‚™æ¸¬è©¦è³‡æ–™ï¼ˆé¸ä¸€å€‹å­é›†ï¼‰ds_test = ds_multi.sel(time=slice('2019-01', '2019-12'))
# æ¸¬è©¦ä¸åŒçš„ chunk å¤§å°chunk_configs = [
    {'time': 10, 'latitude': 20, 'longitude': 20},   # å° chunk    {'time': 24, 'latitude': 50, 'longitude': 50},   # ä¸­ç­‰ chunk    {'time': 100, 'latitude': 100, 'longitude': 100} # å¤§ chunk]
results = []
for config in chunk_configs:
    # Rechunk    ds_rechunked = ds_test.chunk(config)
    # åŸ·è¡Œç›¸åŒè¨ˆç®—ï¼šå…¨åŸŸå¹³å‡    start = time.time()
    result = ds_rechunked['temperature'].mean().compute()
    elapsed = time.time() - start
    results.append({
        'config': config,
        'time': elapsed,
        'n_tasks': len(ds_rechunked['temperature'].__dask_graph__())
    })
    print(f"Chunk size {config}: {elapsed:.2f}s, {results[-1]['n_tasks']} tasks")
# è¦–è¦ºåŒ–import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(results)
df['config_str'] = df['config'].astype(str)
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].bar(range(len(df)), df['time'])
axes[0].set_xticks(range(len(df)))
axes[0].set_xticklabels(['Small', 'Medium', 'Large'])
axes[0].set_ylabel('Time (s)')
axes[0].set_title('Computation Time')
axes[1].bar(range(len(df)), df['n_tasks'])
axes[1].set_xticks(range(len(df)))
axes[1].set_xticklabels(['Small', 'Medium', 'Large'])
axes[1].set_ylabel('Number of Tasks')
axes[1].set_title('Task Count')
plt.tight_layout()
plt.savefig('chunk_size_analysis.png', dpi=150)
plt.show()
```

**è§€å¯Ÿé‡é»**ï¼š
- å° chunkï¼šä»»å‹™å¤šï¼Œèª¿åº¦é–‹éŠ·å¤§
- å¤§ chunkï¼šä»»å‹™å°‘ï¼Œä½†æ¯å€‹ä»»å‹™è¨˜æ†¶é«”éœ€æ±‚é«˜
- ä¸­ç­‰ chunkï¼šé€šå¸¸æ˜¯æœ€ä½³å¹³è¡¡

---

### å¯¦é©— 2: è¨˜æ†¶é«”ä½¿ç”¨è§€å¯Ÿ

```python
# æ¸¬è©¦æœƒä¸æœƒ OOM çš„æ“ä½œ# æƒ…æ³ 1: Lazy æ“ä½œï¼ˆä¸æœƒ OOMï¼‰large_result = ds_multi['temperature'].mean(dim='time')
print(f"Lazy result created. Memory usage: {large_result.nbytes / 1e9:.2f} GB (è™›æ“¬)")
# æƒ…æ³ 2: Eager æ“ä½œï¼ˆå¯èƒ½ OOMï¼‰try:
    large_result_eager = large_result.compute()
    print(f"Computed! Memory usage: {large_result_eager.nbytes / 1e9:.2f} GB")
except MemoryError:
    print("âŒ Out of Memory!")
# æƒ…æ³ 3: Persistï¼ˆè¼‰å…¥åˆ°åˆ†æ•£å¼è¨˜æ†¶é«”ï¼‰large_result_persist = large_result.persist()
# é€™æœƒå°‡çµæœåˆ†æ•£å„²å­˜åœ¨ Dask workers çš„è¨˜æ†¶é«”ä¸­print("âœ… Persisted to distributed memory")
```

**Dashboard è§€å¯Ÿ**ï¼š
- Memory é é¢ï¼šå¯ä»¥çœ‹åˆ°æ¯å€‹ worker çš„è¨˜æ†¶é«”ä½¿ç”¨
- `persist()` å¾Œè¨˜æ†¶é«”ä½¿ç”¨æœƒä¸Šå‡ä¸¦ç¶­æŒ

---

# Part 3: ML Pipeline å¯¦æˆ°

## 3.1 ä»»å‹™å®šç¾©ï¼šå°æµåˆ†é¡

### ç§‘å­¸èƒŒæ™¯

**å°æµ (Convection)** æ˜¯å¤§æ°£ä¸­é‡è¦çš„å‚ç›´é‹å‹•ï¼Œå¸¸ä¼´éš¨ï¼š
- å¼·é™é›¨
- é›·æš´
- å†°é›¹
- é¾æ²é¢¨

**é æ¸¬å°æµ**å°æ–¼ç½å®³é è­¦è‡³é—œé‡è¦ã€‚

### å°æµæŒ‡æ¨™

æˆ‘å€‘ä½¿ç”¨ä»¥ä¸‹åƒæ•¸ä¾†é æ¸¬å°æµï¼š

| åƒæ•¸ | ç¸®å¯« | æ„ç¾© | å…¸å‹å°æµå€¼ |
| --- | --- | --- | --- |
| **Convective Available Potential Energy** | CAPE | å°æµå¯ç”¨ä½èƒ½ï¼ˆèƒ½é‡ï¼‰ | > 1000 J/kg |
| **Convective Inhibition** | CIN | å°æµæŠ‘åˆ¶ï¼ˆé˜»ç¤™ï¼‰ | < -50 J/kg |
| **K-Index** | KI | ç¶œåˆä¸ç©©å®šæŒ‡æ•¸ | > 30 |
| **Boundary Layer Height** | BLH | é‚Šç•Œå±¤é«˜åº¦ | > 1500 m |

### ä»»å‹™å®šç¾©

**Inputï¼ˆç‰¹å¾µï¼‰**ï¼š
- CAPE, CIN, K-Index, BLHï¼ˆ4 å€‹è®Šæ•¸ï¼‰
- ç©ºé–“ç¶­åº¦ï¼š16Ã—16 patches
- æ™‚é–“ç¶­åº¦ï¼š32 å€‹æ™‚é–“é»ä¸€æ‰¹

**Outputï¼ˆæ¨™ç±¤ï¼‰**ï¼š
- æ˜¯å¦ç™¼ç”Ÿå°æµï¼ˆäºŒå…ƒåˆ†é¡ï¼š0 æˆ– 1ï¼‰
- å®šç¾©ï¼š`total_precipitation > 5 mm/hr` è¦–ç‚ºå°æµ

**ç›®æ¨™**ï¼š
å»ºç«‹ä¸€å€‹è³‡æ–™ pipelineï¼Œå°‡ Zarr â†’ Xarray â†’ xbatcher â†’ PyTorch

### å¯¦ä½œï¼šæº–å‚™è³‡æ–™

```python
import xarray as xr
import numpy as np
# è®€å–è¨“ç·´è³‡æ–™ï¼ˆ2019-2020ï¼‰train_files = [
    '/home/sungche/NAS/dataset/era5/era5_2019_10N40N_100E140E.zarr',
    '/home/sungche/NAS/dataset/era5/era5_2020_10N40N_100E140E.zarr']
ds_train = xr.open_mfdataset(
    train_files,
    engine='zarr',
    parallel=True,
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}
)
# é¸å–ç‰¹å¾µè®Šæ•¸feature_vars = [
    'convective_available_potential_energy',
    'convective_inhibition',
    'k_index',
    'boundary_layer_height']
features = ds_train[feature_vars]
# å»ºç«‹æ¨™ç±¤ï¼ˆé™é›¨ > 5mm/hr å®šç¾©ç‚ºå°æµï¼‰# ERA5 é™é›¨å–®ä½é€šå¸¸æ˜¯ç´¯ç©é‡ï¼Œéœ€è¦è½‰æ›æˆ mm/hrprecip_threshold = 5  # mm/hrlabel = (ds_train['total_precipitation'] > precip_threshold).astype(int)
label.name = 'convection_flag'# åˆä½µæˆè¨“ç·´è³‡æ–™é›†train_ds = xr.merge([features, label])
print("Training dataset:")
print(train_ds)
print(f"\nå°æµäº‹ä»¶æ¯”ä¾‹ï¼š{label.mean().compute().values * 100:.2f}%")
```

### æº–å‚™é©—è­‰è³‡æ–™

```python
# è®€å–é©—è­‰è³‡æ–™ï¼ˆ2021ï¼‰ds_valid = xr.open_zarr('/home/sungche/NAS/dataset/era5/era5_2021_10N40N_100E140E.zarr')
valid_ds = xr.merge([
    ds_valid[feature_vars],
    (ds_valid['total_precipitation'] > precip_threshold).astype(int).rename('convection_flag')
])
print("Validation dataset:")
print(valid_ds)
```

## 3.2 xbatcherï¼šæ‰¹æ¬¡åˆ‡å‰²

> ğŸ’¡ æ ¸å¿ƒå•é¡Œï¼šå¦‚ä½•æŠŠ TB ç´šçš„ Xarray è³‡æ–™é¤µçµ¦ PyTorchï¼Ÿ
> 
> 
> **ç­”æ¡ˆ**ï¼šä½¿ç”¨ `xbatcher` åˆ‡æˆå°æ‰¹æ¬¡ï¼Œlazy è®€å–
> 

### xbatcher ç°¡ä»‹

**xbatcher** æ˜¯å°ˆç‚º Xarray è¨­è¨ˆçš„æ‰¹æ¬¡ç”¢ç”Ÿå™¨ï¼š
- è‡ªå‹•åˆ‡å‰²ç©ºé–“/æ™‚é–“ç¶­åº¦
- æ”¯æ´ overlapï¼ˆé¿å…é‚Šç•Œæ•ˆæ‡‰ï¼‰
- å®Œå…¨ lazyï¼ˆä¸æœƒä¸€æ¬¡è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼‰
- èˆ‡ Dask å®Œç¾æ•´åˆ

### åŸºæœ¬ä½¿ç”¨

```python
import xbatcher
# å»ºç«‹ BatchGeneratorbgen = xbatcher.BatchGenerator(
    train_ds,
    input_dims={'latitude': 16, 'longitude': 16},     # ç©ºé–“ patch size    input_overlap={'latitude': 4, 'longitude': 4},    # 50% overlap    batch_dims={'time': 32}                            # æ™‚é–“æ‰¹æ¬¡å¤§å°)
print(f"Total batches: {len(bgen)}")
# æŸ¥çœ‹ç¬¬ä¸€å€‹ batchfor i, batch in enumerate(bgen):
    print(f"\n--- Batch {i} ---")
    print(batch)
    print(f"CAPE shape: {batch['convective_available_potential_energy'].shape}")
    print(f"Label shape: {batch['convection_flag'].shape}")
    if i == 0:  # åªçœ‹ç¬¬ä¸€å€‹        break
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š

```
Total batches: 1250

--- Batch 0 ---
<xarray.Dataset>
Dimensions:  (time: 32, latitude: 16, longitude: 16)
Data variables:
    convective_available_potential_energy  (time, latitude, longitude) float32 dask.array<...>
    convective_inhibition                   (time, latitude, longitude) float32 dask.array<...>
    k_index                                 (time, latitude, longitude) float32 dask.array<...>
    boundary_layer_height                   (time, latitude, longitude) float32 dask.array<...>
    convection_flag                         (time, latitude, longitude) int32 dask.array<...>

CAPE shape: (32, 16, 16)
Label shape: (32, 16, 16)
```

### åƒæ•¸è©³è§£

### `input_dims`

```python
# ç©ºé–“ patch çš„å¤§å°input_dims={'latitude': 16, 'longitude': 16}
# å¦‚ä½•é¸æ“‡ï¼Ÿ# - å¤ªå°ï¼ˆå¦‚ 4Ã—4ï¼‰ï¼šcontext ä¸è¶³ï¼Œæ¨¡å‹é›£å­¸ç¿’# - å¤ªå¤§ï¼ˆå¦‚ 128Ã—128ï¼‰ï¼šè¨˜æ†¶é«”éœ€æ±‚é«˜ï¼Œbatch size å—é™# - æ¨è–¦ï¼š16Ã—16 åˆ° 64Ã—64
```

### `input_overlap`

```python
# Patch ä¹‹é–“çš„é‡ç–Šinput_overlap={'latitude': 4, 'longitude': 4}  # 25% overlap# ç‚ºä»€éº¼éœ€è¦ overlapï¼Ÿ# - é¿å…é‚Šç•Œæ•ˆæ‡‰ï¼ˆCNN åœ¨é‚Šç•Œçš„é æ¸¬è¼ƒå·®ï¼‰# - å¢åŠ è¨“ç·´è³‡æ–™é‡ï¼ˆdata augmentationï¼‰# - è®“ç›¸é„° patch ä¹‹é–“æœ‰é€£çºŒæ€§
```

### `batch_dims`

```python
# åœ¨æ™‚é–“ç¶­åº¦ä¸Šæ‰¹æ¬¡åŒ–batch_dims={'time': 32}
# æ³¨æ„ï¼š# - é€™æœƒç”¢ç”Ÿ 32 å€‹æ™‚é–“é»çš„åºåˆ—# - å¦‚æœä½ çš„æ¨¡å‹ä¸è™•ç†æ™‚åºï¼Œå¯ä»¥ä¹‹å¾Œå†å¹³å‡æˆ–é¸å–# - ä¹Ÿå¯ä»¥è¨­å®šå…¶ä»–ç¶­åº¦ï¼Œå¦‚ {'latitude': 4}ï¼ˆä½†è¼ƒå°‘è¦‹ï¼‰
```

### è½‰æ›æˆ NumPy

```python
# å–å¾—ä¸€å€‹ batchbatch = next(iter(bgen))
# æ–¹æ³• 1: to_arrayï¼ˆæ¨è–¦ï¼‰# å°‡å¤šå€‹è®Šæ•¸åˆä½µæˆä¸€å€‹æ–°ç¶­åº¦ 'variable'X = batch[feature_vars].to_array(dim='variable').values
y = batch['convection_flag'].values
print(f"X shape: {X.shape}")  # (4, 32, 16, 16) = (variables, time, lat, lon)print(f"y shape: {y.shape}")  # (32, 16, 16) = (time, lat, lon)# æ–¹æ³• 2: æ‰‹å‹• stackX_manual = np.stack([batch[var].values for var in feature_vars], axis=0)
```

### é€²éšï¼šå‹•æ…‹æ‰¹æ¬¡ï¼ˆIterableï¼‰

```python
# å¦‚æœè³‡æ–™å¤ªå¤§ï¼Œä¸æƒ³é å…ˆç”Ÿæˆæ‰€æœ‰ batch ç´¢å¼•# å¯ä»¥ç›´æ¥è¿­ä»£ï¼ˆæ›´çœè¨˜æ†¶é«”ï¼‰for i, batch in enumerate(bgen):
    # è™•ç† batch    X = batch[feature_vars].to_array(dim='variable').values
    y = batch['convection_flag'].values
    print(f"Batch {i}: X={X.shape}, y={y.shape}")
    if i >= 5:  # åªçœ‹å‰ 5 å€‹        break# é€™æ¨£åšçš„å¥½è™•ï¼š# - ä¸æœƒä¸€æ¬¡ç”Ÿæˆæ‰€æœ‰ batch çš„ç´¢å¼•# - è¨˜æ†¶é«”ä½¿ç”¨æ›´å°‘# - é©åˆè¶…å¤§è³‡æ–™é›†
```

## 3.3 PyTorch DataLoader æ©‹æ¥

### ç›®æ¨™

å»ºç«‹ä¸€å€‹ã€Œæ©‹æ¥å±¤ã€ï¼Œè®“ PyTorch çš„ `DataLoader` èƒ½å¤ è®€å– `xbatcher` ç”¢ç”Ÿçš„è³‡æ–™ã€‚

### å¯¦ä½œï¼šXarrayDataset

```python
from torch.utils.data import Dataset, DataLoader
import torch
import numpy as np
class XarrayDataset(Dataset):
    """    å°‡ xbatcher åŒ…è£æˆ PyTorch Dataset    é€™æ˜¯ä¸€å€‹å¯é‡è¤‡ä½¿ç”¨çš„æ©‹æ¥å±¤ï¼    """    def __init__(self, ds, feature_vars, label_var, batch_config):
        """        Args:            ds: xarray.Datasetï¼ˆè¼¸å…¥è³‡æ–™ï¼‰            feature_vars: list of strï¼ˆç‰¹å¾µè®Šæ•¸åç¨±ï¼‰            label_var: strï¼ˆæ¨™ç±¤è®Šæ•¸åç¨±ï¼‰            batch_config: dictï¼ˆxbatcher è¨­å®šï¼‰        """        self.ds = ds
        self.feature_vars = feature_vars
        self.label_var = label_var
        # å»ºç«‹ BatchGenerator        self.bgen = xbatcher.BatchGenerator(ds, **batch_config)
        # é å…ˆç”Ÿæˆæ‰€æœ‰ batchï¼ˆåªæ˜¯ç´¢å¼•ï¼Œä¸æ˜¯è³‡æ–™ï¼‰        self.batches = list(self.bgen)
        print(f"âœ… XarrayDataset initialized with {len(self.batches)} batches")
    def __len__(self):
        return len(self.batches)
    def __getitem__(self, idx):
        """        å–å¾—ç¬¬ idx å€‹ batchï¼ˆé€™æ™‚æ‰çœŸæ­£è®€å–è³‡æ–™ï¼‰        """        # å–å¾— batch        batch = self.batches[idx]
        # è½‰æˆ NumPyï¼ˆé€™æ™‚æœƒ computeï¼‰        X = batch[self.feature_vars].to_array(dim='variable').values
        y = batch[self.label_var].values
        # è™•ç† NaNï¼ˆå¦‚æœæœ‰ï¼‰        X = np.nan_to_num(X, nan=0.0)
        y = np.nan_to_num(y, nan=0)
        # è½‰æˆ Torch Tensor        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.LongTensor(y)
        return X_tensor, y_tensor
```

### å»ºç«‹ DataLoader

```python
# å®šç¾© batch è¨­å®šbatch_config = {
    'input_dims': {'latitude': 16, 'longitude': 16},
    'input_overlap': {'latitude': 4, 'longitude': 4},
    'batch_dims': {'time': 32}
}
# å»ºç«‹è¨“ç·´ Datasettrain_dataset = XarrayDataset(
    train_ds,
    feature_vars=feature_vars,
    label_var='convection_flag',
    batch_config=batch_config
)
# å»ºç«‹ DataLoadertrain_loader = DataLoader(
    train_dataset,
    batch_size=4,        # ä¸€æ¬¡è®€ 4 å€‹ xarray batch    shuffle=True,        # è¨“ç·´æ™‚æ‰“äº‚    num_workers=2,       # å¹³è¡Œè®€å–ï¼ˆé‡è¦ï¼ï¼‰    pin_memory=True,     # GPU å„ªåŒ–    persistent_workers=True  # ä¿æŒ workers å­˜æ´»ï¼ˆåŠ é€Ÿï¼‰)
print(f"âœ… DataLoader created with {len(train_loader)} batches")
# æ¸¬è©¦ä¸€ä¸‹for X, y in train_loader:
    print(f"X: {X.shape}, dtype: {X.dtype}, device: {X.device}")
    print(f"y: {y.shape}, dtype: {y.dtype}, device: {y.device}")
    break
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š

```
âœ… XarrayDataset initialized with 1250 batches
âœ… DataLoader created with 313 batches
X: torch.Size([4, 4, 32, 16, 16]), dtype: torch.float32, device: cpu
y: torch.Size([4, 32, 16, 16]), dtype: torch.int64, device: cpu
```

### é‡è¦åƒæ•¸èªªæ˜

### `num_workers`

```python
# num_workers=0ï¼šä¸»ç¨‹åºè®€å–ï¼ˆæ…¢ï¼‰# num_workers=2ï¼šé–‹ 2 å€‹å­ç¨‹åºå¹³è¡Œè®€å–ï¼ˆå¿«ï¼‰# num_workers=4ï¼šé–‹ 4 å€‹å­ç¨‹åºï¼ˆæ›´å¿«ï¼Œä½†è¨˜æ†¶é«”éœ€æ±‚é«˜ï¼‰# æ¨è–¦è¨­å®šï¼š# - CPU å……è¶³ï¼šnum_workers = CPU cores / 2# - è¨˜æ†¶é«”æœ‰é™ï¼šnum_workers = 2# - Debug æ™‚ï¼šnum_workers = 0ï¼ˆé¿å…å¤šç¨‹åºéŒ¯èª¤é›£è¿½è¹¤ï¼‰
```

### `pin_memory`

```python
# pin_memory=Trueï¼ˆæ¨è–¦ï¼Œå¦‚æœæœ‰ GPUï¼‰# - å°‡è³‡æ–™å›ºå®šåœ¨ CPU è¨˜æ†¶é«”ä¸­# - åŠ é€Ÿ CPU â†’ GPU è³‡æ–™å‚³è¼¸# pin_memory=False# - ä¸ä½¿ç”¨ GPU æ™‚ï¼Œæˆ–è¨˜æ†¶é«”ä¸è¶³æ™‚
```

### `persistent_workers`

```python
# persistent_workers=Trueï¼ˆæ¨è–¦ï¼‰# - ä¿æŒ workers å­˜æ´»ï¼Œä¸ç”¨æ¯å€‹ epoch é‡å•Ÿ# - åŠ é€Ÿè¨“ç·´ï¼Œç‰¹åˆ¥æ˜¯å¤š epoch æ™‚# persistent_workers=False# - æ¯å€‹ epoch çµæŸå¾Œé—œé–‰ workers# - ç¯€çœè¨˜æ†¶é«”ï¼Œä½†æ¯æ¬¡é‡å•Ÿæœ‰é–‹éŠ·
```

### Dashboard è§€å¯Ÿ

åŸ·è¡Œä»¥ä¸‹ç¨‹å¼ç¢¼ï¼Œè§€å¯Ÿ Dashboardï¼š

```python
# è¿­ä»£å¹¾å€‹ batchï¼Œè§€å¯Ÿ Dashboardfor i, (X, y) in enumerate(train_loader):
    print(f"Batch {i}: X={X.shape}")
    if i >= 5:
        break
```

**è§€å¯Ÿé‡é»**ï¼š
- **Task Stream**ï¼šçœ‹åˆ°è³‡æ–™è®€å–ä»»å‹™ï¼ˆè—è‰²ï¼‰
- **Workers**ï¼š`num_workers=2` æ™‚ï¼Œæœƒçœ‹åˆ°å¤šå€‹ workers åŒæ™‚å·¥ä½œ
- **Memory**ï¼šè¨˜æ†¶é«”ä½¿ç”¨æœƒæ³¢å‹•ï¼ˆè®€å– â†’ è™•ç† â†’ é‡‹æ”¾ï¼‰

## 3.4 æ¨¡å‹è¨“ç·´ï¼ˆç°¡åŒ–ç¤ºç¯„ï¼‰

> âš ï¸ é‡é»æé†’ï¼š
> 
> 
> é€™éƒ¨åˆ†åªæ˜¯ç¤ºç¯„ã€Œè³‡æ–™ pipeline é€šäº†ã€ï¼Œä¸æ·±å…¥è¬›è§£æ¨¡å‹è¨“ç·´æŠ€å·§ã€‚
> æ¨¡å‹æ¶æ§‹ã€è¨“ç·´èª¿åƒæ˜¯å¦ä¸€å ‚èª²çš„å…§å®¹ã€‚
> 

### ç°¡å–®çš„ CNN æ¨¡å‹

```python
import torch.nn as nn
import torch.nn.functional as F
class SimpleConvNet(nn.Module):
    """    è¶…ç´šç°¡å–®çš„ CNNï¼ˆåƒ…ç”¨æ–¼ç¤ºç¯„ï¼‰    Input: (batch, 4, 32, 16, 16)  # (batch, vars, time, lat, lon)    Output: (batch, 2)              # (batch, num_classes)    """    def __init__(self, in_channels=4, num_classes=2):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 16Ã—16 â†’ 8Ã—8            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # å…¨å±€å¹³å‡æ± åŒ– â†’ 1Ã—1        )
        self.classifier = nn.Linear(64, num_classes)
    def forward(self, x):
        # x: (batch, vars, time, lat, lon)        # ç°¡åŒ–ï¼šå°æ™‚é–“ç¶­åº¦å–å¹³å‡        x = x.mean(dim=2)  # â†’ (batch, vars, lat, lon)        # CNN        x = self.features(x)  # â†’ (batch, 64, 1, 1)        x = x.view(x.size(0), -1)  # â†’ (batch, 64)        # åˆ†é¡        x = self.classifier(x)  # â†’ (batch, 2)        return x
```

### è¨“ç·´ä¸€å€‹ Batchï¼ˆç¤ºç¯„ï¼‰

```python
import torch.optim as optim
# å»ºç«‹æ¨¡å‹model = SimpleConvNet(in_channels=len(feature_vars), num_classes=2)
model = model.cuda()  # ç§»åˆ° GPU# å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# è¨“ç·´ä¸€å€‹ batchï¼ˆåªæ˜¯ç¤ºç¯„ï¼ï¼‰model.train()
for X, y in train_loader:
    # ç§»åˆ° GPU    X = X.cuda()
    y = y.cuda()
    # ç°¡åŒ–æ¨™ç±¤ï¼ˆå–ç©ºé–“å¹³å‡å¾ŒäºŒå€¼åŒ–ï¼‰    # y shape: (batch, time, lat, lon) â†’ (batch,)    y_simple = (y.float().mean(dim=[1, 2, 3]) > 0.5).long()
    # å‰å‘å‚³æ’­    outputs = model(X)
    loss = criterion(outputs, y_simple)
    # åå‘å‚³æ’­    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"âœ… Loss: {loss.item():.4f}")
    print(f"âœ… Pipeline é€šäº†ï¼è³‡æ–™èƒ½é †åˆ©å¾ Zarr â†’ Xarray â†’ PyTorch")
    break  # åªè·‘ä¸€å€‹ batch
```

**é æœŸè¼¸å‡º**ï¼š

```
âœ… Loss: 0.6931
âœ… Pipeline é€šäº†ï¼è³‡æ–™èƒ½é †åˆ©å¾ Zarr â†’ Xarray â†’ PyTorch
```

### è¬›è§£é‡é»

è·Ÿå­¸å“¡å¼·èª¿ï¼š

1. **é€™åªæ˜¯è­‰æ˜ pipeline å¯ä»¥é‹ä½œ**
    - è³‡æ–™èƒ½å¾ Zarr è®€å–
    - ç¶“é Xarray è™•ç†
    - é€é xbatcher åˆ‡æ‰¹æ¬¡
    - é€²å…¥ PyTorch æ¨¡å‹
2. **æ¨¡å‹æ¶æ§‹ä¸é‡è¦**
    - ç”šè‡³å¯ä»¥ç”¨ `torchvision.models.resnet18`
    - é‡é»æ˜¯ã€Œè³‡æ–™æµã€ï¼Œä¸æ˜¯ã€Œæ¨¡å‹è¨­è¨ˆã€
3. **çœŸæ­£çš„è¨“ç·´æ˜¯å¦ä¸€å ‚èª²**
    - Learning rate èª¿æ•´
    - Regularization
    - Data augmentation
    - é€™äº›éƒ½ä¸åœ¨æœ¬èª²ç¨‹ç¯„åœ

## 3.5 é æ¸¬çµæœ â†’ Xarray

> ğŸ’¡ æ ¸å¿ƒå•é¡Œï¼šæ¨¡å‹è¼¸å‡ºæ˜¯ Tensor/NumPyï¼Œå¦‚ä½•è½‰å›å¸¶åº§æ¨™çš„ Xarrayï¼Ÿ
> 
> 
> **ç­”æ¡ˆ**ï¼šæ‰‹å‹•å»ºç«‹ `xr.DataArray`ï¼Œä¿ç•™åŸå§‹åº§æ¨™è³‡è¨Š
> 

### ç‚ºä»€éº¼éœ€è¦è½‰å› Xarrayï¼Ÿ

å‚³çµ± ML workflowï¼š

```
Model output: numpy array (n_samples,)
è©•ä¼°: sklearn.metrics.accuracy_score(y_true, y_pred)
çµæœ: 0.85ï¼ˆåªæœ‰ä¸€å€‹æ•¸å­—ï¼‰
```

ç§‘å­¸è³‡æ–™ workflowï¼š

```
Model output: xarray.DataArray with coords (time, lat, lon)
è©•ä¼°: xskillscore.rmse(pred, obs, dim='time')
çµæœ: RMSE at every (lat, lon) pointï¼ˆä¸€å€‹ç©ºé–“å ´ï¼‰
```

**å·®ç•°**ï¼š
- å‚³çµ±æ–¹æ³•ï¼šåªçŸ¥é“ã€Œæ•´é«”æº–ç¢ºç‡ 85%ã€
- ç§‘å­¸æ–¹æ³•ï¼šçŸ¥é“ã€Œå°ç£åŒ—éƒ¨é æ¸¬å¥½ï¼Œå—éƒ¨é æ¸¬å·®ã€

### å¯¦ä½œï¼šç©ºé–“é æ¸¬

```python
# å»ºç«‹é©—è­‰ Datasetï¼ˆä¸ shuffleï¼‰valid_dataset = XarrayDataset(
    valid_ds,
    feature_vars=feature_vars,
    label_var='convection_flag',
    batch_config=batch_config
)
valid_loader = DataLoader(
    valid_dataset,
    batch_size=1,
    shuffle=False,  # é‡è¦ï¼ä¿æŒé †åº    num_workers=2)
# é æ¸¬model.eval()
predictions = []
with torch.no_grad():
    for idx, (X, y) in enumerate(valid_loader):
        X = X.cuda()
        # é æ¸¬        outputs = model(X)
        pred = outputs.argmax(dim=1).cpu().numpy()  # (batch,)        # é€™è£¡åªæœ‰ä¸€å€‹é æ¸¬å€¼ï¼ˆå› ç‚ºæˆ‘å€‘ç°¡åŒ–äº†æ¨™ç±¤ï¼‰        # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œä½ å¯èƒ½æƒ³ä¿ç•™ç©ºé–“ç¶­åº¦        predictions.append(pred)
        if idx >= 100:  # åªé æ¸¬å‰ 100 å€‹ batchï¼ˆç¤ºç¯„ï¼‰            break# åˆä½µé æ¸¬pred_array = np.concatenate(predictions, axis=0)
print(f"Predictions shape: {pred_array.shape}")  # (101,)
```

### å¯¦ä½œï¼šä¿ç•™ç©ºé–“è³‡è¨Šçš„é æ¸¬

å¦‚æœä½ æƒ³è¦ç©ºé–“åˆ†ä½ˆçš„é æ¸¬ï¼ˆè€Œä¸æ˜¯å–®ä¸€å€¼ï¼‰ï¼Œéœ€è¦ä¿®æ”¹æ¨¡å‹ï¼š

```python
class SpatialConvNet(nn.Module):
    """    è¼¸å‡ºç©ºé–“é æ¸¬çš„ CNN    Input: (batch, 4, 32, 16, 16)    Output: (batch, 2, 16, 16)  # ä¿ç•™ç©ºé–“ç¶­åº¦    """    def __init__(self, in_channels=4, num_classes=2):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, num_classes, 1)  # 1Ã—1 convï¼Œä¿ç•™ç©ºé–“        )
    def forward(self, x):
        # x: (batch, vars, time, lat, lon)        
        x = x.mean(dim=2)  # â†’ (batch, vars, lat, lon)        
        x = self.features(x)  # â†’ (batch, num_classes, lat, lon)        
        return x

# ä½¿ç”¨é€™å€‹æ¨¡å‹é æ¸¬
spatial_model = SpatialConvNet().cuda()
# ... è¨“ç·´éç¨‹é¡ä¼¼ ...
# é æ¸¬æ™‚ï¼Œä¿ç•™ç©ºé–“è³‡è¨Š

spatial_model.eval()
spatial_predictions = []
coords_list = []
with torch.no_grad():
    for idx, (X, y) in enumerate(valid_loader):
        X = X.cuda()
        outputs = spatial_model(X)  # (batch, 2, lat, lon)        
        pred = outputs.argmax(dim=1).cpu().numpy()  # (batch, lat, lon)        
        # å–å¾—å°æ‡‰çš„åº§æ¨™        
        batch_data = valid_dataset.batches[idx]
        # å»ºç«‹ DataArray        
        pred_da = xr.DataArray(
            pred[0],  # å–ç¬¬ä¸€å€‹ï¼ˆå› ç‚º batch_size=1ï¼‰            
            coords={
                'latitude': batch_data.latitude,
                'longitude': batch_data.longitude
            },
            dims=['latitude', 'longitude']
        )
        spatial_predictions.append(pred_da)
        if idx >= 100:
            break
            # åˆä½µæ‰€æœ‰ patchï¼ˆéœ€è¦è™•ç† overlapï¼‰
            # é€™æ˜¯ä¸€å€‹é€²éšè©±é¡Œï¼Œé€™è£¡å…ˆç°¡åŒ–
            print(f"âœ… æ”¶é›†äº† {len(spatial_predictions)} å€‹ç©ºé–“é æ¸¬")
```

## 3.6 xskillscore ç©ºé–“é©—è­‰

> ğŸ’¡ xskillscore çš„æ ¸å¿ƒå„ªå‹¢ï¼š
> 
> 
> å¯ä»¥è¨ˆç®—ã€Œä¿ç•™åº§æ¨™è³‡è¨Šã€çš„é©—è­‰æŒ‡æ¨™ï¼ŒçŸ¥é“ã€Œå“ªè£¡é æ¸¬å¾—å¥½/å·®ã€
> 

### å®‰è£èˆ‡å°å…¥

```python
import xskillscore as xs
import numpy as np
import matplotlib.pyplot as plt
```

### ç¯„ä¾‹ï¼šè¨ˆç®— RMSE

```python
# ç‚ºäº†ç¤ºç¯„ï¼Œæˆ‘å€‘å…ˆå»ºç«‹ä¸€äº›å‡è³‡æ–™# å¯¦éš›æ‡‰ç”¨æ™‚ï¼Œé€™æœƒæ˜¯ä½ çš„æ¨¡å‹é æ¸¬# è§€æ¸¬å€¼obs = valid_ds['convection_flag'].isel(time=slice(0, 100))
# å‡è¨­çš„é æ¸¬å€¼ï¼ˆå¯¦éš›ä¸Šæ‡‰è©²ä¾†è‡ªæ¨¡å‹ï¼‰# é€™è£¡ç”¨è§€æ¸¬å€¼åŠ ä¸Šä¸€äº›é›œè¨Špred = obs + np.random.randn(*obs.shape) * 0.2pred = pred.clip(0, 1)  # é™åˆ¶åœ¨ [0, 1]print(f"Observation shape: {obs.shape}")  # (100, 121, 161)print(f"Prediction shape: {pred.shape}")
# è¨ˆç®—ç©ºé–“åˆ†ä½ˆçš„ RMSEï¼ˆå°æ™‚é–“ç¶­åº¦ï¼‰rmse_spatial = xs.rmse(pred, obs, dim='time')
print("Spatial RMSE:")
print(rmse_spatial)  # (121, 161) - æ¯å€‹æ ¼é»çš„ RMSE
```

### ç¯„ä¾‹ï¼šè¨ˆç®—å¤šç¨®æŒ‡æ¨™

```python
# 1. Mean Absolute Errormae = xs.mae(pred, obs, dim='time')
# 2. Mean Squared Errormse = xs.mse(pred, obs, dim='time')
# 3. Pearson Correlationcorr = xs.pearson_r(pred, obs, dim='time')
# 4. R-squaredr2 = xs.r2(pred, obs, dim='time')
print(f"MAE (spatial mean): {mae.mean().values:.4f}")
print(f"MSE (spatial mean): {mse.mean().values:.4f}")
print(f"Correlation (spatial mean): {corr.mean().values:.4f}")
print(f"RÂ² (spatial mean): {r2.mean().values:.4f}")
```

### è¦–è¦ºåŒ–é©—è­‰çµæœ

```python
import cartopy.crs as ccrs
import cartopy.feature as cfeature
# å»ºç«‹åœ–è¡¨fig, axes = plt.subplots(2, 3, figsize=(18, 10),
                          subplot_kw={'projection': ccrs.PlateCarree()})
# ç¬¬ä¸€åˆ—ï¼šè§€æ¸¬ã€é æ¸¬ã€å·®ç•°t_idx = 0  # ç¬¬ä¸€å€‹æ™‚é–“é»obs.isel(time=t_idx).plot(
    ax=axes[0, 0],
    cmap='RdBu_r',
    vmin=0, vmax=1,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Convection Flag'}
)
axes[0, 0].coastlines()
axes[0, 0].set_title('Observation (t=0)')
pred.isel(time=t_idx).plot(
    ax=axes[0, 1],
    cmap='RdBu_r',
    vmin=0, vmax=1,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Convection Flag'}
)
axes[0, 1].coastlines()
axes[0, 1].set_title('Prediction (t=0)')
(pred.isel(time=t_idx) - obs.isel(time=t_idx)).plot(
    ax=axes[0, 2],
    cmap='RdBu',
    vmin=-0.5, vmax=0.5,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Difference'}
)
axes[0, 2].coastlines()
axes[0, 2].set_title('Difference (Pred - Obs)')
# ç¬¬äºŒåˆ—ï¼šé©—è­‰æŒ‡æ¨™rmse_spatial.plot(
    ax=axes[1, 0],
    cmap='viridis',
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'RMSE'}
)
axes[1, 0].coastlines()
axes[1, 0].set_title('RMSE (over time)')
corr.plot(
    ax=axes[1, 1],
    cmap='RdBu_r',
    vmin=-1, vmax=1,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Correlation'}
)
axes[1, 1].coastlines()
axes[1, 1].set_title('Correlation (over time)')
mae.plot(
    ax=axes[1, 2],
    cmap='viridis',
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'MAE'}
)
axes[1, 2].coastlines()
axes[1, 2].set_title('MAE (over time)')
plt.tight_layout()
plt.savefig('validation_results.png', dpi=150, bbox_inches='tight')
plt.show()
print("âœ… é©—è­‰çµæœå·²å„²å­˜ï¼švalidation_results.png")
```

### xskillscore é€²éšåŠŸèƒ½

### åˆ†é¡å•é¡Œçš„æ··æ·†çŸ©é™£ç›¸é—œæŒ‡æ¨™

```python
# å°‡é æ¸¬å’Œè§€æ¸¬è½‰æˆäºŒå…ƒpred_binary = (pred > 0.5).astype(int)
obs_binary = obs.astype(int)
# å¯ä»¥ä½¿ç”¨ sklearn è¨ˆç®—æ··æ·†çŸ©é™£ï¼Œç„¶å¾Œå¯è¦–åŒ–from sklearn.metrics import confusion_matrix, classification_report
# å±•å¹³æˆ 1Dpred_flat = pred_binary.values.ravel()
obs_flat = obs_binary.values.ravel()
# è¨ˆç®—cm = confusion_matrix(obs_flat, pred_flat)
print("Confusion Matrix:")
print(cm)
report = classification_report(obs_flat, pred_flat, target_names=['No Convection', 'Convection'])
print("\nClassification Report:")
print(report)
```

### æ™‚é–“åºåˆ—é©—è­‰

```python
# è¨ˆç®—æ™‚é–“åºåˆ—çš„ç›¸é—œä¿‚æ•¸ï¼ˆå°ç©ºé–“ç¶­åº¦ï¼‰corr_temporal = xs.pearson_r(pred, obs, dim=['latitude', 'longitude'])
# ç¹ªè£½æ™‚é–“åºåˆ—fig, ax = plt.subplots(figsize=(12, 4))
corr_temporal.plot(ax=ax)
ax.set_xlabel('Time')
ax.set_ylabel('Spatial Correlation')
ax.set_title('Temporal Evolution of Spatial Correlation')
ax.axhline(0.5, color='r', linestyle='--', label='Threshold')
ax.legend()
plt.tight_layout()
plt.savefig('temporal_correlation.png', dpi=150)
plt.show()
```

### èˆ‡å‚³çµ±æ–¹æ³•çš„å°æ¯”

```python
# å‚³çµ±æ–¹æ³•ï¼ˆsklearnï¼‰
from sklearn.metrics import accuracy_score, precision_score, recall_score
pred_flat = (pred > 0.5).astype(int).values.ravel()
obs_flat = obs.astype(int).values.ravel()
acc = accuracy_score(obs_flat, pred_flat)
prec = precision_score(obs_flat, pred_flat)
rec = recall_score(obs_flat, pred_flat)
print("=== å‚³çµ±æ–¹æ³•ï¼ˆæ•´é«”æŒ‡æ¨™ï¼‰===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
# xskillscore æ–¹æ³•ï¼ˆç©ºé–“åˆ†ä½ˆï¼‰print("\n=== xskillscore æ–¹æ³•ï¼ˆç©ºé–“åˆ†ä½ˆï¼‰===")
print(f"RMSE (mean): {rmse_spatial.mean().values:.4f}")
print(f"RMSE (std):  {rmse_spatial.std().values:.4f}")
print(f"RMSE (min):  {rmse_spatial.min().values:.4f}")
print(f"RMSE (max):  {rmse_spatial.max().values:.4f}")
print("\nâœ… xskillscore å¯ä»¥å‘Šè¨´ä½ ã€Œå“ªè£¡ã€é æ¸¬å¾—å¥½/å·®ï¼")
```

# ç¸½çµèˆ‡ä¸‹ä¸€æ­¥

## æœ¬èª²ç¨‹å­¸åˆ°çš„æ ¸å¿ƒæŠ€èƒ½

### 1. Zarr å„²å­˜å„ªåŒ–

- ç†è§£ Zarr vs NetCDF çš„å·®ç•°
- çŸ¥é“ç‚ºä½•è¦ç”¨ Zarr < 3.0
- èƒ½å¤ è®€å–ä¸¦å„ªåŒ– Zarr æª”æ¡ˆ

### 2. Xarray + Dask è³‡æ–™è™•ç†

- ä½¿ç”¨ lazy evaluation è™•ç†è¶…éè¨˜æ†¶é«”çš„è³‡æ–™
- æ™‚ç©ºåˆ‡ç‰‡èˆ‡é‡æ¡æ¨£
- ç†è§£ chunking å°æ•ˆèƒ½çš„å½±éŸ¿
- ä½¿ç”¨ Dashboard ç›£æ§æ•ˆèƒ½

### 3. ML Pipeline å»ºæ§‹

- ä½¿ç”¨ xbatcher åˆ‡æ‰¹æ¬¡
- å»ºç«‹ Xarray â†’ PyTorch æ©‹æ¥å±¤
- å°‡é æ¸¬çµæœè½‰å› Xarray
- ä½¿ç”¨ xskillscore é€²è¡Œç©ºé–“é©—è­‰

---

## é‡è¦è§€å¿µå›é¡§

### Lazy Evaluation

```python
# Lazyï¼ˆä¸è¨ˆç®—ï¼‰result_lazy = ds['temperature'].mean(dim='time')
# Eagerï¼ˆè¨ˆç®—ï¼‰result_eager = result_lazy.compute()
```

**ä½•æ™‚ç”¨ lazyï¼Ÿ**
- æ¢ç´¢è³‡æ–™æ™‚ï¼ˆæƒ³å¿«é€Ÿçœ‹çµæœï¼‰
- å»ºç«‹è¤‡é›œè¨ˆç®—æµç¨‹æ™‚
- è³‡æ–™å¤§æ–¼è¨˜æ†¶é«”æ™‚

**ä½•æ™‚ç”¨ computeï¼Ÿ**
- éœ€è¦å¯¦éš›æ•¸å€¼æ™‚
- è¦å„²å­˜çµæœæ™‚
- è¦ç¹ªåœ–æˆ–è¼¸å‡ºæ™‚

---

### Chunking ç­–ç•¥

| Chunk Size | ä»»å‹™æ•¸ | è¨˜æ†¶é«” | é©ç”¨æƒ…å¢ƒ |
| --- | --- | --- | --- |
| å°ï¼ˆ10 MBï¼‰ | å¤š | ä½ | è¨˜æ†¶é«”æœ‰é™ |
| ä¸­ï¼ˆ50 MBï¼‰ | é©ä¸­ | é©ä¸­ | **æ¨è–¦** |
| å¤§ï¼ˆ500 MBï¼‰ | å°‘ | é«˜ | CPU ç¶å®šçš„è¨ˆç®— |

**ç¶“é©—æ³•å‰‡**ï¼š
- å–®å€‹ chunkï¼š10-100 MB
- ç¬¦åˆè®€å–æ¨¡å¼ï¼ˆæ™‚é–“åºåˆ—ï¼Ÿç©ºé–“åˆ‡ç‰‡ï¼Ÿï¼‰
- å¹³è¡¡ä»»å‹™æ•¸èˆ‡å‚³è¼¸é–‹éŠ·

---

### ML Pipeline æœ€ä½³å¯¦è¸

```python
# è³‡æ–™æµå‘Zarr files
  â†“ xr.open_zarr() [Xarray Dataset]
  â†“ xbatcher.BatchGenerator() [Lazy Batches]
  â†“ torch.utils.data.Dataset [PyTorch DataLoader]
  â†“ model.forward() [Predictions (Tensor)]
  â†“ xr.DataArray() [Xarray DataArray with coords]
  â†“ xskillscore [Spatial validation metrics]
```

## å»¶ä¼¸å­¸ç¿’è³‡æº

## å®˜æ–¹æ–‡ä»¶

- **Xarray**: https://docs.xarray.dev/
- **Dask**: https://docs.dask.org/
- **Zarr**: https://zarr.readthedocs.io/
- **xbatcher**: [https://xbatcher.readthedocs.io/](https://xbatcher.readthedocs.io/)
- **xskillscore**: [https://xskillscore.readthedocs.io/](https://xskillscore.readthedocs.io/)
- 

### é€²éšä¸»é¡Œ

### 1. åˆ†æ•£å¼é‹ç®—ï¼ˆDask Clusterï¼‰

```python
from dask.distributed import Client
from dask_jobqueue import SLURMCluster
# åœ¨ HPC ä¸Šå»ºç«‹ cluster
cluster = SLURMCluster(cores=4, memory='16GB')
cluster.scale(jobs=10)  # å•Ÿå‹• 10 å€‹ workersclient = Client(cluster)
```

### 2. é›²ç«¯å„²å­˜ï¼ˆS3, GCSï¼‰

```python
import fsspec
# å¾ S3 è®€å– Zarr
ds = xr.open_zarr(
    's3://bucket-name/data.zarr',
    storage_options={'anon': True}
)
```

### 3. GPU åŠ é€Ÿï¼ˆcupy, cuDFï¼‰

```python
# ä½¿ç”¨ CuPy åŠ é€Ÿ Dask Array
import cupy as cp
import dask.array as da

# å»ºç«‹ GPU array
x = da.from_array(cp.random.random((10000, 10000)), chunks=(1000, 1000))
result = x.mean().compute()  # åœ¨ GPU ä¸Šè¨ˆç®—
```

---

## ä¸‹ä¸€æ­¥å»ºè­°

### å¦‚æœä½ æƒ³æ·±å…¥è³‡æ–™è™•ç†ï¼š

- å­¸ç¿’ Dask DataFrameï¼ˆPart 2ï¼Œå¦‚æœé–‹èª²çš„è©±ï¼‰
- æ¢ç´¢ Polars / DuckDBï¼ˆé«˜æ•ˆèƒ½è¡¨æ ¼è™•ç†ï¼‰
- ç ”ç©¶ Apache Arrowï¼ˆè¨˜æ†¶é«”æ ¼å¼ï¼‰

### å¦‚æœä½ æƒ³æ·±å…¥ MLï¼š

- å­¸ç¿’ PyTorch Lightningï¼ˆé«˜éšè¨“ç·´æ¡†æ¶ï¼‰
- æ¢ç´¢ Hugging Face Datasetsï¼ˆML è³‡æ–™é›†å·¥å…·ï¼‰

### å¦‚æœä½ æƒ³æ·±å…¥æ°£è±¡æ‡‰ç”¨ï¼š

- æ¢ç´¢ MetPyï¼ˆæ°£è±¡è¨ˆç®—ï¼‰
- å­¸ç¿’ Satpyï¼ˆè¡›æ˜Ÿè³‡æ–™è™•ç†ï¼‰
- ç ”ç©¶ Climate Data Operators (CDO)

---

## åƒè€ƒè³‡æ–™

### netCDF vs Zarr

[netCDF vs Zarr, an Incomplete Comparison | NSF Unidata](https://www.unidata.ucar.edu/blogs/news/entry/netcdf-vs-zarr-an-incomplete)

### **Pangeo**: å¤§æ°£èˆ‡æµ·æ´‹ç§‘å­¸çš„é–‹æºç¤¾ç¾¤

[Pangeo: A community for open, reproducible, scalable geoscience](https://pangeo.io/)

[SBOTOP:Link Alternatif SBOTOP, Agen SBOTOP Login, Daftar Akun SBOTOP Mobile Terbaru](https://xarray-spatial.org/)

- **Intake**: è³‡æ–™ç›®éŒ„ç³»çµ±
    - https://intake.readthedocs.io/
- **Xarray-spatial**: åœ°ç†ç©ºé–“åˆ†æ
    - https://xarray-spatial.org/