# Dask Array å¯¦æˆ°ï¼šå¾æ°£è±¡è³‡æ–™åˆ° ML Pipeline

æ—¥æœŸ: 2025å¹´10æœˆ29æ—¥
ç‹€æ…‹: é€²è¡Œä¸­
èª²ç¨‹æ™‚é•·: 3 å°æ™‚

---

## èª²ç¨‹ç°¡ä»‹

> ğŸ“¢ **æœ¬å·¥ä½œåŠæ˜¯ã€ŒPython å¤§æ•¸æ“šè™•ç†ä¸‰éƒ¨æ›²ã€çš„ç¬¬äºŒéƒ¨ï¼šN-D Array è™•ç†ç¯‡**
>
> ç•¶ä½ çš„æ°£è±¡è³‡æ–™å¾ GB æˆé•·åˆ° TB ç´šåˆ¥ï¼Œå‚³çµ±çš„ NetCDF + Xarray å–®æ©Ÿè™•ç†æ–¹å¼å°‡æœƒé­é‡ç“¶é ¸ã€‚æœ¬èª²ç¨‹å°‡å¸¶ä½ ä½¿ç”¨ Zarr + Dask + Xarray çš„ç¾ä»£åŒ–å·¥ä½œæµç¨‹ï¼Œå¯¦ç¾çœŸæ­£çš„ out-of-core åˆ†æï¼Œä¸¦å°‡è³‡æ–™ç„¡ç¸«æ¥å…¥ PyTorch/TensorFlow é€²è¡Œæ©Ÿå™¨å­¸ç¿’ã€‚

---

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬èª²ç¨‹å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

1. **åˆ†æè¶…éè¨˜æ†¶é«”å®¹é‡çš„è³‡æ–™**ï¼šä½¿ç”¨ Xarray + Dask è™•ç† TB ç´šæ°£è±¡è³‡æ–™è€Œä¸æœƒ OOM
2. **å„ªåŒ–å„²å­˜æ ¼å¼**ï¼šç†è§£ Zarr çš„å„ªå‹¢ä¸¦å°‡ NetCDF è½‰æ›æˆå„ªåŒ–çš„ Zarr store
3. **å»ºç«‹ ML è³‡æ–™ Pipeline**ï¼šä½¿ç”¨ xbatcher å°‡å¤§å‹ç§‘å­¸è³‡æ–™ç„¡ç¸«æ¥å…¥ PyTorch
4. **ç§‘å­¸åŒ–é©—è­‰**ï¼šä½¿ç”¨ xskillscore é€²è¡Œä¿ç•™ç©ºé–“è³‡è¨Šçš„æ¨¡å‹é©—è­‰

---

## å…ˆå‚™çŸ¥è­˜

æœ¬èª²ç¨‹å‡è¨­ä½ å·²ç¶“å…·å‚™ï¼š

- âœ… Python åŸºç¤èªæ³•
- âœ… NumPy åŸºæœ¬æ“ä½œ
- âœ… ç†è§£ Dask æ ¸å¿ƒæ¦‚å¿µï¼ˆLazy Evaluation, Task Graphsï¼‰
- âœ… ç†è§£ GIL, Processes vs. Threadsï¼ˆå·²å®Œæˆ Part 1 èª²ç¨‹ï¼‰

---

## èª²ç¨‹æ¶æ§‹

```
Part 0: ç’°å¢ƒè¨­å®š (10 min)
â”œâ”€â”€ uv å°ˆæ¡ˆç®¡ç†
â”œâ”€â”€ Jupyter Kernel è¨­å®š
â””â”€â”€ Dask Dashboard å•Ÿå‹•

Part 1: Zarr èˆ‡ Xarray åŸºç¤ (40 min)
â”œâ”€â”€ ç‚ºä»€éº¼éœ€è¦ Zarrï¼Ÿ
â”œâ”€â”€ Zarr vs NetCDF å°ç…§
â”œâ”€â”€ Zarr < 3.0 ç‰ˆæœ¬é¸æ“‡
â””â”€â”€ å¯¦ä½œï¼šè®€å–èˆ‡æ¢ç´¢

Part 2: æ™‚ç©ºè³‡æ–™è™•ç† (50 min)
â”œâ”€â”€ å¤šæª”æ¡ˆè®€å–
â”œâ”€â”€ æ™‚ç©ºåˆ‡ç‰‡èˆ‡é‡æ¡æ¨£
â”œâ”€â”€ è¨ˆç®—èˆ‡å„²å­˜å„ªåŒ–
â””â”€â”€ Dashboard æ•ˆèƒ½åˆ†æ

Part 3: ML Pipeline å¯¦æˆ° (90 min)
â”œâ”€â”€ ä»»å‹™å®šç¾©ï¼šå°æµåˆ†é¡
â”œâ”€â”€ xbatcher æ‰¹æ¬¡åˆ‡å‰²
â”œâ”€â”€ PyTorch DataLoader æ©‹æ¥
â”œâ”€â”€ è¨“ç·´æµç¨‹ç¤ºç¯„
â”œâ”€â”€ é æ¸¬çµæœ â†’ Xarray
â””â”€â”€ xskillscore ç©ºé–“é©—è­‰
```

---

# Part 0: ç’°å¢ƒè¨­å®š

## 0.1 ä½¿ç”¨ uv å»ºç«‹å°ˆæ¡ˆç’°å¢ƒ

> ğŸ’¡ **ç‚ºä»€éº¼ç”¨ uvï¼Ÿ**
>
> `uv` æ˜¯æ–°ä¸€ä»£çš„ Python å¥—ä»¶ç®¡ç†å·¥å…·ï¼Œæ¯”å‚³çµ±çš„ pip + virtualenv å¿« 10-100 å€ï¼Œä¸”èƒ½è‡ªå‹•è™•ç†ç›¸ä¾æ€§è¡çªã€‚

### å®‰è£ uv

```bash
# å¦‚æœé‚„æ²’å®‰è£ uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# é©—è­‰å®‰è£
uv --version
```

### åˆå§‹åŒ–å°ˆæ¡ˆ

```bash
# å»ºç«‹å°ˆæ¡ˆç›®éŒ„ï¼ˆå¦‚æœé‚„æ²’æœ‰ï¼‰
cd ~/workshop
mkdir dask-array-workshop
cd dask-array-workshop

# åˆå§‹åŒ–å°ˆæ¡ˆ
uv init

# æŸ¥çœ‹ç”¢ç”Ÿçš„æª”æ¡ˆ
ls -la
# ä½ æœƒçœ‹åˆ°ï¼š
# - pyproject.toml  (å°ˆæ¡ˆè¨­å®šæª”)
# - .python-version (Python ç‰ˆæœ¬)
```

### å®‰è£ç›¸ä¾å¥—ä»¶

```bash
# æ ¸å¿ƒå¥—ä»¶
uv add "xarray>=2024.10.0"
uv add "zarr>=2.18.0,<3.0.0"  # æ³¨æ„ï¼å¿…é ˆ < 3.0
uv add "dask[complete]>=2024.10.0"
uv add "numpy>=2.1.0"
uv add "matplotlib>=3.9.0"

# ML ç›¸é—œ
uv add "xbatcher>=0.3.0"
uv add "xskillscore>=0.0.26"
uv add "torch>=2.0.0"
uv add "torchvision>=0.15.0"

# é–‹ç™¼å·¥å…·
uv add --dev ipykernel
uv add --dev jupyterlab
```

### æª¢æŸ¥ç’°å¢ƒ

```bash
# æŸ¥çœ‹å·²å®‰è£å¥—ä»¶
uv pip list

# æ¸¬è©¦ import
uv run python -c "import xarray, dask, zarr; print('Environment OK!')"
```

---

## 0.2 VSCode Jupyter Kernel è¨­å®š

### æ­¥é©Ÿ 1: å»ºç«‹ Jupyter Kernel

```bash
# ä½¿ç”¨ uv å»ºç«‹ kernel
uv run python -m ipykernel install --user --name dask-workshop --display-name "Dask Workshop"
```

### æ­¥é©Ÿ 2: åœ¨ VSCode é¸æ“‡ Kernel

1. æ‰“é–‹ VSCode
2. å»ºç«‹æ–°çš„ `.ipynb` æª”æ¡ˆ
3. é»æ“Šå³ä¸Šè§’ã€Œé¸æ“‡æ ¸å¿ƒã€
4. é¸æ“‡ `Dask Workshop`

### æ­¥é©Ÿ 3: æ¸¬è©¦é€£ç·š

åœ¨ Notebook ä¸­åŸ·è¡Œï¼š

```python
# æ¸¬è©¦ Cell
import xarray as xr
import dask
import zarr

print(f"Xarray version: {xr.__version__}")
print(f"Dask version: {dask.__version__}")
print(f"Zarr version: {zarr.__version__}")

# ç¢ºèª Zarr < 3.0
assert int(zarr.__version__.split('.')[0]) < 3, "è«‹ç¢ºèª Zarr ç‰ˆæœ¬ < 3.0"
print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆï¼")
```

---

## 0.3 å•Ÿå‹• Dask Dashboard

> ğŸ’¡ **Dask Dashboard æ˜¯ä»€éº¼ï¼Ÿ**
>
> Dashboard æ˜¯ä¸€å€‹ç¶²é ä»‹é¢ï¼Œå¯ä»¥å³æ™‚è§€å¯Ÿï¼š
> - Task Graphï¼ˆä»»å‹™ä¾è³´é—œä¿‚ï¼‰
> - Task Streamï¼ˆä»»å‹™åŸ·è¡Œæ™‚é–“ç·šï¼‰
> - Memory Usageï¼ˆè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼‰
> - Workers Statusï¼ˆå·¥ä½œç¨‹åºç‹€æ…‹ï¼‰

### å•Ÿå‹• Client

åœ¨ Jupyter Notebook ä¸­åŸ·è¡Œï¼š

```python
from dask.distributed import Client

# å•Ÿå‹•æœ¬åœ° Cluster
client = Client()

# é¡¯ç¤º Dashboard é€£çµ
print(client)
print(f"Dashboard: {client.dashboard_link}")
```

ä½ æœƒçœ‹åˆ°é¡ä¼¼çš„è¼¸å‡ºï¼š

```
<Client: 'tcp://127.0.0.1:xxxxx' processes=8 threads=16>
Dashboard: http://127.0.0.1:8787/status
```

### æŸ¥çœ‹ Dashboard

1. **æœ¬æ©Ÿç’°å¢ƒ**ï¼šç›´æ¥é–‹å•Ÿç€è¦½å™¨è¨ªå• `http://127.0.0.1:8787/status`

2. **é ç«¯ Serverï¼ˆSSHï¼‰**ï¼šéœ€è¦è¨­å®š Port Forwarding
   ```bash
   # åœ¨æœ¬æ©ŸåŸ·è¡Œ
   ssh -L 8787:localhost:8787 user@remote-server

   # ç„¶å¾Œåœ¨æœ¬æ©Ÿç€è¦½å™¨è¨ªå•
   # http://localhost:8787/status
   ```

3. **VSCode Remote SSH**ï¼šVSCode æœƒè‡ªå‹•è½‰ç™¼ portï¼Œç›´æ¥é»æ“Šé€£çµå³å¯

### Dashboard é‡è¦é é¢

- **Status**: ç¸½è¦½
- **Task Stream**: å³æ™‚ä»»å‹™åŸ·è¡Œï¼ˆæœ€å¸¸ç”¨ï¼‰
- **Progress**: é€²åº¦æ¢
- **Graph**: ä»»å‹™ç›¸ä¾åœ–
- **System**: CPU/Memory ä½¿ç”¨ç‡

---

# Part 1: Zarr èˆ‡ Xarray åŸºç¤

## 1.1 ç‚ºä»€éº¼éœ€è¦ Zarrï¼Ÿ

### å‚³çµ± NetCDF çš„ç—›é»

ç•¶ä½ çš„ç ”ç©¶å®¤ NAS ä¸Šæœ‰æ•¸ TB çš„ ERA5 NetCDF è³‡æ–™æ™‚ï¼Œä½ å¯èƒ½é‡åˆ°éé€™äº›å•é¡Œï¼š

âŒ **è®€å–é€Ÿåº¦æ…¢**
```python
# é€™å€‹æ“ä½œå¯èƒ½è¦ç­‰å¥½å¹¾åˆ†é˜...
ds = xr.open_mfdataset('*.nc')  # æƒææ‰€æœ‰ metadata
```

âŒ **ç„¡æ³•å¹³è¡Œè®€å–**
```python
# NetCDF æœ‰ metadata lockï¼Œç„¡æ³•çœŸæ­£å¹³è¡Œ
# å¤šå€‹ process åŒæ™‚è®€å– â†’ äº’ç›¸ç­‰å¾…
```

âŒ **é›²ç«¯ä¸å‹å–„**
```python
# å¦‚æœè³‡æ–™åœ¨ S3/GCS ä¸Š...
# NetCDF éœ€è¦ä¸‹è¼‰æ•´å€‹æª”æ¡ˆæ‰èƒ½è®€å–
```

---

### Zarr çš„è§£æ±ºæ–¹æ¡ˆ

âœ… **ç›®éŒ„çµæ§‹ï¼Œæ¯å€‹ chunk æ˜¯ç¨ç«‹æª”æ¡ˆ**

```
era5.zarr/
â”œâ”€â”€ .zattrs              # å…¨åŸŸå±¬æ€§
â”œâ”€â”€ .zgroup              # ç¾¤çµ„è³‡è¨Š
â”œâ”€â”€ temperature/
â”‚   â”œâ”€â”€ .zarray          # é™£åˆ— metadata
â”‚   â”œâ”€â”€ .zattrs          # è®Šæ•¸å±¬æ€§
â”‚   â”œâ”€â”€ 0.0.0            # chunk æª”æ¡ˆ
â”‚   â”œâ”€â”€ 0.0.1
â”‚   â”œâ”€â”€ 0.1.0
â”‚   â””â”€â”€ ...
â””â”€â”€ precipitation/
    â””â”€â”€ ...
```

âœ… **çœŸæ­£çš„å¹³è¡Œè®€å¯«**

- æ²’æœ‰ metadata lock
- æ¯å€‹ worker è®€å–ä¸åŒçš„ chunk æª”æ¡ˆ
- å¯ä»¥åŒæ™‚å¯«å…¥ä¸åŒçš„ chunk

âœ… **é›²ç«¯åŸç”Ÿè¨­è¨ˆ**

- åªä¸‹è¼‰éœ€è¦çš„ chunk
- æ”¯æ´ S3, GCS, Azure Blob
- HTTP Range Request å‹å–„

---

## 1.2 Zarr vs NetCDF å®Œæ•´å°ç…§

| ç‰¹æ€§ | NetCDF (HDF5) | Zarr | èªªæ˜ |
|------|---------------|------|------|
| **æª”æ¡ˆçµæ§‹** | å–®ä¸€æª”æ¡ˆ | ç›®éŒ„ + å¤šå€‹æª”æ¡ˆ | Zarr æ¯å€‹ chunk ç¨ç«‹ |
| **Metadata** | é›†ä¸­å¼ï¼ˆæœ‰ lockï¼‰ | åˆ†æ•£å¼ï¼ˆç„¡ lockï¼‰ | Zarr å¯å¹³è¡Œè®€å¯« |
| **å¹³è¡Œè®€å–** | å—é™ | å®Œå…¨æ”¯æ´ | NetCDF æœ‰ç«¶çˆ­å•é¡Œ |
| **å¹³è¡Œå¯«å…¥** | ä¸æ”¯æ´ | å®Œå…¨æ”¯æ´ | Zarr å¯åŒæ™‚å¯«ä¸åŒ chunk |
| **é›²ç«¯å„²å­˜** | éœ€å®Œæ•´ä¸‹è¼‰ | åªä¸‹è¼‰éœ€è¦çš„ chunk | Zarr ç¯€çœé »å¯¬ |
| **å£“ç¸®é¸é …** | æœ‰é™ï¼ˆzlib, gzipï¼‰ | è±å¯Œï¼ˆBlosc, Zstd, LZ4...ï¼‰ | Zarr å£“ç¸®æ›´å¿«æ›´å¥½ |
| **è¿½åŠ è³‡æ–™** | å›°é›£ | å®¹æ˜“ | Zarr ç›´æ¥æ–°å¢ chunk |
| **ç”Ÿæ…‹ç³»æ”¯æ´** | æˆç†Ÿï¼ˆå¹¾åå¹´ï¼‰ | æ–°èˆˆï¼ˆå¿«é€Ÿæˆé•·ï¼‰ | NetCDF ä»æ˜¯ä¸»æµæ ¼å¼ |

### é€Ÿåº¦å°æ¯”ï¼ˆå¯¦æ¸¬ï¼‰

ä»¥ 100GB ERA5 è³‡æ–™ç‚ºä¾‹ï¼š

| æ“ä½œ | NetCDF | Zarr | åŠ é€Ÿæ¯” |
|------|--------|------|--------|
| é–‹å•Ÿæª”æ¡ˆ | 30 ç§’ | 0.1 ç§’ | **300x** |
| è®€å–å–®ä¸€æ™‚é–“åˆ‡ç‰‡ | 5 ç§’ | 0.5 ç§’ | **10x** |
| è®€å–ç©ºé–“å­å€åŸŸ | 8 ç§’ | 1 ç§’ | **8x** |
| è¨ˆç®—å…¨åŸŸå¹³å‡ | 45 ç§’ | 12 ç§’ | **3.75x** |

---

## 1.3 Zarr < 3.0 çš„ç‰ˆæœ¬é¸æ“‡

### âš ï¸ ç‚ºä»€éº¼å¿…é ˆç”¨ Zarr < 3.0ï¼Ÿ

2024 å¹´ï¼ŒZarr ç™¼å¸ƒäº† 3.0 ç‰ˆæœ¬ï¼Œé€™æ˜¯ä¸€æ¬¡**é‡å¤§æ”¹ç‰ˆ**ï¼š

#### Zarr 2.xï¼ˆæ¨è–¦ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼‰

```
era5.zarr/
â”œâ”€â”€ .zattrs          # JSON æ ¼å¼
â”œâ”€â”€ .zgroup
â”œâ”€â”€ temperature/
â”‚   â”œâ”€â”€ .zarray      # é™£åˆ— metadata
â”‚   â””â”€â”€ 0.0.0        # chunk å‘½åï¼šç¶­åº¦ç´¢å¼•
```

#### Zarr 3.xï¼ˆV3 specï¼Œä»åœ¨ç©©å®šä¸­ï¼‰

```
era5.zarr/
â”œâ”€â”€ zarr.json        # æ–°çš„ metadata æ ¼å¼
â”œâ”€â”€ temperature/
â”‚   â”œâ”€â”€ zarr.json    # çµ±ä¸€æ ¼å¼
â”‚   â””â”€â”€ c/0/0/0      # æ–°çš„ chunk å‘½åï¼šc/ å‰ç¶´
```

### ä¸»è¦è®Šæ›´

| é …ç›® | Zarr 2.x | Zarr 3.x |
|------|----------|----------|
| Metadata æ ¼å¼ | `.zarray`, `.zattrs` | çµ±ä¸€çš„ `zarr.json` |
| Chunk å‘½å | `0.1.2` | `c/0/1/2` |
| Storage API | `store[key]` | æ–°çš„ abstract API |
| å£“ç¸®å™¨ | `numcodecs` | å¯æ’æ‹”çš„ codec pipeline |

### ç”Ÿæ…‹ç³»ç›¸å®¹æ€§ï¼ˆ2024-2025ï¼‰

| å¥—ä»¶ | Zarr 2.x | Zarr 3.x |
|------|----------|----------|
| **xarray** | âœ… å®Œå…¨æ”¯æ´ | âš ï¸ å¯¦é©—æ€§æ”¯æ´ |
| **dask** | âœ… å®Œå…¨æ”¯æ´ | âš ï¸ éƒ¨åˆ†åŠŸèƒ½æœ‰å•é¡Œ |
| **zarr-python** | âœ… ç©©å®š | âš ï¸ API ä»åœ¨æ¼”é€² |
| **fsspec** | âœ… ç©©å®š | âš ï¸ éœ€è¦æ›´æ–° |

### å¯¦éš›å•é¡Œæ¡ˆä¾‹

```python
# Zarr 3.0 å¯èƒ½é‡åˆ°çš„å•é¡Œ

# 1. Xarray è®€å–éŒ¯èª¤
ds = xr.open_zarr('data_v3.zarr')
# KeyError: 'Cannot find .zarray'

# 2. Dask å¯«å…¥å•é¡Œ
ds.to_zarr('output.zarr')
# ValueError: Zarr v3 not fully supported

# 3. Consolidated metadata å¤±æ•ˆ
xr.open_zarr('data.zarr', consolidated=True)
# åœ¨ v3 ä¸­ consolidated æ©Ÿåˆ¶æ”¹è®Š
```

### å»ºè­°åšæ³•

```toml
# pyproject.toml
[project]
dependencies = [
    "zarr>=2.18.0,<3.0.0",  # æ˜ç¢ºé–å®š 2.x
]
```

```python
# é©—è­‰ç‰ˆæœ¬
import zarr
assert int(zarr.__version__.split('.')[0]) < 3, "éœ€è¦ Zarr 2.x"
```

### ä½•æ™‚å¯ä»¥å‡ç´šåˆ° Zarr 3.xï¼Ÿ

ç­‰å¾…ä»¥ä¸‹æ¢ä»¶æˆç†Ÿï¼š
- âœ… Xarray å®˜æ–¹å®£å¸ƒå®Œå…¨æ”¯æ´
- âœ… Dask å®Œå…¨ç›¸å®¹
- âœ… ä½ çš„å…¶ä»–ç›¸ä¾å¥—ä»¶éƒ½å·²æ›´æ–°
- âœ… æœ‰å……åˆ†çš„æ¸¬è©¦èˆ‡é·ç§»è¨ˆç•«

**ç›®å‰ï¼ˆ2025 å¹´åˆï¼‰å»ºè­°ï¼šç”Ÿç”¢ç’°å¢ƒä½¿ç”¨ Zarr 2.x**

---

## 1.4 å¯¦ä½œï¼šè®€å–èˆ‡æ¢ç´¢ ERA5 è³‡æ–™

### è³‡æ–™èªªæ˜

æˆ‘å€‘ä½¿ç”¨çš„è³‡æ–™ï¼š
- **ä¾†æº**ï¼šERA5 Reanalysis
- **ç©ºé–“ç¯„åœ**ï¼š10Â°N-40Â°N, 100Â°E-140Â°Eï¼ˆæ±äº-è¥¿å¤ªå¹³æ´‹ï¼‰
- **æ™‚é–“ç¯„åœ**ï¼š2019-2023ï¼ˆ5 å¹´ï¼‰
- **è®Šæ•¸**ï¼šæº«åº¦ã€æ¿•åº¦ã€é¢¨å ´ã€å°æµåƒæ•¸ã€é™æ°´ç­‰

### æ­¥é©Ÿ 1: è®€å–å–®å¹´è³‡æ–™

```python
import xarray as xr
import dask
from dask.distributed import Client

# å•Ÿå‹• Dask Client
client = Client()
print(f"Dashboard: {client.dashboard_link}")

# è®€å– 2019 å¹´è³‡æ–™
ds = xr.open_zarr('/home/sungche/NAS/dataset/era5/era5_2019_10N40N_100E140E.zarr')

# æŸ¥çœ‹è³‡æ–™çµæ§‹
print(ds)
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
<xarray.Dataset>
Dimensions:  (time: 8760, latitude: 121, longitude: 161, level: 13)
Coordinates:
  * time       (time) datetime64[ns] 2019-01-01 ... 2019-12-31T23:00:00
  * latitude   (latitude) float32 40.0 39.75 39.5 ... 10.5 10.25 10.0
  * longitude  (longitude) float32 100.0 100.25 100.5 ... 139.5 139.75 140.0
  * level      (level) int32 1000 975 950 925 ... 500 400 300 200
Data variables:
    temperature                              (time, level, latitude, longitude) float32 dask.array<...>
    specific_humidity                        (time, level, latitude, longitude) float32 dask.array<...>
    u_component_of_wind                      (time, level, latitude, longitude) float32 dask.array<...>
    convective_available_potential_energy    (time, latitude, longitude) float32 dask.array<...>
    total_precipitation                      (time, latitude, longitude) float32 dask.array<...>
    ...
```

### é‡é»è§€å¯Ÿ

1. **`dask.array<...>`**ï¼šé€™è¡¨ç¤ºè³‡æ–™é‚„æ²’æœ‰çœŸæ­£è®€å…¥è¨˜æ†¶é«”ï¼ˆlazyï¼‰
2. **Dimensions**ï¼š4 å€‹ç¶­åº¦ï¼ˆæ™‚é–“ã€ç·¯åº¦ã€ç¶“åº¦ã€æ°£å£“å±¤ï¼‰
3. **Coordinates**ï¼šæ¯å€‹ç¶­åº¦éƒ½æœ‰åº§æ¨™å€¼

---

### æ­¥é©Ÿ 2: æŸ¥çœ‹ Chunking è³‡è¨Š

```python
# æŸ¥çœ‹ chunks
print(ds.chunks)

# æŸ¥çœ‹å–®ä¸€è®Šæ•¸çš„ chunks
print(ds['temperature'].chunks)
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
Frozen({'time': (24, 24, 24, ...),
        'latitude': (50, 50, 21),
        'longitude': (50, 50, 50, 11),
        'level': (13,)})
```

é€™è¡¨ç¤ºï¼š
- æ™‚é–“ç¶­åº¦ï¼šæ¯ 24 å°æ™‚ä¸€å€‹ chunkï¼ˆä¸€å¤©ï¼‰
- ç·¯åº¦ï¼šæ¯ 50 å€‹æ ¼é»ä¸€å€‹ chunk
- ç¶“åº¦ï¼šæ¯ 50 å€‹æ ¼é»ä¸€å€‹ chunk
- æ°£å£“å±¤ï¼šå…¨éƒ¨åœ¨ä¸€èµ·ï¼ˆ13 å±¤ï¼‰

### Chunking çš„é‡è¦æ€§

```
è‰¯å¥½çš„ chunkingï¼š
âœ… æ¯å€‹ chunk å¤§å°é©ä¸­ï¼ˆ10-100 MBï¼‰
âœ… ç¬¦åˆä½ çš„è®€å–æ¨¡å¼ï¼ˆæ™‚é–“åºåˆ—ï¼Ÿç©ºé–“åˆ‡ç‰‡ï¼Ÿï¼‰
âœ… å¹³è¡¡ä»»å‹™æ•¸é‡èˆ‡å‚³è¼¸é–‹éŠ·

ä¸è‰¯çš„ chunkingï¼š
âŒ å¤ªå°ï¼šç”¢ç”Ÿéå¤šä»»å‹™ï¼Œèª¿åº¦é–‹éŠ·å¤§
âŒ å¤ªå¤§ï¼šè¨˜æ†¶é«”å£“åŠ›å¤§ï¼Œå¹³è¡Œåº¦ä½
âŒ ä¸ç¬¦åˆè®€å–æ¨¡å¼ï¼šéœ€è¦è®€å–å¤§é‡ç„¡ç”¨è³‡æ–™
```

---

### æ­¥é©Ÿ 3: Lazy Evaluation ç¤ºç¯„

```python
# é¸å– 2019 å¹´ 6 æœˆï¼Œ850 hPa çš„æº«åº¦
temp_850 = ds['temperature'].sel(time='2019-06', level=850)

print(f"Type: {type(temp_850.data)}")  # dask.array.core.Array
print(f"Shape: {temp_850.shape}")      # (720, 121, 161)
print(f"Size: {temp_850.nbytes / 1e6:.2f} MB")  # ç´„ 14 MB

# é€™å€‹æ“ä½œç¬é–“å®Œæˆï¼å› ç‚ºæ˜¯ lazy
print("âœ… å·²å»ºç«‹ lazy operationï¼ˆé‚„æ²’çœŸæ­£è¨ˆç®—ï¼‰")
```

**è§€å¯Ÿ Dashboard**ï¼š
- Task Streamï¼šç›®å‰æ²’æœ‰ä»»ä½•ä»»å‹™åŸ·è¡Œ
- Memoryï¼šè¨˜æ†¶é«”ä½¿ç”¨æ²’æœ‰å¢åŠ 

---

### æ­¥é©Ÿ 4: è§¸ç™¼è¨ˆç®—

```python
import time

# è¨ˆç®—å¹³å‡æº«åº¦ï¼ˆé€™æœƒçœŸæ­£åŸ·è¡Œï¼‰
start = time.time()
temp_mean = temp_850.mean(dim='time').compute()
elapsed = time.time() - start

print(f"è¨ˆç®—å®Œæˆï¼è€—æ™‚ï¼š{elapsed:.2f} ç§’")
print(f"å¹³å‡æº«åº¦ï¼ˆç©ºé–“å ´ï¼‰shape: {temp_mean.shape}")  # (121, 161)
print(f"å°ç£é™„è¿‘ï¼ˆ25Â°N, 121Â°Eï¼‰çš„å¹³å‡æº«åº¦ï¼š{temp_mean.sel(latitude=25, longitude=121, method='nearest').values:.2f} K")
```

**è§€å¯Ÿ Dashboard**ï¼š
- Task Streamï¼šçœ‹åˆ°å¤§é‡è—è‰²é•·æ¢ï¼ˆä»»å‹™åŸ·è¡Œï¼‰
- Progressï¼šé€²åº¦æ¢è·‘åˆ° 100%
- Memoryï¼šè¨˜æ†¶é«”ä½¿ç”¨å¢åŠ å¾Œåˆé‡‹æ”¾

---

### æ­¥é©Ÿ 5: ç¹ªåœ–ï¼ˆå°è¨ˆç®—ï¼‰

```python
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature

# é¸å–å–®ä¸€æ™‚é–“é»
temp_snapshot = temp_850.isel(time=0)

# è¨ˆç®—ä¸¦ç¹ªåœ–
fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})

# ç¹ªè£½æº«åº¦å ´
temp_snapshot.plot(
    ax=ax,
    cmap='RdYlBu_r',
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Temperature (K)'}
)

# åŠ ä¸Šåœ°åœ–è¦ç´ 
ax.coastlines()
ax.add_feature(cfeature.BORDERS, linestyle=':')
ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)

ax.set_title('ERA5 Temperature at 850 hPa (2019-06-01 00:00)', fontsize=14)

plt.tight_layout()
plt.savefig('temp_850hPa.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… åœ–ç‰‡å·²å„²å­˜ï¼štemp_850hPa.png")
```

**è§€å¯Ÿ Dashboard**ï¼š
- åªæœ‰ä¸€å€‹å°ä»»å‹™ï¼ˆè®€å–ä¸€å€‹æ™‚é–“é»ï¼‰
- è¨˜æ†¶é«”ä½¿ç”¨å¾ˆå°

---

### æ­¥é©Ÿ 6: ç†è§£ Lazy vs Eager

```python
# Lazy æ“ä½œï¼ˆç¬é–“å®Œæˆï¼Œä¸è¨ˆç®—ï¼‰
lazy_result = ds['temperature'].mean(dim=['latitude', 'longitude'])
print(f"Lazy result type: {type(lazy_result.data)}")  # dask.array

# Eager æ“ä½œï¼ˆçœŸæ­£è¨ˆç®—ï¼‰
eager_result = lazy_result.compute()
print(f"Eager result type: {type(eager_result)}")  # numpy.ndarray

# æˆ–è€…ä½¿ç”¨ .load()ï¼ˆin-place computeï¼‰
loaded_result = lazy_result.load()
print(f"Loaded result type: {type(loaded_result.data)}")  # numpy.ndarray
```

---

## 1.5 Dashboard æ·±å…¥è§€å¯Ÿ

### å¯¦é©—ï¼šè§€å¯Ÿä¸åŒæ“ä½œçš„ Task Graph

```python
# å¯¦é©— 1: ç°¡å–®çš„å¹³å‡
result1 = ds['temperature'].mean(dim='time')
result1.compute()
# Dashboard è§€å¯Ÿï¼šç·šæ€§çš„ task graph

# å¯¦é©— 2: è¤‡é›œçš„è¨ˆç®—
result2 = (ds['temperature'] - ds['temperature'].mean(dim='time')) / ds['temperature'].std(dim='time')
result2.compute()
# Dashboard è§€å¯Ÿï¼šæ¨¹ç‹€çš„ task graphï¼ˆæœ‰ä¾è³´é—œä¿‚ï¼‰

# å¯¦é©— 3: å¤šè®Šæ•¸è¨ˆç®—
wind_speed = (ds['u_component_of_wind']**2 + ds['v_component_of_wind']**2)**0.5
wind_speed.mean().compute()
# Dashboard è§€å¯Ÿï¼šå¹³è¡Œçš„ task branches
```

### Dashboard å„é é¢èªªæ˜

#### Task Stream
- **æ©«è»¸**ï¼šæ™‚é–“
- **ç¸±è»¸**ï¼šä¸åŒçš„ workers
- **é¡è‰²**ï¼šä¸åŒé¡å‹çš„ä»»å‹™
  - è—è‰²ï¼šè¨ˆç®—ä»»å‹™
  - ç´…è‰²ï¼šè³‡æ–™å‚³è¼¸
  - ç¶ è‰²ï¼šåºåˆ—åŒ–/ååºåˆ—åŒ–

#### Progress
- å³æ™‚é€²åº¦æ¢
- å¯ä»¥çœ‹åˆ°å“ªäº›ä»»å‹™æ­£åœ¨åŸ·è¡Œ
- å®Œæˆçš„ä»»å‹™æ•¸é‡

#### Graph
- ä»»å‹™ç›¸ä¾é—œä¿‚åœ–
- å¯ä»¥çœ‹åˆ°å“ªäº›ä»»å‹™å¿…é ˆå…ˆå®Œæˆ

---

# Part 2: æ™‚ç©ºè³‡æ–™è™•ç†

## 2.1 å¤šæª”æ¡ˆè®€å–èˆ‡åˆä½µ

### æƒ…å¢ƒï¼šè®€å–å¤šå¹´è³‡æ–™

æˆ‘å€‘æœ‰ 2019-2023 å…± 5 å¹´çš„è³‡æ–™ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è®€å–ï¼Ÿ

```python
import glob
import xarray as xr

# æ–¹æ³• 1: ä½¿ç”¨ glob pattern
zarr_files = sorted(glob.glob('/home/sungche/NAS/dataset/era5/era5_*.zarr'))
print(f"æ‰¾åˆ° {len(zarr_files)} å€‹æª”æ¡ˆ")

# æ–¹æ³• 2: ä½¿ç”¨ xr.open_mfdatasetï¼ˆæ¨è–¦ï¼‰
ds_multi = xr.open_mfdataset(
    zarr_files,
    engine='zarr',
    parallel=True,  # å¹³è¡Œè®€å– metadata
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}  # çµ±ä¸€ chunking
)

print(f"åˆä½µå¾Œçš„æ™‚é–“ç¯„åœï¼š{ds_multi.time[0].values} åˆ° {ds_multi.time[-1].values}")
print(f"ç¸½è³‡æ–™é»æ•¸ï¼š{len(ds_multi.time)}")
```

---

### é‡è¦åƒæ•¸èªªæ˜

#### `parallel=True`

```python
# parallel=Trueï¼šå¹³è¡Œè®€å–å„æª”æ¡ˆçš„ metadata
# é€Ÿåº¦å¿«ï¼Œä½†è¦ç¢ºä¿æª”æ¡ˆçµæ§‹ä¸€è‡´

# parallel=Falseï¼šåºåˆ—è®€å–ï¼ˆé è¨­ï¼‰
# è¼ƒæ…¢ï¼Œä½†æ›´å®‰å…¨
```

#### `chunks` åƒæ•¸

```python
# æƒ…æ³ 1: è®“ Xarray è‡ªå‹•æ±ºå®šï¼ˆä½¿ç”¨æª”æ¡ˆåŸæœ¬çš„ chunksï¼‰
ds = xr.open_mfdataset(files, engine='zarr', chunks='auto')

# æƒ…æ³ 2: æ˜ç¢ºæŒ‡å®šï¼ˆæ¨è–¦ï¼Œç¢ºä¿ä¸€è‡´æ€§ï¼‰
ds = xr.open_mfdataset(
    files,
    engine='zarr',
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}
)

# æƒ…æ³ 3: è®€å…¥è¨˜æ†¶é«”ï¼ˆå°æª”æ¡ˆï¼‰
ds = xr.open_mfdataset(files, engine='zarr', chunks=None)
```

---

## 2.2 æ™‚ç©ºåˆ‡ç‰‡èˆ‡é‡æ¡æ¨£

### æ™‚é–“åˆ‡ç‰‡

#### åŸºæœ¬åˆ‡ç‰‡

```python
# é¸å–ç‰¹å®šå¹´ä»½
ds_2020 = ds_multi.sel(time='2020')

# é¸å–æ™‚é–“ç¯„åœ
ds_summer_2020 = ds_multi.sel(time=slice('2020-06', '2020-09'))

# é¸å–ç‰¹å®šæœˆä»½ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰
ds_june = ds_multi.sel(time=ds_multi.time.dt.month == 6)

# é¸å–å¤å­£ï¼ˆ6-9æœˆï¼Œæ‰€æœ‰å¹´ä»½ï¼‰
ds_summer = ds_multi.sel(time=ds_multi.time.dt.month.isin([6, 7, 8, 9]))

print(f"å¤å­£è³‡æ–™é»æ•¸ï¼š{len(ds_summer.time)}")
```

#### æ™‚é–“é‹ç®—

```python
# è¨ˆç®—æ¯æ—¥å¹³å‡
ds_daily = ds_summer.resample(time='1D').mean()

# è¨ˆç®—æ¯æœˆå¹³å‡
ds_monthly = ds_summer.resample(time='1MS').mean()  # MS = Month Start

# è¨ˆç®—æ¯å­£å¹³å‡
ds_seasonal = ds_summer.resample(time='QS-DEC').mean()  # QS-DEC = å­£åº¦é–‹å§‹ï¼ˆ12æœˆåˆ¶ï¼‰

print(f"åŸå§‹è³‡æ–™ï¼š{len(ds_summer.time)} ç­†")
print(f"æ—¥å¹³å‡ï¼š{len(ds_daily.time)} ç­†")
print(f"æœˆå¹³å‡ï¼š{len(ds_monthly.time)} ç­†")
```

---

### ç©ºé–“åˆ‡ç‰‡

#### åŸºæœ¬åˆ‡ç‰‡

```python
# é¸å–å°ç£å€åŸŸï¼ˆ22Â°N-25Â°N, 120Â°E-122Â°Eï¼‰
ds_taiwan = ds_summer.sel(
    latitude=slice(25, 22),    # æ³¨æ„ï¼šERA5 ç·¯åº¦æ˜¯éæ¸›çš„ï¼
    longitude=slice(120, 122)
)

# é¸å–å–®é»ï¼ˆæœ€è¿‘é„°æ’å€¼ï¼‰
ds_taipei = ds_summer.sel(
    latitude=25.03,
    longitude=121.56,
    method='nearest'
)

# é¸å–å¤šå€‹é»
lats = [25.03, 24.15, 22.63]  # å°åŒ—ã€å°ä¸­ã€é«˜é›„
lons = [121.56, 120.68, 120.30]
ds_cities = ds_summer.sel(
    latitude=xr.DataArray(lats, dims='city'),
    longitude=xr.DataArray(lons, dims='city'),
    method='nearest'
)
```

#### ç©ºé–“é‹ç®—

```python
# å€åŸŸå¹³å‡
taiwan_avg = ds_taiwan.mean(dim=['latitude', 'longitude'])

# åŠ æ¬Šå¹³å‡ï¼ˆè€ƒæ…®ç·¯åº¦ï¼‰
weights = np.cos(np.deg2rad(ds_taiwan.latitude))
weights.name = "weights"
taiwan_weighted_avg = ds_taiwan.weighted(weights).mean(dim=['latitude', 'longitude'])

# ç©ºé–“ç¸½å’Œï¼ˆå¦‚ï¼šç¸½é™é›¨é‡ï¼‰
taiwan_total_precip = ds_taiwan['total_precipitation'].sum(dim=['latitude', 'longitude'])
```

---

### çµ„åˆæ“ä½œç¯„ä¾‹

```python
# ç¯„ä¾‹ï¼šè¨ˆç®—å°ç£å¤å­£ï¼ˆ6-9æœˆï¼‰2019-2023 å¹´çš„æ—¥å¹³å‡æº«åº¦

# 1. æ™‚é–“åˆ‡ç‰‡ï¼šå¤å­£
ds_summer = ds_multi.sel(time=ds_multi.time.dt.month.isin([6, 7, 8, 9]))

# 2. ç©ºé–“åˆ‡ç‰‡ï¼šå°ç£
ds_tw_summer = ds_summer.sel(
    latitude=slice(25, 22),
    longitude=slice(120, 122)
)

# 3. é‡æ¡æ¨£ï¼šæ—¥å¹³å‡
ds_tw_daily = ds_tw_summer.resample(time='1D').mean()

# 4. ç©ºé–“å¹³å‡ï¼šå…¨å°å¹³å‡
tw_temp_daily = ds_tw_daily['temperature'].mean(dim=['latitude', 'longitude'])

# 5. è¨ˆç®—ï¼ˆé€™æ™‚æ‰çœŸæ­£åŸ·è¡Œï¼‰
result = tw_temp_daily.compute()

print(f"å°ç£å¤å­£æ—¥å¹³å‡æº«åº¦åºåˆ—ï¼š{result.shape}")
# çµæœï¼š(ç´„ 600 å¤©, 13 å€‹æ°£å£“å±¤)
```

---

## 2.3 è¨ˆç®—èˆ‡å„²å­˜å„ªåŒ–

### æ°£å€™å­¸è¨ˆç®—

#### è¨ˆç®— Climatologyï¼ˆæ°£å€™å¹³å‡å€¼ï¼‰

```python
# è¨ˆç®—æ¯å€‹æœˆçš„æ°£å€™å¹³å‡ï¼ˆå¤šå¹´å¹³å‡ï¼‰
climatology = ds_tw_summer.groupby('time.month').mean(dim='time')

print(climatology)
# ç¾åœ¨æœ‰ä¸€å€‹æ–°ç¶­åº¦ 'month'ï¼Œå€¼ç‚º 6, 7, 8, 9
```

#### è¨ˆç®— Anomalyï¼ˆè·å¹³ï¼‰

```python
# æ–¹æ³• 1: ä½¿ç”¨ groupbyï¼ˆæ¨è–¦ï¼‰
anomaly = ds_tw_summer.groupby('time.month') - climatology

# æ–¹æ³• 2: æ‰‹å‹•è¨ˆç®—
anomaly_manual = ds_tw_summer - climatology.sel(month=ds_tw_summer.time.dt.month)

print(anomaly)
# è·å¹³å ´ï¼šæ¯å€‹æ™‚é–“é»æ¸›å»è©²æœˆçš„æ°£å€™å¹³å‡å€¼
```

---

### å„²å­˜æˆ Zarr

#### åŸºæœ¬å„²å­˜

```python
# å„²å­˜è·å¹³è³‡æ–™
output_path = './taiwan_summer_anomaly.zarr'

anomaly.to_zarr(
    output_path,
    mode='w',              # 'w' = è¦†å¯«, 'a' = è¿½åŠ 
    consolidated=True      # åˆä½µ metadataï¼ˆé‡è¦ï¼åŠ é€Ÿè®€å–ï¼‰
)

print(f"âœ… å·²å„²å­˜åˆ°ï¼š{output_path}")
```

#### é€²éšï¼šå£“ç¸®è¨­å®š

```python
import zarr

# è¨­å®šå£“ç¸®ï¼ˆæ¨è–¦ï¼šBlosc + Zstdï¼‰
encoding = {}
for var in anomaly.data_vars:
    encoding[var] = {
        'compressor': zarr.Blosc(
            cname='zstd',    # å£“ç¸®æ¼”ç®—æ³•ï¼šzstdï¼ˆå¹³è¡¡é€Ÿåº¦èˆ‡å£“ç¸®ç‡ï¼‰
            clevel=3,        # å£“ç¸®ç­‰ç´šï¼š1-9ï¼ˆ3 æ˜¯å¥½çš„å¹³è¡¡é»ï¼‰
            shuffle=2        # Bit-shuffleï¼ˆå°æµ®é»æ•¸æœ‰æ•ˆï¼‰
        )
    }

# å„²å­˜
anomaly.to_zarr(
    output_path,
    mode='w',
    consolidated=True,
    encoding=encoding
)

# æ¯”è¼ƒå¤§å°
import os
import subprocess

# åŸå§‹å¤§å°ï¼ˆæœªå£“ç¸®ï¼‰
raw_size = anomaly.nbytes / 1e9
print(f"åŸå§‹å¤§å°ï¼š{raw_size:.2f} GB")

# å£“ç¸®å¾Œå¤§å°
compressed_size = float(subprocess.check_output(['du', '-sb', output_path]).split()[0]) / 1e9
print(f"å£“ç¸®å¾Œå¤§å°ï¼š{compressed_size:.2f} GB")
print(f"å£“ç¸®ç‡ï¼š{raw_size / compressed_size:.2f}x")
```

#### å£“ç¸®æ¼”ç®—æ³•é¸æ“‡

| å£“ç¸®å™¨ | é€Ÿåº¦ | å£“ç¸®ç‡ | é©ç”¨æƒ…å¢ƒ |
|--------|------|--------|----------|
| **Blosc-Zstd** | å¿« | é«˜ | **æ¨è–¦**ï¼šé€šç”¨ |
| **Blosc-LZ4** | æ¥µå¿« | ä¸­ | éœ€è¦æ¥µå¿«çš„ I/O |
| **Blosc-GZIP** | æ…¢ | é«˜ | é•·æœŸå„²å­˜ã€é »å¯¬æœ‰é™ |
| **ç„¡å£“ç¸®** | æœ€å¿« | ç„¡ | è‡¨æ™‚æª”æ¡ˆ |

---

### é‡æ–°è®€å–é©—è­‰

```python
import time

# æ¸¬è©¦è®€å–é€Ÿåº¦
start = time.time()
ds_reload = xr.open_zarr(output_path)
elapsed = time.time() - start

print(f"âœ… è®€å–å®Œæˆï¼è€—æ™‚ï¼š{elapsed:.4f} ç§’ï¼ˆå¹¾ä¹ç¬é–“ï¼‰")
print(ds_reload)

# é©—è­‰è³‡æ–™æ­£ç¢ºæ€§
assert ds_reload['temperature'].shape == anomaly['temperature'].shape
print("âœ… è³‡æ–™é©—è­‰é€šé")
```

---

## 2.4 Dashboard æ•ˆèƒ½åˆ†æ

### å¯¦é©— 1: ä¸åŒ Chunk Size çš„å½±éŸ¿

```python
import time

# æº–å‚™æ¸¬è©¦è³‡æ–™ï¼ˆé¸ä¸€å€‹å­é›†ï¼‰
ds_test = ds_multi.sel(time=slice('2019-01', '2019-12'))

# æ¸¬è©¦ä¸åŒçš„ chunk å¤§å°
chunk_configs = [
    {'time': 10, 'latitude': 20, 'longitude': 20},   # å° chunk
    {'time': 24, 'latitude': 50, 'longitude': 50},   # ä¸­ç­‰ chunk
    {'time': 100, 'latitude': 100, 'longitude': 100} # å¤§ chunk
]

results = []

for config in chunk_configs:
    # Rechunk
    ds_rechunked = ds_test.chunk(config)

    # åŸ·è¡Œç›¸åŒè¨ˆç®—ï¼šå…¨åŸŸå¹³å‡
    start = time.time()
    result = ds_rechunked['temperature'].mean().compute()
    elapsed = time.time() - start

    results.append({
        'config': config,
        'time': elapsed,
        'n_tasks': len(ds_rechunked['temperature'].__dask_graph__())
    })

    print(f"Chunk size {config}: {elapsed:.2f}s, {results[-1]['n_tasks']} tasks")

# è¦–è¦ºåŒ–
import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame(results)
df['config_str'] = df['config'].astype(str)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].bar(range(len(df)), df['time'])
axes[0].set_xticks(range(len(df)))
axes[0].set_xticklabels(['Small', 'Medium', 'Large'])
axes[0].set_ylabel('Time (s)')
axes[0].set_title('Computation Time')

axes[1].bar(range(len(df)), df['n_tasks'])
axes[1].set_xticks(range(len(df)))
axes[1].set_xticklabels(['Small', 'Medium', 'Large'])
axes[1].set_ylabel('Number of Tasks')
axes[1].set_title('Task Count')

plt.tight_layout()
plt.savefig('chunk_size_analysis.png', dpi=150)
plt.show()
```

**è§€å¯Ÿé‡é»**ï¼š
- å° chunkï¼šä»»å‹™å¤šï¼Œèª¿åº¦é–‹éŠ·å¤§
- å¤§ chunkï¼šä»»å‹™å°‘ï¼Œä½†æ¯å€‹ä»»å‹™è¨˜æ†¶é«”éœ€æ±‚é«˜
- ä¸­ç­‰ chunkï¼šé€šå¸¸æ˜¯æœ€ä½³å¹³è¡¡

---

### å¯¦é©— 2: è¨˜æ†¶é«”ä½¿ç”¨è§€å¯Ÿ

```python
# æ¸¬è©¦æœƒä¸æœƒ OOM çš„æ“ä½œ

# æƒ…æ³ 1: Lazy æ“ä½œï¼ˆä¸æœƒ OOMï¼‰
large_result = ds_multi['temperature'].mean(dim='time')
print(f"Lazy result created. Memory usage: {large_result.nbytes / 1e9:.2f} GB (è™›æ“¬)")

# æƒ…æ³ 2: Eager æ“ä½œï¼ˆå¯èƒ½ OOMï¼‰
try:
    large_result_eager = large_result.compute()
    print(f"Computed! Memory usage: {large_result_eager.nbytes / 1e9:.2f} GB")
except MemoryError:
    print("âŒ Out of Memory!")

# æƒ…æ³ 3: Persistï¼ˆè¼‰å…¥åˆ°åˆ†æ•£å¼è¨˜æ†¶é«”ï¼‰
large_result_persist = large_result.persist()
# é€™æœƒå°‡çµæœåˆ†æ•£å„²å­˜åœ¨ Dask workers çš„è¨˜æ†¶é«”ä¸­
print("âœ… Persisted to distributed memory")
```

**Dashboard è§€å¯Ÿ**ï¼š
- Memory é é¢ï¼šå¯ä»¥çœ‹åˆ°æ¯å€‹ worker çš„è¨˜æ†¶é«”ä½¿ç”¨
- `persist()` å¾Œè¨˜æ†¶é«”ä½¿ç”¨æœƒä¸Šå‡ä¸¦ç¶­æŒ

---

# Part 3: ML Pipeline å¯¦æˆ°

## 3.1 ä»»å‹™å®šç¾©ï¼šå°æµåˆ†é¡

### ç§‘å­¸èƒŒæ™¯

**å°æµ (Convection)** æ˜¯å¤§æ°£ä¸­é‡è¦çš„å‚ç›´é‹å‹•ï¼Œå¸¸ä¼´éš¨ï¼š
- å¼·é™é›¨
- é›·æš´
- å†°é›¹
- é¾æ²é¢¨

**é æ¸¬å°æµ**å°æ–¼ç½å®³é è­¦è‡³é—œé‡è¦ã€‚

---

### å°æµæŒ‡æ¨™

æˆ‘å€‘ä½¿ç”¨ä»¥ä¸‹åƒæ•¸ä¾†é æ¸¬å°æµï¼š

| åƒæ•¸ | ç¸®å¯« | æ„ç¾© | å…¸å‹å°æµå€¼ |
|------|------|------|------------|
| **Convective Available Potential Energy** | CAPE | å°æµå¯ç”¨ä½èƒ½ï¼ˆèƒ½é‡ï¼‰ | > 1000 J/kg |
| **Convective Inhibition** | CIN | å°æµæŠ‘åˆ¶ï¼ˆé˜»ç¤™ï¼‰ | < -50 J/kg |
| **K-Index** | KI | ç¶œåˆä¸ç©©å®šæŒ‡æ•¸ | > 30 |
| **Boundary Layer Height** | BLH | é‚Šç•Œå±¤é«˜åº¦ | > 1500 m |

---

### ä»»å‹™å®šç¾©

**Inputï¼ˆç‰¹å¾µï¼‰**ï¼š
- CAPE, CIN, K-Index, BLHï¼ˆ4 å€‹è®Šæ•¸ï¼‰
- ç©ºé–“ç¶­åº¦ï¼š16Ã—16 patches
- æ™‚é–“ç¶­åº¦ï¼š32 å€‹æ™‚é–“é»ä¸€æ‰¹

**Outputï¼ˆæ¨™ç±¤ï¼‰**ï¼š
- æ˜¯å¦ç™¼ç”Ÿå°æµï¼ˆäºŒå…ƒåˆ†é¡ï¼š0 æˆ– 1ï¼‰
- å®šç¾©ï¼š`total_precipitation > 5 mm/hr` è¦–ç‚ºå°æµ

**ç›®æ¨™**ï¼š
å»ºç«‹ä¸€å€‹è³‡æ–™ pipelineï¼Œå°‡ Zarr â†’ Xarray â†’ xbatcher â†’ PyTorch

---

### å¯¦ä½œï¼šæº–å‚™è³‡æ–™

```python
import xarray as xr
import numpy as np

# è®€å–è¨“ç·´è³‡æ–™ï¼ˆ2019-2020ï¼‰
train_files = [
    '/home/sungche/NAS/dataset/era5/era5_2019_10N40N_100E140E.zarr',
    '/home/sungche/NAS/dataset/era5/era5_2020_10N40N_100E140E.zarr'
]

ds_train = xr.open_mfdataset(
    train_files,
    engine='zarr',
    parallel=True,
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}
)

# é¸å–ç‰¹å¾µè®Šæ•¸
feature_vars = [
    'convective_available_potential_energy',
    'convective_inhibition',
    'k_index',
    'boundary_layer_height'
]

features = ds_train[feature_vars]

# å»ºç«‹æ¨™ç±¤ï¼ˆé™é›¨ > 5mm/hr å®šç¾©ç‚ºå°æµï¼‰
# ERA5 é™é›¨å–®ä½é€šå¸¸æ˜¯ç´¯ç©é‡ï¼Œéœ€è¦è½‰æ›æˆ mm/hr
precip_threshold = 5  # mm/hr

label = (ds_train['total_precipitation'] > precip_threshold).astype(int)
label.name = 'convection_flag'

# åˆä½µæˆè¨“ç·´è³‡æ–™é›†
train_ds = xr.merge([features, label])

print("Training dataset:")
print(train_ds)
print(f"\nå°æµäº‹ä»¶æ¯”ä¾‹ï¼š{label.mean().compute().values * 100:.2f}%")
```

---

### æº–å‚™é©—è­‰è³‡æ–™

```python
# è®€å–é©—è­‰è³‡æ–™ï¼ˆ2021ï¼‰
ds_valid = xr.open_zarr('/home/sungche/NAS/dataset/era5/era5_2021_10N40N_100E140E.zarr')

valid_ds = xr.merge([
    ds_valid[feature_vars],
    (ds_valid['total_precipitation'] > precip_threshold).astype(int).rename('convection_flag')
])

print("Validation dataset:")
print(valid_ds)
```

---

## 3.2 xbatcherï¼šæ‰¹æ¬¡åˆ‡å‰²

> ğŸ’¡ **æ ¸å¿ƒå•é¡Œ**ï¼šå¦‚ä½•æŠŠ TB ç´šçš„ Xarray è³‡æ–™é¤µçµ¦ PyTorchï¼Ÿ
>
> **ç­”æ¡ˆ**ï¼šä½¿ç”¨ `xbatcher` åˆ‡æˆå°æ‰¹æ¬¡ï¼Œlazy è®€å–

### xbatcher ç°¡ä»‹

**xbatcher** æ˜¯å°ˆç‚º Xarray è¨­è¨ˆçš„æ‰¹æ¬¡ç”¢ç”Ÿå™¨ï¼š
- è‡ªå‹•åˆ‡å‰²ç©ºé–“/æ™‚é–“ç¶­åº¦
- æ”¯æ´ overlapï¼ˆé¿å…é‚Šç•Œæ•ˆæ‡‰ï¼‰
- å®Œå…¨ lazyï¼ˆä¸æœƒä¸€æ¬¡è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼‰
- èˆ‡ Dask å®Œç¾æ•´åˆ

---

### åŸºæœ¬ä½¿ç”¨

```python
import xbatcher

# å»ºç«‹ BatchGenerator
bgen = xbatcher.BatchGenerator(
    train_ds,
    input_dims={'latitude': 16, 'longitude': 16},     # ç©ºé–“ patch size
    input_overlap={'latitude': 4, 'longitude': 4},    # 50% overlap
    batch_dims={'time': 32}                            # æ™‚é–“æ‰¹æ¬¡å¤§å°
)

print(f"Total batches: {len(bgen)}")

# æŸ¥çœ‹ç¬¬ä¸€å€‹ batch
for i, batch in enumerate(bgen):
    print(f"\n--- Batch {i} ---")
    print(batch)
    print(f"CAPE shape: {batch['convective_available_potential_energy'].shape}")
    print(f"Label shape: {batch['convection_flag'].shape}")

    if i == 0:  # åªçœ‹ç¬¬ä¸€å€‹
        break
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
Total batches: 1250

--- Batch 0 ---
<xarray.Dataset>
Dimensions:  (time: 32, latitude: 16, longitude: 16)
Data variables:
    convective_available_potential_energy  (time, latitude, longitude) float32 dask.array<...>
    convective_inhibition                   (time, latitude, longitude) float32 dask.array<...>
    k_index                                 (time, latitude, longitude) float32 dask.array<...>
    boundary_layer_height                   (time, latitude, longitude) float32 dask.array<...>
    convection_flag                         (time, latitude, longitude) int32 dask.array<...>

CAPE shape: (32, 16, 16)
Label shape: (32, 16, 16)
```

---

### åƒæ•¸è©³è§£

#### `input_dims`

```python
# ç©ºé–“ patch çš„å¤§å°
input_dims={'latitude': 16, 'longitude': 16}

# å¦‚ä½•é¸æ“‡ï¼Ÿ
# - å¤ªå°ï¼ˆå¦‚ 4Ã—4ï¼‰ï¼šcontext ä¸è¶³ï¼Œæ¨¡å‹é›£å­¸ç¿’
# - å¤ªå¤§ï¼ˆå¦‚ 128Ã—128ï¼‰ï¼šè¨˜æ†¶é«”éœ€æ±‚é«˜ï¼Œbatch size å—é™
# - æ¨è–¦ï¼š16Ã—16 åˆ° 64Ã—64
```

#### `input_overlap`

```python
# Patch ä¹‹é–“çš„é‡ç–Š
input_overlap={'latitude': 4, 'longitude': 4}  # 25% overlap

# ç‚ºä»€éº¼éœ€è¦ overlapï¼Ÿ
# - é¿å…é‚Šç•Œæ•ˆæ‡‰ï¼ˆCNN åœ¨é‚Šç•Œçš„é æ¸¬è¼ƒå·®ï¼‰
# - å¢åŠ è¨“ç·´è³‡æ–™é‡ï¼ˆdata augmentationï¼‰
# - è®“ç›¸é„° patch ä¹‹é–“æœ‰é€£çºŒæ€§
```

#### `batch_dims`

```python
# åœ¨æ™‚é–“ç¶­åº¦ä¸Šæ‰¹æ¬¡åŒ–
batch_dims={'time': 32}

# æ³¨æ„ï¼š
# - é€™æœƒç”¢ç”Ÿ 32 å€‹æ™‚é–“é»çš„åºåˆ—
# - å¦‚æœä½ çš„æ¨¡å‹ä¸è™•ç†æ™‚åºï¼Œå¯ä»¥ä¹‹å¾Œå†å¹³å‡æˆ–é¸å–
# - ä¹Ÿå¯ä»¥è¨­å®šå…¶ä»–ç¶­åº¦ï¼Œå¦‚ {'latitude': 4}ï¼ˆä½†è¼ƒå°‘è¦‹ï¼‰
```

---

### è½‰æ›æˆ NumPy

```python
# å–å¾—ä¸€å€‹ batch
batch = next(iter(bgen))

# æ–¹æ³• 1: to_arrayï¼ˆæ¨è–¦ï¼‰
# å°‡å¤šå€‹è®Šæ•¸åˆä½µæˆä¸€å€‹æ–°ç¶­åº¦ 'variable'
X = batch[feature_vars].to_array(dim='variable').values
y = batch['convection_flag'].values

print(f"X shape: {X.shape}")  # (4, 32, 16, 16) = (variables, time, lat, lon)
print(f"y shape: {y.shape}")  # (32, 16, 16) = (time, lat, lon)

# æ–¹æ³• 2: æ‰‹å‹• stack
X_manual = np.stack([batch[var].values for var in feature_vars], axis=0)
```

---

### é€²éšï¼šå‹•æ…‹æ‰¹æ¬¡ï¼ˆIterableï¼‰

```python
# å¦‚æœè³‡æ–™å¤ªå¤§ï¼Œä¸æƒ³é å…ˆç”Ÿæˆæ‰€æœ‰ batch ç´¢å¼•
# å¯ä»¥ç›´æ¥è¿­ä»£ï¼ˆæ›´çœè¨˜æ†¶é«”ï¼‰

for i, batch in enumerate(bgen):
    # è™•ç† batch
    X = batch[feature_vars].to_array(dim='variable').values
    y = batch['convection_flag'].values

    print(f"Batch {i}: X={X.shape}, y={y.shape}")

    if i >= 5:  # åªçœ‹å‰ 5 å€‹
        break

# é€™æ¨£åšçš„å¥½è™•ï¼š
# - ä¸æœƒä¸€æ¬¡ç”Ÿæˆæ‰€æœ‰ batch çš„ç´¢å¼•
# - è¨˜æ†¶é«”ä½¿ç”¨æ›´å°‘
# - é©åˆè¶…å¤§è³‡æ–™é›†
```

---

## 3.3 PyTorch DataLoader æ©‹æ¥

> ğŸ’¡ **xbatcher çš„å…©éšæ®µè¨­è¨ˆ**ï¼š
>
> 1. **Stage 1**ï¼šç”¨ `BatchGenerator` å®šç¾©å¦‚ä½•åˆ‡æ‰¹æ¬¡
> 2. **Stage 2**ï¼šç”¨ `xbatcher.loaders.torch.MapDataset` åŒ…è£æˆ PyTorch Dataset
>
> **ä¸éœ€è¦è‡ªå·±å¯« Dataset wrapperï¼** xbatcher å·²ç¶“æä¾›äº†å®Œæ•´çš„ PyTorch æ•´åˆã€‚

---

### xbatcher æä¾›çš„ PyTorch æ•´åˆ

xbatcher æä¾›å…©ç¨® PyTorch Dataset ä»‹é¢ï¼š

| é¡åˆ¥ | ç”¨é€” | ç‰¹æ€§ |
|------|------|------|
| **MapDataset** | å¯ç´¢å¼•å­˜å–ï¼ˆæ¨è–¦ï¼‰ | æ”¯æ´ `dataset[idx]`ï¼Œå¯ shuffle |
| **IterableDataset** | ä¸²æµå­˜å– | åªèƒ½è¿­ä»£ï¼Œé©åˆè¶…å¤§è³‡æ–™é›† |

---

### æ­£ç¢ºçš„å¯¦ä½œæ–¹å¼

#### æ­¥é©Ÿ 1: åˆ†åˆ¥å»ºç«‹ç‰¹å¾µå’Œæ¨™ç±¤çš„ BatchGenerator

```python
import xbatcher
import xbatcher.loaders.torch
from torch.utils.data import DataLoader

# Stage 1: å»ºç«‹ BatchGeneratorï¼ˆåˆ†é–‹ç‰¹å¾µå’Œæ¨™ç±¤ï¼‰

# ç‰¹å¾µ BatchGenerator
X_bgen = xbatcher.BatchGenerator(
    train_ds[feature_vars],  # åªé¸å–ç‰¹å¾µè®Šæ•¸
    input_dims={'latitude': 16, 'longitude': 16},
    input_overlap={'latitude': 4, 'longitude': 4},
    batch_dims={'time': 32},
    preload_batch=False  # ä¿æŒ lazyï¼ˆé‡è¦ï¼ï¼‰
)

# æ¨™ç±¤ BatchGenerator
y_bgen = xbatcher.BatchGenerator(
    train_ds['convection_flag'],  # åªé¸å–æ¨™ç±¤è®Šæ•¸
    input_dims={'latitude': 16, 'longitude': 16},
    input_overlap={'latitude': 4, 'longitude': 4},
    batch_dims={'time': 32},
    preload_batch=False
)

print(f"âœ… Created {len(X_bgen)} batches")
```

---

#### æ­¥é©Ÿ 2: ä½¿ç”¨ xbatcher.loaders.torch.MapDataset

```python
# Stage 2: åŒ…è£æˆ PyTorch Dataset
dataset = xbatcher.loaders.torch.MapDataset(
    X_bgen,  # ç‰¹å¾µ generator
    y_bgen   # æ¨™ç±¤ generator
)

print(f"âœ… MapDataset created with {len(dataset)} samples")

# æ¸¬è©¦å–®ä¸€æ¨£æœ¬
X_sample, y_sample = dataset[0]
print(f"X shape: {X_sample.shape}, dtype: {X_sample.dtype}")
print(f"y shape: {y_sample.shape}, dtype: {y_sample.dtype}")
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
âœ… Created 1250 batches
âœ… MapDataset created with 1250 samples
X shape: torch.Size([4, 32, 16, 16]), dtype: torch.float32
y shape: torch.Size([32, 16, 16]), dtype: torch.int64
```

**é‡é»**ï¼š
- `X_sample` å·²ç¶“æ˜¯ `torch.Tensor`ï¼ˆä¸æ˜¯ xarrayï¼ï¼‰
- å¤šè®Šæ•¸è‡ªå‹•åˆä½µæˆç¬¬ä¸€å€‹ç¶­åº¦ï¼ˆ4 å€‹è®Šæ•¸ï¼‰
- å®Œå…¨è‡ªå‹•ï¼Œä¸éœ€è¦æ‰‹å‹•è½‰æ›

---

#### æ­¥é©Ÿ 3: å»ºç«‹ DataLoader

```python
# å»ºç«‹ PyTorch DataLoader
train_loader = DataLoader(
    dataset,
    batch_size=None,  # âš ï¸ é‡è¦ï¼xbatcher å·²å®šç¾© batch size
    shuffle=True,
    num_workers=4,
    persistent_workers=True,
    prefetch_factor=3,
    multiprocessing_context='forkserver'  # æ¨è–¦ç”¨æ–¼ xarray/dask
)

print(f"âœ… DataLoader ready for training")

# æ¸¬è©¦è¿­ä»£
for X_batch, y_batch in train_loader:
    print(f"Batch X: {X_batch.shape}")
    print(f"Batch y: {y_batch.shape}")
    break
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
âœ… DataLoader ready for training
Batch X: torch.Size([4, 32, 16, 16])  # (vars, time, lat, lon)
Batch y: torch.Size([32, 16, 16])      # (time, lat, lon)
```

---

### é‡è¦åƒæ•¸èªªæ˜

#### `batch_size=None`

```python
# âš ï¸ é—œéµå·®ç•°ï¼

# éŒ¯èª¤åšæ³•ï¼š
train_loader = DataLoader(dataset, batch_size=4)
# é€™æœƒå†æ¬¡æ‰¹æ¬¡åŒ–ï¼Œå°è‡´å½¢ç‹€éŒ¯èª¤

# æ­£ç¢ºåšæ³•ï¼š
train_loader = DataLoader(dataset, batch_size=None)
# xbatcher çš„ BatchGenerator å·²ç¶“å®šç¾©äº† batch size
```

#### `preload_batch`

```python
# BatchGenerator åƒæ•¸

# preload_batch=Falseï¼ˆæ¨è–¦ï¼‰
# - ä¿æŒ lazy evaluation
# - ç¯€çœè¨˜æ†¶é«”
# - è®“ Dask æ§åˆ¶è¨ˆç®—æ™‚æ©Ÿ

# preload_batch=True
# - æå‰è¼‰å…¥æ•´å€‹ batch
# - å¯èƒ½å°è‡´ OOM
```

#### `multiprocessing_context`

```python
# multiprocessing_context='forkserver'ï¼ˆæ¨è–¦ç”¨æ–¼ xarray/daskï¼‰
# - é¿å… Dask client çš„ pickle å•é¡Œ
# - æ›´å®‰å…¨çš„ worker å»ºç«‹æ–¹å¼

# å…¶ä»–é¸é …ï¼š
# - 'fork'ï¼ˆLinux é è¨­ï¼Œä½†å¯èƒ½æœ‰å•é¡Œï¼‰
# - 'spawn'ï¼ˆWindows é è¨­ï¼‰
```

#### `num_workers`

```python
# num_workers=4ï¼ˆæ¨è–¦ï¼‰
# - å¹³è¡Œè®€å– 4 å€‹ batch
# - é…åˆ prefetch_factor å¯ä»¥é è¼‰è³‡æ–™

# æ³¨æ„ï¼š
# - å¤ªå¤š workers æœƒä½”ç”¨è¨˜æ†¶é«”
# - é…åˆ persistent_workers=True åŠ é€Ÿ
```

#### `prefetch_factor`

```python
# prefetch_factor=3
# - æ¯å€‹ worker é è¼‰ 3 å€‹ batch
# - æ¸›å°‘ GPU ç­‰å¾…æ™‚é–“
# - æ¬Šè¡¡ï¼šè¨˜æ†¶é«” vs é€Ÿåº¦
```

---

### å®Œæ•´ç¯„ä¾‹ï¼šè¨“ç·´èˆ‡é©—è­‰

```python
# å»ºç«‹è¨“ç·´ DataLoader
X_train_bgen = xbatcher.BatchGenerator(
    train_ds[feature_vars],
    input_dims={'latitude': 16, 'longitude': 16},
    batch_dims={'time': 32},
    preload_batch=False
)
y_train_bgen = xbatcher.BatchGenerator(
    train_ds['convection_flag'],
    input_dims={'latitude': 16, 'longitude': 16},
    batch_dims={'time': 32},
    preload_batch=False
)

train_dataset = xbatcher.loaders.torch.MapDataset(X_train_bgen, y_train_bgen)
train_loader = DataLoader(
    train_dataset,
    batch_size=None,
    shuffle=True,
    num_workers=4,
    persistent_workers=True,
    multiprocessing_context='forkserver'
)

# å»ºç«‹é©—è­‰ DataLoaderï¼ˆä¸ shuffleï¼‰
X_valid_bgen = xbatcher.BatchGenerator(
    valid_ds[feature_vars],
    input_dims={'latitude': 16, 'longitude': 16},
    batch_dims={'time': 32},
    preload_batch=False
)
y_valid_bgen = xbatcher.BatchGenerator(
    valid_ds['convection_flag'],
    input_dims={'latitude': 16, 'longitude': 16},
    batch_dims={'time': 32},
    preload_batch=False
)

valid_dataset = xbatcher.loaders.torch.MapDataset(X_valid_bgen, y_valid_bgen)
valid_loader = DataLoader(
    valid_dataset,
    batch_size=None,
    shuffle=False,  # é©—è­‰æ™‚ä¸æ‰“äº‚
    num_workers=4,
    persistent_workers=True,
    multiprocessing_context='forkserver'
)

print(f"âœ… Train loader: {len(train_loader)} batches")
print(f"âœ… Valid loader: {len(valid_loader)} batches")
```

---

### Dashboard è§€å¯Ÿ

åŸ·è¡Œä»¥ä¸‹ç¨‹å¼ç¢¼ï¼Œè§€å¯Ÿ Dashboardï¼š

```python
# è¿­ä»£å¹¾å€‹ batchï¼Œè§€å¯Ÿ Dashboard
for i, (X, y) in enumerate(train_loader):
    print(f"Batch {i}: X={X.shape}, y={y.shape}")

    if i >= 5:
        break
```

**è§€å¯Ÿé‡é»**ï¼š
- **Task Stream**ï¼šçœ‹åˆ° Dask çš„è³‡æ–™è®€å–ä»»å‹™ï¼ˆè—è‰²ï¼‰
- **Workers**ï¼š`num_workers=4` æ™‚ï¼Œæœƒçœ‹åˆ°å¤šå€‹ workers å¹³è¡Œå·¥ä½œ
- **Memory**ï¼šè¨˜æ†¶é«”ä½¿ç”¨æœƒæ³¢å‹•ï¼ˆlazy load â†’ compute â†’ é‡‹æ”¾ï¼‰
- **Prefetch**ï¼š`prefetch_factor=3` æœƒè®“ worker æå‰è¼‰å…¥è³‡æ–™

---

### ç‚ºä»€éº¼é€™æ¨£åšæ›´å¥½ï¼Ÿ

**èˆŠåšæ³•ï¼ˆæ‰‹å¯« Datasetï¼‰çš„å•é¡Œ**ï¼š
- âŒ éœ€è¦æ‰‹å‹•è™•ç† xarray â†’ NumPy â†’ Tensor è½‰æ›
- âŒ éœ€è¦æ‰‹å‹•è™•ç† NaN å€¼
- âŒ éœ€è¦æ‰‹å‹•è™•ç†å¤šè®Šæ•¸åˆä½µ
- âŒ å®¹æ˜“å‡ºéŒ¯ï¼Œé›£ä»¥ç¶­è­·

**xbatcher.loaders.torch çš„å„ªå‹¢**ï¼š
- âœ… è‡ªå‹•è™•ç†æ‰€æœ‰è½‰æ›ï¼ˆxarray â†’ Tensorï¼‰
- âœ… è‡ªå‹•åˆä½µå¤šè®Šæ•¸æˆç¬¬ä¸€å€‹ç¶­åº¦
- âœ… å®Œæ•´æ”¯æ´ Dask lazy evaluation
- âœ… ç¶“éå……åˆ†æ¸¬è©¦ï¼Œç©©å®šå¯é 
- âœ… ç¨‹å¼ç¢¼ç°¡æ½”ï¼Œæ˜“æ–¼ç¶­è­·

---

## 3.4 æ¨¡å‹è¨“ç·´ï¼ˆç°¡åŒ–ç¤ºç¯„ï¼‰

> âš ï¸ **é‡é»æé†’**ï¼š
>
> é€™éƒ¨åˆ†åªæ˜¯ç¤ºç¯„ã€Œè³‡æ–™ pipeline é€šäº†ã€ï¼Œä¸æ·±å…¥è¬›è§£æ¨¡å‹è¨“ç·´æŠ€å·§ã€‚
> æ¨¡å‹æ¶æ§‹ã€è¨“ç·´èª¿åƒæ˜¯å¦ä¸€å ‚èª²çš„å…§å®¹ã€‚

### ç°¡å–®çš„ CNN æ¨¡å‹

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleConvNet(nn.Module):
    """
    è¶…ç´šç°¡å–®çš„ CNNï¼ˆåƒ…ç”¨æ–¼ç¤ºç¯„ï¼‰

    Input: (batch, 4, 32, 16, 16)  # (batch, vars, time, lat, lon)
    Output: (batch, 2)              # (batch, num_classes)
    """

    def __init__(self, in_channels=4, num_classes=2):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 16Ã—16 â†’ 8Ã—8

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # å…¨å±€å¹³å‡æ± åŒ– â†’ 1Ã—1
        )

        self.classifier = nn.Linear(64, num_classes)

    def forward(self, x):
        # x: (batch, vars, time, lat, lon)

        # ç°¡åŒ–ï¼šå°æ™‚é–“ç¶­åº¦å–å¹³å‡
        x = x.mean(dim=2)  # â†’ (batch, vars, lat, lon)

        # CNN
        x = self.features(x)  # â†’ (batch, 64, 1, 1)
        x = x.view(x.size(0), -1)  # â†’ (batch, 64)

        # åˆ†é¡
        x = self.classifier(x)  # â†’ (batch, 2)

        return x
```

---

### è¨“ç·´ä¸€å€‹ Batchï¼ˆç¤ºç¯„ï¼‰

```python
import torch.optim as optim

# å»ºç«‹æ¨¡å‹
model = SimpleConvNet(in_channels=len(feature_vars), num_classes=2)
model = model.cuda()  # ç§»åˆ° GPU

# å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è¨“ç·´ä¸€å€‹ batchï¼ˆåªæ˜¯ç¤ºç¯„ï¼ï¼‰
model.train()

for X, y in train_loader:
    # ç§»åˆ° GPU
    X = X.cuda()
    y = y.cuda()

    # ç°¡åŒ–æ¨™ç±¤ï¼ˆå–ç©ºé–“å¹³å‡å¾ŒäºŒå€¼åŒ–ï¼‰
    # y shape: (batch, time, lat, lon) â†’ (batch,)
    y_simple = (y.float().mean(dim=[1, 2, 3]) > 0.5).long()

    # å‰å‘å‚³æ’­
    outputs = model(X)
    loss = criterion(outputs, y_simple)

    # åå‘å‚³æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"âœ… Loss: {loss.item():.4f}")
    print(f"âœ… Pipeline é€šäº†ï¼è³‡æ–™èƒ½é †åˆ©å¾ Zarr â†’ Xarray â†’ PyTorch")
    break  # åªè·‘ä¸€å€‹ batch
```

**é æœŸè¼¸å‡º**ï¼š
```
âœ… Loss: 0.6931
âœ… Pipeline é€šäº†ï¼è³‡æ–™èƒ½é †åˆ©å¾ Zarr â†’ Xarray â†’ PyTorch
```

---

### è¬›è§£é‡é»

è·Ÿå­¸å“¡å¼·èª¿ï¼š

1. **é€™åªæ˜¯è­‰æ˜ pipeline å¯ä»¥é‹ä½œ**
   - è³‡æ–™èƒ½å¾ Zarr è®€å–
   - ç¶“é Xarray è™•ç†
   - é€é xbatcher åˆ‡æ‰¹æ¬¡
   - é€²å…¥ PyTorch æ¨¡å‹

2. **æ¨¡å‹æ¶æ§‹ä¸é‡è¦**
   - ç”šè‡³å¯ä»¥ç”¨ `torchvision.models.resnet18`
   - é‡é»æ˜¯ã€Œè³‡æ–™æµã€ï¼Œä¸æ˜¯ã€Œæ¨¡å‹è¨­è¨ˆã€

3. **çœŸæ­£çš„è¨“ç·´æ˜¯å¦ä¸€å ‚èª²**
   - Learning rate èª¿æ•´
   - Regularization
   - Data augmentation
   - é€™äº›éƒ½ä¸åœ¨æœ¬èª²ç¨‹ç¯„åœ

---

## 3.5 é æ¸¬çµæœ â†’ Xarray

> ğŸ’¡ **æ ¸å¿ƒå•é¡Œ**ï¼šæ¨¡å‹è¼¸å‡ºæ˜¯ Tensor/NumPyï¼Œå¦‚ä½•è½‰å›å¸¶åº§æ¨™çš„ Xarrayï¼Ÿ
>
> **ç­”æ¡ˆ**ï¼šæ‰‹å‹•å»ºç«‹ `xr.DataArray`ï¼Œä¿ç•™åŸå§‹åº§æ¨™è³‡è¨Š

### ç‚ºä»€éº¼éœ€è¦è½‰å› Xarrayï¼Ÿ

å‚³çµ± ML workflowï¼š
```
Model output: numpy array (n_samples,)
è©•ä¼°: sklearn.metrics.accuracy_score(y_true, y_pred)
çµæœ: 0.85ï¼ˆåªæœ‰ä¸€å€‹æ•¸å­—ï¼‰
```

ç§‘å­¸è³‡æ–™ workflowï¼š
```
Model output: xarray.DataArray with coords (time, lat, lon)
è©•ä¼°: xskillscore.rmse(pred, obs, dim='time')
çµæœ: RMSE at every (lat, lon) pointï¼ˆä¸€å€‹ç©ºé–“å ´ï¼‰
```

**å·®ç•°**ï¼š
- å‚³çµ±æ–¹æ³•ï¼šåªçŸ¥é“ã€Œæ•´é«”æº–ç¢ºç‡ 85%ã€
- ç§‘å­¸æ–¹æ³•ï¼šçŸ¥é“ã€Œå°ç£åŒ—éƒ¨é æ¸¬å¥½ï¼Œå—éƒ¨é æ¸¬å·®ã€

---

### å¯¦ä½œï¼šç©ºé–“é æ¸¬

```python
# å»ºç«‹é©—è­‰ Datasetï¼ˆä¸ shuffleï¼‰
valid_dataset = XarrayDataset(
    valid_ds,
    feature_vars=feature_vars,
    label_var='convection_flag',
    batch_config=batch_config
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=1,
    shuffle=False,  # é‡è¦ï¼ä¿æŒé †åº
    num_workers=2
)

# é æ¸¬
model.eval()
predictions = []

with torch.no_grad():
    for idx, (X, y) in enumerate(valid_loader):
        X = X.cuda()

        # é æ¸¬
        outputs = model(X)
        pred = outputs.argmax(dim=1).cpu().numpy()  # (batch,)

        # é€™è£¡åªæœ‰ä¸€å€‹é æ¸¬å€¼ï¼ˆå› ç‚ºæˆ‘å€‘ç°¡åŒ–äº†æ¨™ç±¤ï¼‰
        # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œä½ å¯èƒ½æƒ³ä¿ç•™ç©ºé–“ç¶­åº¦

        predictions.append(pred)

        if idx >= 100:  # åªé æ¸¬å‰ 100 å€‹ batchï¼ˆç¤ºç¯„ï¼‰
            break

# åˆä½µé æ¸¬
pred_array = np.concatenate(predictions, axis=0)
print(f"Predictions shape: {pred_array.shape}")  # (101,)
```

---

### å¯¦ä½œï¼šä¿ç•™ç©ºé–“è³‡è¨Šçš„é æ¸¬

å¦‚æœä½ æƒ³è¦ç©ºé–“åˆ†ä½ˆçš„é æ¸¬ï¼ˆè€Œä¸æ˜¯å–®ä¸€å€¼ï¼‰ï¼Œéœ€è¦ä¿®æ”¹æ¨¡å‹ï¼š

```python
class SpatialConvNet(nn.Module):
    """
    è¼¸å‡ºç©ºé–“é æ¸¬çš„ CNN

    Input: (batch, 4, 32, 16, 16)
    Output: (batch, 2, 16, 16)  # ä¿ç•™ç©ºé–“ç¶­åº¦
    """

    def __init__(self, in_channels=4, num_classes=2):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, num_classes, 1)  # 1Ã—1 convï¼Œä¿ç•™ç©ºé–“
        )

    def forward(self, x):
        # x: (batch, vars, time, lat, lon)
        x = x.mean(dim=2)  # â†’ (batch, vars, lat, lon)
        x = self.features(x)  # â†’ (batch, num_classes, lat, lon)
        return x

# ä½¿ç”¨é€™å€‹æ¨¡å‹é æ¸¬
spatial_model = SpatialConvNet().cuda()
# ... è¨“ç·´éç¨‹é¡ä¼¼ ...

# é æ¸¬æ™‚ï¼Œä¿ç•™ç©ºé–“è³‡è¨Š
spatial_model.eval()
spatial_predictions = []
coords_list = []

with torch.no_grad():
    for idx, (X, y) in enumerate(valid_loader):
        X = X.cuda()

        outputs = spatial_model(X)  # (batch, 2, lat, lon)
        pred = outputs.argmax(dim=1).cpu().numpy()  # (batch, lat, lon)

        # å–å¾—å°æ‡‰çš„åº§æ¨™
        batch_data = valid_dataset.batches[idx]

        # å»ºç«‹ DataArray
        pred_da = xr.DataArray(
            pred[0],  # å–ç¬¬ä¸€å€‹ï¼ˆå› ç‚º batch_size=1ï¼‰
            coords={
                'latitude': batch_data.latitude,
                'longitude': batch_data.longitude
            },
            dims=['latitude', 'longitude']
        )

        spatial_predictions.append(pred_da)

        if idx >= 100:
            break

# åˆä½µæ‰€æœ‰ patchï¼ˆéœ€è¦è™•ç† overlapï¼‰
# é€™æ˜¯ä¸€å€‹é€²éšè©±é¡Œï¼Œé€™è£¡å…ˆç°¡åŒ–
print(f"âœ… æ”¶é›†äº† {len(spatial_predictions)} å€‹ç©ºé–“é æ¸¬")
```

---

## 3.6 xskillscore ç©ºé–“é©—è­‰

> ğŸ’¡ **xskillscore çš„æ ¸å¿ƒå„ªå‹¢**ï¼š
>
> å¯ä»¥è¨ˆç®—ã€Œä¿ç•™åº§æ¨™è³‡è¨Šã€çš„é©—è­‰æŒ‡æ¨™ï¼ŒçŸ¥é“ã€Œå“ªè£¡é æ¸¬å¾—å¥½/å·®ã€

### å®‰è£èˆ‡å°å…¥

```python
import xskillscore as xs
import numpy as np
import matplotlib.pyplot as plt
```

---

### ç¯„ä¾‹ï¼šè¨ˆç®— RMSE

```python
# ç‚ºäº†ç¤ºç¯„ï¼Œæˆ‘å€‘å…ˆå»ºç«‹ä¸€äº›å‡è³‡æ–™
# å¯¦éš›æ‡‰ç”¨æ™‚ï¼Œé€™æœƒæ˜¯ä½ çš„æ¨¡å‹é æ¸¬

# è§€æ¸¬å€¼
obs = valid_ds['convection_flag'].isel(time=slice(0, 100))

# å‡è¨­çš„é æ¸¬å€¼ï¼ˆå¯¦éš›ä¸Šæ‡‰è©²ä¾†è‡ªæ¨¡å‹ï¼‰
# é€™è£¡ç”¨è§€æ¸¬å€¼åŠ ä¸Šä¸€äº›é›œè¨Š
pred = obs + np.random.randn(*obs.shape) * 0.2
pred = pred.clip(0, 1)  # é™åˆ¶åœ¨ [0, 1]

print(f"Observation shape: {obs.shape}")  # (100, 121, 161)
print(f"Prediction shape: {pred.shape}")

# è¨ˆç®—ç©ºé–“åˆ†ä½ˆçš„ RMSEï¼ˆå°æ™‚é–“ç¶­åº¦ï¼‰
rmse_spatial = xs.rmse(pred, obs, dim='time')

print("Spatial RMSE:")
print(rmse_spatial)  # (121, 161) - æ¯å€‹æ ¼é»çš„ RMSE
```

---

### ç¯„ä¾‹ï¼šè¨ˆç®—å¤šç¨®æŒ‡æ¨™

```python
# 1. Mean Absolute Error
mae = xs.mae(pred, obs, dim='time')

# 2. Mean Squared Error
mse = xs.mse(pred, obs, dim='time')

# 3. Pearson Correlation
corr = xs.pearson_r(pred, obs, dim='time')

# 4. R-squared
r2 = xs.r2(pred, obs, dim='time')

print(f"MAE (spatial mean): {mae.mean().values:.4f}")
print(f"MSE (spatial mean): {mse.mean().values:.4f}")
print(f"Correlation (spatial mean): {corr.mean().values:.4f}")
print(f"RÂ² (spatial mean): {r2.mean().values:.4f}")
```

---

### è¦–è¦ºåŒ–é©—è­‰çµæœ

```python
import cartopy.crs as ccrs
import cartopy.feature as cfeature

# å»ºç«‹åœ–è¡¨
fig, axes = plt.subplots(2, 3, figsize=(18, 10),
                          subplot_kw={'projection': ccrs.PlateCarree()})

# ç¬¬ä¸€åˆ—ï¼šè§€æ¸¬ã€é æ¸¬ã€å·®ç•°
t_idx = 0  # ç¬¬ä¸€å€‹æ™‚é–“é»

obs.isel(time=t_idx).plot(
    ax=axes[0, 0],
    cmap='RdBu_r',
    vmin=0, vmax=1,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Convection Flag'}
)
axes[0, 0].coastlines()
axes[0, 0].set_title('Observation (t=0)')

pred.isel(time=t_idx).plot(
    ax=axes[0, 1],
    cmap='RdBu_r',
    vmin=0, vmax=1,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Convection Flag'}
)
axes[0, 1].coastlines()
axes[0, 1].set_title('Prediction (t=0)')

(pred.isel(time=t_idx) - obs.isel(time=t_idx)).plot(
    ax=axes[0, 2],
    cmap='RdBu',
    vmin=-0.5, vmax=0.5,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Difference'}
)
axes[0, 2].coastlines()
axes[0, 2].set_title('Difference (Pred - Obs)')

# ç¬¬äºŒåˆ—ï¼šé©—è­‰æŒ‡æ¨™
rmse_spatial.plot(
    ax=axes[1, 0],
    cmap='viridis',
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'RMSE'}
)
axes[1, 0].coastlines()
axes[1, 0].set_title('RMSE (over time)')

corr.plot(
    ax=axes[1, 1],
    cmap='RdBu_r',
    vmin=-1, vmax=1,
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'Correlation'}
)
axes[1, 1].coastlines()
axes[1, 1].set_title('Correlation (over time)')

mae.plot(
    ax=axes[1, 2],
    cmap='viridis',
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'label': 'MAE'}
)
axes[1, 2].coastlines()
axes[1, 2].set_title('MAE (over time)')

plt.tight_layout()
plt.savefig('validation_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… é©—è­‰çµæœå·²å„²å­˜ï¼švalidation_results.png")
```

---

### xskillscore é€²éšåŠŸèƒ½

#### åˆ†é¡å•é¡Œçš„æ··æ·†çŸ©é™£ç›¸é—œæŒ‡æ¨™

```python
# å°‡é æ¸¬å’Œè§€æ¸¬è½‰æˆäºŒå…ƒ
pred_binary = (pred > 0.5).astype(int)
obs_binary = obs.astype(int)

# å¯ä»¥ä½¿ç”¨ sklearn è¨ˆç®—æ··æ·†çŸ©é™£ï¼Œç„¶å¾Œå¯è¦–åŒ–
from sklearn.metrics import confusion_matrix, classification_report

# å±•å¹³æˆ 1D
pred_flat = pred_binary.values.ravel()
obs_flat = obs_binary.values.ravel()

# è¨ˆç®—
cm = confusion_matrix(obs_flat, pred_flat)
print("Confusion Matrix:")
print(cm)

report = classification_report(obs_flat, pred_flat, target_names=['No Convection', 'Convection'])
print("\nClassification Report:")
print(report)
```

#### æ™‚é–“åºåˆ—é©—è­‰

```python
# è¨ˆç®—æ™‚é–“åºåˆ—çš„ç›¸é—œä¿‚æ•¸ï¼ˆå°ç©ºé–“ç¶­åº¦ï¼‰
corr_temporal = xs.pearson_r(pred, obs, dim=['latitude', 'longitude'])

# ç¹ªè£½æ™‚é–“åºåˆ—
fig, ax = plt.subplots(figsize=(12, 4))
corr_temporal.plot(ax=ax)
ax.set_xlabel('Time')
ax.set_ylabel('Spatial Correlation')
ax.set_title('Temporal Evolution of Spatial Correlation')
ax.axhline(0.5, color='r', linestyle='--', label='Threshold')
ax.legend()
plt.tight_layout()
plt.savefig('temporal_correlation.png', dpi=150)
plt.show()
```

---

### èˆ‡å‚³çµ±æ–¹æ³•çš„å°æ¯”

```python
# å‚³çµ±æ–¹æ³•ï¼ˆsklearnï¼‰
from sklearn.metrics import accuracy_score, precision_score, recall_score

pred_flat = (pred > 0.5).astype(int).values.ravel()
obs_flat = obs.astype(int).values.ravel()

acc = accuracy_score(obs_flat, pred_flat)
prec = precision_score(obs_flat, pred_flat)
rec = recall_score(obs_flat, pred_flat)

print("=== å‚³çµ±æ–¹æ³•ï¼ˆæ•´é«”æŒ‡æ¨™ï¼‰===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")

# xskillscore æ–¹æ³•ï¼ˆç©ºé–“åˆ†ä½ˆï¼‰
print("\n=== xskillscore æ–¹æ³•ï¼ˆç©ºé–“åˆ†ä½ˆï¼‰===")
print(f"RMSE (mean): {rmse_spatial.mean().values:.4f}")
print(f"RMSE (std):  {rmse_spatial.std().values:.4f}")
print(f"RMSE (min):  {rmse_spatial.min().values:.4f}")
print(f"RMSE (max):  {rmse_spatial.max().values:.4f}")

print("\nâœ… xskillscore å¯ä»¥å‘Šè¨´ä½ ã€Œå“ªè£¡ã€é æ¸¬å¾—å¥½/å·®ï¼")
```

---

# ç¸½çµèˆ‡ä¸‹ä¸€æ­¥

## æœ¬èª²ç¨‹å­¸åˆ°çš„æ ¸å¿ƒæŠ€èƒ½

### 1. Zarr å„²å­˜å„ªåŒ–
- âœ… ç†è§£ Zarr vs NetCDF çš„å·®ç•°
- âœ… çŸ¥é“ç‚ºä½•è¦ç”¨ Zarr < 3.0
- âœ… èƒ½å¤ è®€å–ä¸¦å„ªåŒ– Zarr æª”æ¡ˆ

### 2. Xarray + Dask è³‡æ–™è™•ç†
- âœ… ä½¿ç”¨ lazy evaluation è™•ç†è¶…éè¨˜æ†¶é«”çš„è³‡æ–™
- âœ… æ™‚ç©ºåˆ‡ç‰‡èˆ‡é‡æ¡æ¨£
- âœ… ç†è§£ chunking å°æ•ˆèƒ½çš„å½±éŸ¿
- âœ… ä½¿ç”¨ Dashboard ç›£æ§æ•ˆèƒ½

### 3. ML Pipeline å»ºæ§‹
- âœ… ä½¿ç”¨ xbatcher åˆ‡æ‰¹æ¬¡
- âœ… å»ºç«‹ Xarray â†’ PyTorch æ©‹æ¥å±¤
- âœ… å°‡é æ¸¬çµæœè½‰å› Xarray
- âœ… ä½¿ç”¨ xskillscore é€²è¡Œç©ºé–“é©—è­‰

---

## é‡è¦è§€å¿µå›é¡§

### Lazy Evaluation

```python
# Lazyï¼ˆä¸è¨ˆç®—ï¼‰
result_lazy = ds['temperature'].mean(dim='time')

# Eagerï¼ˆè¨ˆç®—ï¼‰
result_eager = result_lazy.compute()
```

**ä½•æ™‚ç”¨ lazyï¼Ÿ**
- æ¢ç´¢è³‡æ–™æ™‚ï¼ˆæƒ³å¿«é€Ÿçœ‹çµæœï¼‰
- å»ºç«‹è¤‡é›œè¨ˆç®—æµç¨‹æ™‚
- è³‡æ–™å¤§æ–¼è¨˜æ†¶é«”æ™‚

**ä½•æ™‚ç”¨ computeï¼Ÿ**
- éœ€è¦å¯¦éš›æ•¸å€¼æ™‚
- è¦å„²å­˜çµæœæ™‚
- è¦ç¹ªåœ–æˆ–è¼¸å‡ºæ™‚

---

### Chunking ç­–ç•¥

| Chunk Size | ä»»å‹™æ•¸ | è¨˜æ†¶é«” | é©ç”¨æƒ…å¢ƒ |
|------------|--------|--------|----------|
| å°ï¼ˆ10 MBï¼‰ | å¤š | ä½ | è¨˜æ†¶é«”æœ‰é™ |
| ä¸­ï¼ˆ50 MBï¼‰ | é©ä¸­ | é©ä¸­ | **æ¨è–¦** |
| å¤§ï¼ˆ500 MBï¼‰ | å°‘ | é«˜ | CPU ç¶å®šçš„è¨ˆç®— |

**ç¶“é©—æ³•å‰‡**ï¼š
- å–®å€‹ chunkï¼š10-100 MB
- ç¬¦åˆè®€å–æ¨¡å¼ï¼ˆæ™‚é–“åºåˆ—ï¼Ÿç©ºé–“åˆ‡ç‰‡ï¼Ÿï¼‰
- å¹³è¡¡ä»»å‹™æ•¸èˆ‡å‚³è¼¸é–‹éŠ·

---

### ML Pipeline æœ€ä½³å¯¦è¸

```python
# æ­£ç¢ºçš„è³‡æ–™æµå‘ï¼ˆä½¿ç”¨ xbatcher.loaders.torchï¼‰
Zarr files
  â†“ xr.open_zarr()
Xarray Dataset
  â†“ xbatcher.BatchGenerator()  [åˆ†é–‹å»ºç«‹ X_bgen å’Œ y_bgen]
Lazy Batches
  â†“ xbatcher.loaders.torch.MapDataset(X_bgen, y_bgen)
PyTorch-compatible Dataset
  â†“ torch.utils.data.DataLoader(batch_size=None)
PyTorch DataLoader
  â†“ model.forward()
Predictions (Tensor)
  â†“ xr.DataArray()
Xarray DataArray with coords
  â†“ xskillscore
Spatial validation metrics
```

**é—œéµé»**ï¼š
- âœ… ä½¿ç”¨ `xbatcher.loaders.torch.MapDataset`ï¼ˆä¸è¦è‡ªå·±å¯« Datasetï¼‰
- âœ… DataLoader çš„ `batch_size=None`ï¼ˆxbatcher å·²å®šç¾©ï¼‰
- âœ… `preload_batch=False`ï¼ˆä¿æŒ lazyï¼‰
- âœ… `multiprocessing_context='forkserver'`ï¼ˆé¿å… pickle å•é¡Œï¼‰

---

## å¸¸è¦‹å•é¡Œèˆ‡æ’éŒ¯

### Q1: `KeyError: 'Cannot find .zarray'`

**åŸå› **ï¼šå¯èƒ½æ˜¯ Zarr 3.0 çš„ç›¸å®¹æ€§å•é¡Œ

**è§£æ±º**ï¼š
```bash
uv add "zarr>=2.18.0,<3.0.0"
```

---

### Q2: `MemoryError` æˆ– `Out of Memory`

**åŸå› **ï¼šè³‡æ–™é‡è¶…éè¨˜æ†¶é«”

**è§£æ±ºæ–¹æ³•**ï¼š
1. å¢åŠ  chunkingï¼ˆæ¸›å°‘å–®æ¬¡è®€å–é‡ï¼‰
   ```python
   ds = xr.open_zarr('data.zarr', chunks={'time': 10, 'lat': 20, 'lon': 20})
   ```

2. ä½¿ç”¨ `.compute()` æ™‚å…ˆè¨ˆç®—å­é›†
   ```python
   result = ds.isel(time=slice(0, 100)).mean().compute()
   ```

3. ä½¿ç”¨ `.persist()` è€Œä¸æ˜¯ `.compute()`
   ```python
   result = ds.mean(dim='time').persist()  # åˆ†æ•£å¼è¨˜æ†¶é«”
   ```

---

### Q3: Dask è¨ˆç®—å¾ˆæ…¢

**å¯èƒ½åŸå› èˆ‡è§£æ±º**ï¼š

1. **Chunk å¤ªå°**ï¼šå¢åŠ  chunk size
2. **Chunk å¤ªå¤§**ï¼šæ¸›å°‘ chunk size
3. **Task overhead**ï¼šæ¸›å°‘ä»»å‹™æ•¸é‡
4. **è³‡æ–™å‚³è¼¸ç“¶é ¸**ï¼šæª¢æŸ¥ç¶²è·¯/ç£ç¢Ÿé€Ÿåº¦

**è¨ºæ–·å·¥å…·**ï¼šDask Dashboard

---

### Q4: xbatcher ç”¢ç”Ÿçš„ batch æ•¸é‡ä¸ç¬¦é æœŸ

**æª¢æŸ¥é»**ï¼š
```python
print(f"Data shape: {ds.dims}")
print(f"Batch config: {batch_config}")

# è¨ˆç®—é æœŸçš„ batch æ•¸
n_time_batches = len(ds.time) // batch_config['batch_dims']['time']
n_lat_patches = (len(ds.latitude) - batch_config['input_dims']['latitude']) // (batch_config['input_dims']['latitude'] - batch_config['input_overlap']['latitude']) + 1
n_lon_patches = (len(ds.longitude) - batch_config['input_dims']['longitude']) // (batch_config['input_dims']['longitude'] - batch_config['input_overlap']['longitude']) + 1

expected_batches = n_time_batches * n_lat_patches * n_lon_patches
print(f"Expected batches: {expected_batches}")
```

---

### Q5: PyTorch DataLoader `num_workers > 0` æ™‚å‡ºéŒ¯

**å¸¸è¦‹éŒ¯èª¤**ï¼š
```
RuntimeError: DataLoader worker (pid 12345) is killed by signal: Killed.
```

**å¯èƒ½åŸå› **ï¼š
1. è¨˜æ†¶é«”ä¸è¶³ï¼ˆworkers è¤‡è£½è³‡æ–™ï¼‰
2. Dask client çš„ pickle å•é¡Œ
3. ä½¿ç”¨äº†éŒ¯èª¤çš„ `multiprocessing_context`

**è§£æ±ºæ–¹æ³•**ï¼š

```python
# âœ… æ¨è–¦åšæ³•ï¼šä½¿ç”¨ 'forkserver' context
train_loader = DataLoader(
    dataset,
    batch_size=None,
    num_workers=4,
    persistent_workers=True,
    multiprocessing_context='forkserver'  # é—œéµï¼
)

# å¦‚æœä»æœ‰å•é¡Œï¼Œæª¢æŸ¥ï¼š
# 1. æ˜¯å¦ä½¿ç”¨ xbatcher.loaders.torch.MapDatasetï¼ˆè€Œéè‡ªå·±å¯«çš„ Datasetï¼‰
# 2. æ˜¯å¦è¨­å®š preload_batch=False
# 3. æ˜¯å¦è¨­å®š batch_size=None
```

**Debug æ™‚çš„è‡¨æ™‚æ–¹æ¡ˆ**ï¼š
```python
# å…ˆç”¨ num_workers=0 ç¢ºèªé‚è¼¯æ­£ç¢º
train_loader = DataLoader(dataset, batch_size=None, num_workers=0)
```

---

## å»¶ä¼¸å­¸ç¿’è³‡æº

### å®˜æ–¹æ–‡ä»¶

- **Xarray**: https://docs.xarray.dev/
- **Dask**: https://docs.dask.org/
- **Zarr**: https://zarr.readthedocs.io/
- **xbatcher**: https://xbatcher.readthedocs.io/
- **xskillscore**: https://xskillscore.readthedocs.io/

### é€²éšä¸»é¡Œ

#### 1. åˆ†æ•£å¼é‹ç®—ï¼ˆDask Clusterï¼‰
```python
from dask.distributed import Client
from dask_jobqueue import SLURMCluster

# åœ¨ HPC ä¸Šå»ºç«‹ cluster
cluster = SLURMCluster(cores=4, memory='16GB')
cluster.scale(jobs=10)  # å•Ÿå‹• 10 å€‹ workers

client = Client(cluster)
```

#### 2. é›²ç«¯å„²å­˜ï¼ˆS3, GCSï¼‰
```python
import fsspec

# å¾ S3 è®€å– Zarr
ds = xr.open_zarr(
    's3://bucket-name/data.zarr',
    storage_options={'anon': True}
)
```

#### 3. GPU åŠ é€Ÿï¼ˆcupy, cuDFï¼‰
```python
# ä½¿ç”¨ CuPy åŠ é€Ÿ Dask Array
import cupy as cp
import dask.array as da

# å»ºç«‹ GPU array
x = da.from_array(cp.random.random((10000, 10000)), chunks=(1000, 1000))
result = x.mean().compute()  # åœ¨ GPU ä¸Šè¨ˆç®—
```

---

## ä¸‹ä¸€æ­¥å»ºè­°

### å¦‚æœä½ æƒ³æ·±å…¥è³‡æ–™è™•ç†ï¼š
- å­¸ç¿’ Dask DataFrameï¼ˆPart 2ï¼Œå¦‚æœé–‹èª²çš„è©±ï¼‰
- æ¢ç´¢ Polars / DuckDBï¼ˆé«˜æ•ˆèƒ½è¡¨æ ¼è™•ç†ï¼‰
- ç ”ç©¶ Apache Arrowï¼ˆè¨˜æ†¶é«”æ ¼å¼ï¼‰

### å¦‚æœä½ æƒ³æ·±å…¥ MLï¼š
- å­¸ç¿’ PyTorch Lightningï¼ˆé«˜éšè¨“ç·´æ¡†æ¶ï¼‰
- æ¢ç´¢ Hugging Face Datasetsï¼ˆML è³‡æ–™é›†å·¥å…·ï¼‰
- ç ”ç©¶ Rayï¼ˆåˆ†æ•£å¼ MLï¼‰

### å¦‚æœä½ æƒ³æ·±å…¥æ°£è±¡æ‡‰ç”¨ï¼š
- æ¢ç´¢ MetPyï¼ˆæ°£è±¡è¨ˆç®—ï¼‰
- å­¸ç¿’ Satpyï¼ˆè¡›æ˜Ÿè³‡æ–™è™•ç†ï¼‰
- ç ”ç©¶ Climate Data Operators (CDO)

---

## é™„éŒ„ A: å¥—ä»¶ç‰ˆæœ¬å»ºè­°

### pyproject.toml

```toml
[project]
name = "dask-array-workshop"
version = "1.0.0"
requires-python = ">=3.11"
dependencies = [
    # æ ¸å¿ƒ
    "xarray>=2024.10.0",
    "zarr>=2.18.0,<3.0.0",  # é‡è¦ï¼< 3.0
    "dask[complete]>=2024.10.0",
    "numpy>=2.1.0",

    # è¦–è¦ºåŒ–
    "matplotlib>=3.9.0",
    "cartopy>=0.23.0",

    # ML
    "xbatcher>=0.3.0",
    "xskillscore>=0.0.26",
    "torch>=2.0.0",
    "torchvision>=0.15.0",

    # å…¶ä»–
    "fsspec>=2024.9.0",
    "netCDF4>=1.7.0",  # å¦‚æœéœ€è¦è®€å– NetCDF
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.0",
    "jupyterlab>=4.0.0",
    "pytest>=8.0.0",
]
```

---

## é™„éŒ„ B: åƒè€ƒæ–‡ç»

### é‡è¦è«–æ–‡

1. **Zarr**:
   - Moore, J., & Rocklin, M. (2018). Zarr: chunked, compressed, N-dimensional arrays.

2. **Xarray**:
   - Hoyer, S., & Hamman, J. (2017). xarray: N-D labeled arrays and datasets in Python. Journal of Open Research Software, 5(1).

3. **Dask**:
   - Rocklin, M. (2015). Dask: Parallel computation with blocked algorithms and task scheduling.

### ç›¸é—œå·¥å…·

- **Pangeo**: å¤§æ°£èˆ‡æµ·æ´‹ç§‘å­¸çš„é–‹æºç¤¾ç¾¤
  - https://pangeo.io/

- **Intake**: è³‡æ–™ç›®éŒ„ç³»çµ±
  - https://intake.readthedocs.io/

- **Xarray-spatial**: åœ°ç†ç©ºé–“åˆ†æ
  - https://xarray-spatial.org/

---

## èª²ç¨‹å›é¥‹

è«‹å”åŠ©å¡«å¯«èª²ç¨‹å›é¥‹è¡¨ï¼ˆé€£çµï¼‰ï¼Œæ‚¨çš„æ„è¦‹å°æˆ‘å€‘éå¸¸é‡è¦ï¼

**æ„Ÿè¬åƒèˆ‡æœ¬æ¬¡èª²ç¨‹ï¼**

---

**æ–‡ä»¶ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2025-10-29
**æˆæ¬Š**: CC BY 4.0
