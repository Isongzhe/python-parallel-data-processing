#!/usr/bin/env python3
"""
03-ml-pipeline.py - ML Pipeline with xbatcher
Fixed version: variable dimension preserved in input_dims
"""

# ===== Cell 2 =====
import dask
from dask.distributed import Client
import xarray as xr
import xbatcher
from xbatcher.loaders.torch import MapDataset
import intake
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# å•Ÿå‹• Dask Client
client = Client(n_workers=2, threads_per_worker=2, memory_limit='2GB')
print(f"Dask Dashboard: {client.dashboard_link}")

# è¼‰å…¥ catalog
catalog = intake.open_catalog('catalog.yaml')

# ===== Cell 5 =====
ds = catalog.era5_2019_chunked.to_dask()
ds

# ===== Cell 6 =====
# Resample åˆ° daily ä»¥æ¸›å°‘è³‡æ–™é‡
ds_daily = ds.resample(time='1D').mean()

print("Dataset:")
print(ds_daily)
print()
print(f"Shape: {ds_daily['convective_available_potential_energy'].shape}")
print(f"Total size: {ds_daily.nbytes / 1e9:.2f} GB")

# ===== Cell 8 =====
# å»ºç«‹ binary label
convection_flag = (
    (ds_daily['convective_available_potential_energy'] > 1000) & 
    (ds_daily['convective_inhibition'] > -50)
).astype(np.float32)  # è½‰ç‚º float32 ä»¥ä¾¿èˆ‡ features ç›¸å®¹

# åŠ å…¥ Dataset
ds_daily['convection_flag'] = convection_flag

print("Convection flag:")
print(ds_daily['convection_flag'])
print()

# æª¢æŸ¥ class balance
flag_mean = convection_flag.mean().compute()
print(f"Convection occurrence rate: {flag_mean.values * 100:.2f}%")
print(f"  Class 0 (no convection): {(1 - flag_mean.values) * 100:.2f}%")
print(f"  Class 1 (convection): {flag_mean.values * 100:.2f}%")

# ===== Cell 11 =====
# è¨ˆç®—åˆ†å‰²é»
n_total = len(ds_daily.time)
n_train = int(n_total * 0.7)
n_val = int(n_total * 0.15)

# æ™‚é–“åºåˆ—åˆ†å‰²
train_ds = ds_daily.isel(time=slice(0, n_train))
val_ds = ds_daily.isel(time=slice(n_train, n_train + n_val))
test_ds = ds_daily.isel(time=slice(n_train + n_val, None))

print("Data split:")
print(f"  Training: {len(train_ds.time)} days ({train_ds.time.values[0]} to {train_ds.time.values[-1]})")
print(f"  Validation: {len(val_ds.time)} days ({val_ds.time.values[0]} to {val_ds.time.values[-1]})")
print(f"  Test: {len(test_ds.time)} days ({test_ds.time.values[0]} to {test_ds.time.values[-1]})")

# ===== Cell 14 =====
# å®šç¾© feature è®Šæ•¸
feature_vars = ['convective_available_potential_energy', 'convective_inhibition', 'k_index', 'boundary_layer_height']

# Stage 1a: ç‚º features å‰µå»º BatchGenerator
X_bgen = xbatcher.BatchGenerator(
    train_ds[feature_vars],
    input_dims={'latitude': 12, 'longitude': 12},
    batch_dims={'time': 16},                      # 16 time steps per batch
    preload_batch=False  # ä¿æŒ lazy evaluation
)

# Stage 1b: ç‚º labels å‰µå»º BatchGenerator
y_bgen = xbatcher.BatchGenerator(
    train_ds['convection_flag'],
    input_dims={'latitude': 12, 'longitude': 12},  
    batch_dims={'time': 16},
    preload_batch=False
)

print("BatchGenerators created:")
print(f"  X_bgen: {len(list(X_bgen))} batches")
print(f"  y_bgen: {len(list(y_bgen))} batches")
print()
print("Note: ä¸Šé¢çš„ list() æœƒå¯¦éš›è¿­ä»£ï¼Œåªæ˜¯ç‚ºäº†è¨ˆæ•¸ã€‚")
print("      å¯¦éš›ä½¿ç”¨æ™‚ä¸éœ€è¦é€™æ¨£åšã€‚")

# ===== Cell 16 =====
# é‡æ–°å‰µå»ºï¼ˆå› ç‚º generator å·²ç¶“è¢«æ¶ˆè€—äº†ï¼‰
X_bgen = xbatcher.BatchGenerator(
    train_ds[feature_vars],
    input_dims={'latitude': 12, 'longitude': 12},
    batch_dims={'time': 16},
    preload_batch=False
)

# å–å¾—ç¬¬ä¸€å€‹ batch
first_batch = next(iter(X_bgen))

print("First batch (still lazy):")
print(first_batch)
print()
print(f"Dimensions: {first_batch.dims}")
print(f"Shape: {first_batch.dims}")
print(f"Variables: {list(first_batch.data_vars)}")
print()
print(f"CAPE shape in this batch: {first_batch['convective_available_potential_energy'].shape}")
print(f"Type: {type(first_batch['convective_available_potential_energy'].data)}")

# ===== Cell 19 =====
# Configuration
SPATIAL_PATCH_SIZE = {'latitude': 12, 'longitude': 12}  # Small spatial patches
TIME_BATCH_SIZE = 16  # Number of time steps per batch

lat_size = train_ds.sizes['latitude']
lon_size = train_ds.sizes['longitude']

print(f"Data dimensions: latitude={lat_size}, longitude={lon_size}")
print(f"Spatial patch size: {SPATIAL_PATCH_SIZE}")
print(f"Time batch size: {TIME_BATCH_SIZE}")
print()

# ============================================================================
# æ–¹æ³•é¸æ“‡ï¼šè™•ç†å¤šè®Šæ•¸ Dataset
# ============================================================================
# 
# æœ‰å…©ç¨®æ–¹æ³•å¯ä»¥è™•ç†å¤šè®Šæ•¸ Datasetï¼ˆCAPE, CIN, K-index, BLHï¼‰ï¼š
# 
# æ–¹æ³• 1ï¼šæå‰è½‰æ› (æ›´ç›´è§€) âœ… æ¨è–¦
#   - åœ¨å‰µå»º BatchGenerator ä¹‹å‰ï¼Œå…ˆç”¨ .to_array() æŠŠ Dataset è½‰æˆ DataArray
#   - variable ç¶­åº¦æœƒè‡ªå‹•è®Šæˆç¬¬ä¸€å€‹ç¶­åº¦
#   - ä½¿ç”¨ xbatcher é è¨­çš„ to_tensor() å³å¯
# 
# æ–¹æ³• 2ï¼šä½¿ç”¨è‡ªå®šç¾© transform (æ›´éˆæ´»)
#   - ç›´æ¥å‚³å…¥ Dataset çµ¦ BatchGenerator
#   - åœ¨ MapDataset ä¸­æä¾›è‡ªå®šç¾©çš„ transform å‡½æ•¸
#   - transform æœƒåœ¨æ¯å€‹ batch è¼‰å…¥æ™‚è‡ªå‹•è™•ç†è½‰æ›
# 
# å…©ç¨®æ–¹æ³•éƒ½æ˜¯ lazy çš„ï¼Œè¨˜æ†¶é«”ä½¿ç”¨ç›¸åŒã€‚é¸ä½ å–œæ­¡çš„ï¼
# ============================================================================

print("ä½¿ç”¨æ–¹æ³• 1ï¼šæå‰è½‰æ› Dataset â†’ DataArray")
print("-" * 60)

# æå‰æŠŠå¤šè®Šæ•¸ Dataset è½‰æˆ DataArray
train_features = train_ds[feature_vars].to_array(dim='variable')
train_labels = train_ds['convection_flag']

print(f"Features shape: {train_features.shape}")  # (variable=4, time=255, lat=121, lon=161)
print(f"Labels shape: {train_labels.shape}")      # (time=255, lat=121, lon=161)
print(f"Features type: {type(train_features.data)}")  # Still dask array!
print()

# å‰µå»º BatchGenerator
# input_dims: æ¯å€‹ patch è¦ä¿ç•™çš„ç¶­åº¦å¤§å°ï¼ˆæœƒæ²¿è‘—é€™äº›ç¶­åº¦åˆ‡åˆ†æˆå¤šå€‹ patchesï¼‰
# batch_dims: æœƒæ²¿è‘—é€™å€‹ç¶­åº¦åˆ‡åˆ†æˆå¤šå€‹ batches
X_bgen = xbatcher.BatchGenerator(
    train_features,  # DataArray: (variable=4, time=255, lat=121, lon=161)
    input_dims={'variable': 4, **SPATIAL_PATCH_SIZE},  # ç©ºé–“ patch å¤§å°: 12x12
    batch_dims={'time': TIME_BATCH_SIZE},  # æ™‚é–“ batch: 16 time steps
    preload_batch=False
)

y_bgen = xbatcher.BatchGenerator(
    train_labels,  # DataArray: (time=255, lat=121, lon=161)
    input_dims={'variable': 4, **SPATIAL_PATCH_SIZE},  # ç©ºé–“ patch å¤§å°: 12x12
    batch_dims={'time': TIME_BATCH_SIZE},  # æ™‚é–“ batch: 16 time steps
    preload_batch=False
)

# å‰µå»º MapDataset (ä½¿ç”¨é è¨­çš„ to_tensor)
train_dataset = MapDataset(
    X_bgen,
    y_bgen
    # ä¸éœ€è¦ transform åƒæ•¸ï¼
)

print("âœ“ ä½¿ç”¨ xbatcher é è¨­çš„ to_tensor()")


print()
print(f"PyTorch Dataset created: {len(train_dataset)} batches")
print(f"Type: {type(train_dataset)}")
print()
print(f"Expected output shape:")
print(f"  X: (variable=4, time={TIME_BATCH_SIZE}, lat={SPATIAL_PATCH_SIZE['latitude']}, lon={SPATIAL_PATCH_SIZE['longitude']})")
print(f"  y: (time={TIME_BATCH_SIZE}, lat={SPATIAL_PATCH_SIZE['latitude']}, lon={SPATIAL_PATCH_SIZE['longitude']})")

# ===== Cell 21 =====
# å–å¾—ä¸€å€‹æ¨£æœ¬
X_sample, y_sample = train_dataset[0]

print("Sample from Dataset:")
print(f"  X type: {type(X_sample)}")
print(f"  X shape: {X_sample.shape}")
print(f"  X dtype: {X_sample.dtype}")
print()
print(f"  y type: {type(y_sample)}")
print(f"  y shape: {y_sample.shape}")
print(f"  y dtype: {y_sample.dtype}")
print()

# Shape interpretation (handle both methods)
if len(X_sample.shape) == 4:
    print("Shape interpretation:")
    print(f"  X: (variable={X_sample.shape[0]}, time={X_sample.shape[1]}, lat={X_sample.shape[2]}, lon={X_sample.shape[3]})")
    print(f"  y: (time={y_sample.shape[0]}, lat={y_sample.shape[1]}, lon={y_sample.shape[2]})")
    print()
    print("âœ“ Shape is correct! The 'variable' dimension (4 weather variables) serves as the channel dimension for CNN.")
else:
    print(f"âš ï¸ Unexpected shape! Expected 4D tensor but got {len(X_sample.shape)}D")
    print(f"  Actual X shape: {X_sample.shape}")
    print(f"  Actual y shape: {y_sample.shape}")
    
print()
print("Note: The 'variable' dimension represents the 4 feature variables:")
print("  - variable[0]: CAPE (Convective Available Potential Energy)")
print("  - variable[1]: CIN (Convective Inhibition)")
print("  - variable[2]: K-index")
print("  - variable[3]: BLH (Boundary Layer Height)")

# ===== Cell 23 =====
# å‰µå»º Training DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=None,  # ä¸è¦å†å¢åŠ  batch ç¶­åº¦ï¼
    shuffle=True,     # æ‰“äº‚ batches é †åºï¼ˆä¸æ˜¯æ‰“äº‚ batch å…§çš„é †åºï¼‰
    num_workers=0,    # ä¸ä½¿ç”¨ multiprocessingï¼ˆé¿å…åºåˆ—åŒ–å•é¡Œï¼‰
)

print(f"Training DataLoader created: {len(train_loader)} batches")
print()
print("Parameters:")
print(f"  batch_size: None (xbatcher already defines batch)")
print(f"  shuffle: True")
print(f"  num_workers: 0 (no multiprocessing)")


# ===== Cell 25 =====
# ç‚º Validation å’Œ Test sets å‰µå»º datasets å’Œ DataLoaders

# æ–¹æ³• 1ï¼šæå‰è½‰æ›
# Validation set
val_features = val_ds[feature_vars].to_array(dim='variable')
val_labels = val_ds['convection_flag']

X_val_bgen = xbatcher.BatchGenerator(
    val_features,
    input_dims={'variable': 4, **SPATIAL_PATCH_SIZE},
    batch_dims={'time': TIME_BATCH_SIZE},
    preload_batch=False
)

y_val_bgen = xbatcher.BatchGenerator(
    val_labels,
    input_dims={'variable': 4, **SPATIAL_PATCH_SIZE},
    batch_dims={'time': TIME_BATCH_SIZE},
    preload_batch=False
)

val_dataset = MapDataset(X_val_bgen, y_val_bgen)

# Test set
test_features = test_ds[feature_vars].to_array(dim='variable')
test_labels = test_ds['convection_flag']

X_test_bgen = xbatcher.BatchGenerator(
    test_features,
    input_dims={'variable': 4, **SPATIAL_PATCH_SIZE},
    batch_dims={'time': TIME_BATCH_SIZE},
    preload_batch=False
)

y_test_bgen = xbatcher.BatchGenerator(
    test_labels,
    input_dims={'variable': 4, **SPATIAL_PATCH_SIZE},
    batch_dims={'time': TIME_BATCH_SIZE},
    preload_batch=False
)

test_dataset = MapDataset(X_test_bgen, y_test_bgen)

# å‰µå»º DataLoaders
val_loader = DataLoader(
    val_dataset,
    batch_size=None,
    shuffle=False,  # Validation ä¸ shuffle
    num_workers=0
)

test_loader = DataLoader(
    test_dataset,
    batch_size=None,
    shuffle=False,  # Test ä¸ shuffle
    num_workers=0
)

print(f"âœ“ Validation DataLoader created: {len(val_loader)} batches")
print(f"âœ“ Test DataLoader created: {len(test_loader)} batches")
print()
print("All DataLoaders ready for training and evaluation!")

# ===== Cell 27 =====
# è¿­ä»£å–å¾—ä¸€å€‹ batch
for X_batch, y_batch in train_loader:
    print("Batch from DataLoader:")
    print(f"  X: {X_batch.shape}, dtype: {X_batch.dtype}")
    print(f"  y: {y_batch.shape}, dtype: {y_batch.dtype}")
    print()
    print(f"  X min/max: {X_batch.min():.2f} / {X_batch.max():.2f}")
    print(f"  y unique values: {torch.unique(y_batch)}")
    break  # åªçœ‹ç¬¬ä¸€å€‹ batch

# ===== Cell 29 =====
class SimpleConvectionCNN(nn.Module):
    def __init__(self, in_channels=4):
        super().__init__()
        
        # 3D Convolutions (time + space)
        self.conv1 = nn.Conv3d(in_channels, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv3d(32, 1, kernel_size=3, padding=1)  # output: 1 channel
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()  # for binary classification
        
    def forward(self, x):
        # Input from xbatcher: (variable, time, lat, lon) = (4, 16, 121, 161)
        # Need: (batch, channels, time, height, width)
        
        # Add batch dimension if not present
        if x.dim() == 4:
            x = x.unsqueeze(0)  # (1, variable, time, lat, lon)
        
        # x is now: (batch, variable, time, lat, lon)
        # Conv3d expects: (batch, channels, depth, height, width)
        # Map: variable->channels, time->depth, lat/lon->height/width
        # So shape is already correct!
        
        # Convolution layers
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.sigmoid(self.conv3(x))
        
        # Output: (batch, 1, time, lat, lon)
        # Squeeze channel dim
        x = x.squeeze(1)  # (batch, time, lat, lon)
        
        # Remove batch dim if it was added
        if x.size(0) == 1:
            x = x.squeeze(0)  # (time, lat, lon)
        
        return x

# å‰µå»ºæ¨¡å‹
model = SimpleConvectionCNN(in_channels=4)
print(model)
print()

# è¨ˆç®—åƒæ•¸æ•¸é‡
n_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {n_params:,}")

# ===== Cell 31 =====
# å‰µå»º dummy input matching xbatcher output shape
dummy_input = torch.randn(4, 16, 121, 161)  # (variable, time, lat, lon)

# Forward pass
with torch.no_grad():
    output = model(dummy_input)

print(f"Input shape: {dummy_input.shape}")
print(f"Output shape: {output.shape}")
print(f"Output range: [{output.min():.3f}, {output.max():.3f}]")
print()
print("âœ“ Model forward pass successful!")

# ===== Cell 33 =====
# è¨­å®š device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

model = model.to(device)

# Loss function
criterion = nn.BCELoss()  # Binary Cross Entropy

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training config
n_epochs = 2  # Demo: å¿«é€Ÿå±•ç¤ºæµç¨‹

print(f"Training configuration:")
print(f"  Epochs: {n_epochs}")
print(f"  Optimizer: Adam (lr=0.001)")
print(f"  Loss: Binary Cross Entropy")

# ===== Cell 35 =====
# Training loop
history = {'loss': []}

for epoch in range(n_epochs):
    model.train()
    epoch_loss = 0.0
    n_batches = 0
    
    for X_batch, y_batch in train_loader:
        # Move to device
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)

        # Forward pass
        outputs = model(X_batch)
        
        # Calculate loss
        loss = criterion(outputs, y_batch)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Record
        epoch_loss += loss.item()
        n_batches += 1
    
    # Epoch summary
    avg_loss = epoch_loss / n_batches
    history['loss'].append(avg_loss)
    
    print(f"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}")

print("\nâœ“ Training complete!")

# ===== Cell 37 =====
plt.figure(figsize=(8, 5))
plt.plot(range(1, n_epochs+1), history['loss'], marker='o', linewidth=2, markersize=8)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training Loss', fontsize=13)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# ===== Cell 39 =====
# ç‚º test set å‰µå»º DataLoader
# ä½¿ç”¨èˆ‡ training set ç›¸åŒçš„æ–¹æ³•

if USE_METHOD == 1:
    # æ–¹æ³• 1ï¼šæå‰è½‰æ›
    test_features = test_ds[feature_vars].to_array(dim='variable')
    test_labels = test_ds['convection_flag']
    
    X_test_bgen = xbatcher.BatchGenerator(
        test_features,
        input_dims={'latitude': 16, 'longitude': 16},
        batch_dims={'time': 32},
        preload_batch=False
    )
    
    y_test_bgen = xbatcher.BatchGenerator(
        test_labels,
        input_dims={'latitude': 16, 'longitude': 16},
        batch_dims={'time': 32},
        preload_batch=False
    )
    
    test_dataset = MapDataset(X_test_bgen, y_test_bgen)

elif USE_METHOD == 2:
    # æ–¹æ³• 2ï¼šä½¿ç”¨ transform
    # éœ€è¦é‡æ–°å®šç¾© transformï¼ˆå¦‚æœé€™å€‹ cell å–®ç¨åŸ·è¡Œï¼‰
    def dataset_to_tensor(xr_obj):
        if isinstance(xr_obj, xr.Dataset):
            xr_obj = xr_obj.to_array(dim='variable')
        if isinstance(xr_obj, xr.DataArray):
            xr_obj = xr_obj.values
        return torch.from_numpy(xr_obj)
    
    X_test_bgen = xbatcher.BatchGenerator(
        test_ds[feature_vars],
        input_dims={'latitude': 16, 'longitude': 16},
        batch_dims={'time': 32},
        preload_batch=False
    )
    
    y_test_bgen = xbatcher.BatchGenerator(
        test_ds['convection_flag'],
        input_dims={'latitude': 16, 'longitude': 16},
        batch_dims={'time': 32},
        preload_batch=False
    )
    
    test_dataset = MapDataset(
        X_test_bgen, 
        y_test_bgen,
        transform=dataset_to_tensor,
        target_transform=dataset_to_tensor
    )

test_loader = DataLoader(
    test_dataset,
    batch_size=None,
    shuffle=False,  # test set ä¸ shuffle
    num_workers=2,
    multiprocessing_context='forkserver'
)

print(f"Test set: {len(test_loader)} batches")

# ===== Cell 40 =====
# Evaluation
model.eval()
test_loss = 0.0
predictions = []
targets = []

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        
        test_loss += loss.item()
        predictions.append(outputs.cpu())
        targets.append(y_batch.cpu())

avg_test_loss = test_loss / len(test_loader)
print(f"Test Loss: {avg_test_loss:.4f}")

# Concatenate all predictions
predictions = torch.cat(predictions, dim=0)
targets = torch.cat(targets, dim=0)

print(f"\nPredictions shape: {predictions.shape}")
print(f"Targets shape: {targets.shape}")

# ===== Cell 42 =====
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# è½‰ç‚º binary predictions (threshold = 0.5)
pred_binary = (predictions > 0.5).float()

# Flatten for sklearn
pred_flat = pred_binary.flatten().numpy()
target_flat = targets.flatten().numpy()

# Calculate metrics
accuracy = accuracy_score(target_flat, pred_flat)
precision = precision_score(target_flat, pred_flat, zero_division=0)
recall = recall_score(target_flat, pred_flat, zero_division=0)
f1 = f1_score(target_flat, pred_flat, zero_division=0)

print("Classification Metrics:")
print(f"  Accuracy:  {accuracy:.4f}")
print(f"  Precision: {precision:.4f}")
print(f"  Recall:    {recall:.4f}")
print(f"  F1 Score:  {f1:.4f}")

# ===== Cell 44 =====
# é¸å–ä¸€å€‹æ™‚é–“æ­¥é©Ÿå’Œç©ºé–“ patch ä¾†è¦–è¦ºåŒ–
t_idx = 10  # ç¬¬ 10 å€‹æ™‚é–“æ­¥

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# True labels
im1 = axes[0].imshow(targets[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)
axes[0].set_title('Ground Truth', fontsize=12)
axes[0].set_xlabel('Longitude')
axes[0].set_ylabel('Latitude')
plt.colorbar(im1, ax=axes[0])

# Predictions (probability)
im2 = axes[1].imshow(predictions[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)
axes[1].set_title('Predicted Probability', fontsize=12)
axes[1].set_xlabel('Longitude')
plt.colorbar(im2, ax=axes[1])

# Binary predictions
im3 = axes[2].imshow(pred_binary[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)
axes[2].set_title('Binary Prediction (>0.5)', fontsize=12)
axes[2].set_xlabel('Longitude')
plt.colorbar(im3, ax=axes[2])

plt.tight_layout()
plt.show()

# ===== Cell 47 =====
# æ³¨æ„ï¼šé€™è£¡æ˜¯ç°¡åŒ–ç‰ˆï¼Œå¯¦å‹™ä¸Šéœ€è¦æ­£ç¢ºå°æ‡‰æ¯å€‹ patch çš„åº§æ¨™
# ç‚ºäº†ç¤ºç¯„ï¼Œæˆ‘å€‘å‡è¨­ predictions å’Œ test_ds çš„ç©ºé–“ç¯„åœç›¸åŒ

# å–å¾—ä¸€å€‹ batch çš„åº§æ¨™
sample_batch = next(iter(X_test_bgen))
time_coords = sample_batch['time'].values
lat_coords = sample_batch['latitude'].values
lon_coords = sample_batch['longitude'].values

# å‰µå»º Xarray DataArray
pred_da = xr.DataArray(
    predictions[:len(time_coords)].numpy(),  # é™åˆ¶åˆ°å¯¦éš›çš„æ™‚é–“é•·åº¦
    dims=['time', 'latitude', 'longitude'],
    coords={
        'time': time_coords,
        'latitude': lat_coords,
        'longitude': lon_coords
    },
    name='convection_probability'
)

target_da = xr.DataArray(
    targets[:len(time_coords)].numpy(),
    dims=['time', 'latitude', 'longitude'],
    coords={
        'time': time_coords,
        'latitude': lat_coords,
        'longitude': lon_coords
    },
    name='convection_truth'
)

print("Predictions as Xarray:")
print(pred_da)
print()
print("Targets as Xarray:")
print(target_da)

# ===== Cell 49 =====
import xskillscore as xs

# è¨ˆç®—æ¯å€‹æ™‚é–“æ­¥çš„ç©ºé–“ç›¸é—œ
spatial_corr = xs.pearson_r(pred_da, target_da, dim=['latitude', 'longitude'])

print("Spatial correlation (per time step):")
print(spatial_corr.values)
print()
print(f"Mean spatial correlation: {spatial_corr.mean().values:.4f}")
print(f"Std: {spatial_corr.std().values:.4f}")

# ç¹ªåœ–
plt.figure(figsize=(10, 4))
spatial_corr.plot(marker='o')
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.title('Spatial Correlation over Time', fontsize=13)
plt.ylabel('Pearson r', fontsize=12)
plt.xlabel('Time', fontsize=12)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# ===== Cell 51 =====
# è¨ˆç®— RMSE
rmse = xs.rmse(pred_da, target_da, dim=['time', 'latitude', 'longitude'])

print(f"Overall RMSE: {rmse.values:.4f}")

# ä¹Ÿå¯ä»¥è¨ˆç®—æ¯å€‹æ ¼é»çš„æ™‚é–“ RMSE
rmse_spatial = xs.rmse(pred_da, target_da, dim='time')

plt.figure(figsize=(10, 6))
rmse_spatial.plot(cmap='YlOrRd', vmin=0)
plt.title('RMSE by Location (averaged over time)', fontsize=13)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()
plt.show()

print("\nInterpretation:")
print("ç´…è‰²å€åŸŸï¼šæ¨¡å‹é æ¸¬èª¤å·®è¼ƒå¤§")
print("é»ƒè‰²/ç¶ è‰²ï¼šé æ¸¬è¼ƒæº–ç¢º")
print("å¯ä»¥å¹«åŠ©è­˜åˆ¥æ¨¡å‹åœ¨å“ªäº›åœ°ç†ä½ç½®è¡¨ç¾è¼ƒå·®")

# ===== Cell 56 =====
# é—œé–‰ Dask Client
# client.close()

print("Workshop completed! ğŸ‰")
