{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: å¾ Xarray åˆ° ML Pipeline (50 min)\n",
    "\n",
    "é€™å€‹ notebook æ˜¯æ•´å€‹ workshop çš„æ ¸å¿ƒï¼Œå±•ç¤ºå¦‚ä½•å°‡å¤§å‹ N-D array è³‡æ–™ç„¡ç¸«æ•´åˆåˆ°æ©Ÿå™¨å­¸ç¿’æµç¨‹ä¸­ã€‚\n",
    "\n",
    "## æœ¬ç¯€å…§å®¹\n",
    "\n",
    "1. **å®šç¾© ML ä»»å‹™**ï¼šå°æµåˆ†é¡ï¼ˆConvection Classificationï¼‰\n",
    "2. **å»ºç«‹ Labels**ï¼šå¾ CAPE å®šç¾©å°æµäº‹ä»¶\n",
    "3. **xbatcher**ï¼šç”¢ç”Ÿè¨“ç·´ç”¨çš„æ™‚ç©º patches\n",
    "4. **PyTorch æ•´åˆ**ï¼šä½¿ç”¨ `xbatcher.loaders.torch.MapDataset`\n",
    "5. **æ¨¡å‹è¨“ç·´**ï¼šç°¡å–®çš„ CNN åˆ†é¡å™¨\n",
    "6. **ç©ºé–“é©—è­‰**ï¼šä½¿ç”¨ xskillscore è©•ä¼°é æ¸¬å“è³ª\n",
    "\n",
    "---\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "- ç†è§£ã€Œæ™‚ç©º batchã€çš„æ¦‚å¿µ\n",
    "- æŒæ¡ xbatcher çš„å…©éšæ®µè¨­è¨ˆï¼ˆBatchGenerator â†’ MapDatasetï¼‰\n",
    "- æ­£ç¢ºè¨­å®š DataLoader åƒæ•¸ï¼ˆbatch_size=Noneï¼ï¼‰\n",
    "- å¾ Xarray åˆ° PyTorch Tensor çš„è³‡æ–™æµ\n",
    "- ä¿ç•™ç©ºé–“è³‡è¨Šé€²è¡Œé©—è­‰ï¼ˆvs ä¸Ÿæ£„ç©ºé–“è³‡è¨Šçš„å‚³çµ± MLï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "import intake\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# å•Ÿå‹• Dask Client\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "# è¼‰å…¥ catalog\n",
    "catalog = intake.open_catalog('catalog.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å®šç¾© ML ä»»å‹™ï¼šå°æµåˆ†é¡\n",
    "\n",
    "### ä»€éº¼æ˜¯å°æµåˆ†é¡ï¼Ÿ\n",
    "\n",
    "å°æµï¼ˆconvectionï¼‰æ˜¯æŒ‡ç©ºæ°£å‚ç›´é‹å‹•å°è‡´çš„åŠ‡çƒˆå¤©æ°£ç¾è±¡ï¼š\n",
    "- é›·æš´ï¼ˆthunderstormsï¼‰\n",
    "- å¼·é™é›¨\n",
    "- å†°é›¹ã€é¾æ²é¢¨\n",
    "\n",
    "é å ±å°æµéå¸¸é‡è¦ï¼Œä½†å‚³çµ±æ•¸å€¼æ¨¡å¼çš„è§£æåº¦ä¸è¶³ã€‚æˆ‘å€‘å¸Œæœ›ç”¨ ML å¾å¤§å°ºåº¦è®Šæ•¸é æ¸¬å°å°ºåº¦å°æµã€‚\n",
    "\n",
    "### ä»»å‹™å®šç¾©\n",
    "\n",
    "**è¼¸å…¥ï¼ˆFeaturesï¼‰**ï¼š\n",
    "- CAPEï¼ˆConvective Available Potential Energyï¼‰ï¼šå°æµå¯ç”¨ä½èƒ½\n",
    "- CINï¼ˆConvective Inhibitionï¼‰ï¼šå°æµæŠ‘åˆ¶\n",
    "- K-indexï¼šä¸ç©©å®šæŒ‡æ•¸\n",
    "- BLHï¼ˆBoundary Layer Heightï¼‰ï¼šé‚Šç•Œå±¤é«˜åº¦\n",
    "\n",
    "**è¼¸å‡ºï¼ˆLabelï¼‰**ï¼š\n",
    "- äºŒå…ƒåˆ†é¡ï¼šæ˜¯å¦ç™¼ç”Ÿå°æµï¼ˆ0 or 1ï¼‰\n",
    "\n",
    "### Label çš„å®šç¾©\n",
    "\n",
    "å¯¦å‹™ä¸Šï¼Œå°æµçš„å®šç¾©å¯ä»¥æ˜¯ï¼š\n",
    "- é›·é”å›æ³¢ > 40 dBZï¼ˆéœ€è¦é›·é”è³‡æ–™ï¼‰\n",
    "- é™é›¨å¼·åº¦ > 10 mm/hrï¼ˆéœ€è¦é›¨é‡è³‡æ–™ï¼‰\n",
    "- **ç°¡åŒ–ç‰ˆ**ï¼šCAPE > æŸå€‹é–¾å€¼ + CIN < æŸå€‹é–¾å€¼\n",
    "\n",
    "é€™å€‹ workshop æˆ‘å€‘ç”¨ç°¡åŒ–ç‰ˆï¼Œé‡é»æ˜¯å±•ç¤ºæµç¨‹ï¼Œä¸æ˜¯é å ±ç²¾åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æº–å‚™è³‡æ–™èˆ‡å»ºç«‹ Labels\n",
    "\n",
    "### 2.1 è¼‰å…¥è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ 2019 å¹´è³‡æ–™ï¼ˆæˆ–ä½¿ç”¨å‰é¢å„²å­˜çš„å„ªåŒ–ç‰ˆï¼‰\n",
    "ds = catalog.era5_2019.to_dask()\n",
    "\n",
    "# Resample åˆ° daily ä»¥æ¸›å°‘è³‡æ–™é‡\n",
    "# å°æ–¼ MLï¼Œæˆ‘å€‘é€šå¸¸ä¸éœ€è¦ hourly è§£æåº¦\n",
    "ds_daily = ds.resample(time='1D').mean()\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(ds_daily)\n",
    "print()\n",
    "print(f\"Shape: {ds_daily['cape'].shape}\")\n",
    "print(f\"Total size: {ds_daily.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 å»ºç«‹å°æµ Label\n",
    "\n",
    "æˆ‘å€‘å®šç¾©å°æµç™¼ç”Ÿçš„æ¢ä»¶ï¼š\n",
    "- CAPE > 1000 J/kgï¼ˆæœ‰è¶³å¤ çš„ä¸ç©©å®šèƒ½é‡ï¼‰\n",
    "- CIN > -50 J/kgï¼ˆæŠ‘åˆ¶ä¸æœƒå¤ªå¼·ï¼‰\n",
    "\n",
    "é€™å€‹é–¾å€¼æ˜¯ç°¡åŒ–çš„ï¼Œå¯¦å‹™ä¸Šéœ€è¦æ ¹æ“šç•¶åœ°æ°£å€™èª¿æ•´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ binary label\n",
    "convection_flag = (\n",
    "    (ds_daily['cape'] > 1000) & \n",
    "    (ds_daily['cin'] > -50)\n",
    ").astype(np.float32)  # è½‰ç‚º float32 ä»¥ä¾¿èˆ‡ features ç›¸å®¹\n",
    "\n",
    "# åŠ å…¥ Dataset\n",
    "ds_daily['convection_flag'] = convection_flag\n",
    "\n",
    "print(\"Convection flag:\")\n",
    "print(ds_daily['convection_flag'])\n",
    "print()\n",
    "\n",
    "# æª¢æŸ¥ class balance\n",
    "flag_mean = convection_flag.mean().compute()\n",
    "print(f\"Convection occurrence rate: {flag_mean.values * 100:.2f}%\")\n",
    "print(f\"  Class 0 (no convection): {(1 - flag_mean.values) * 100:.2f}%\")\n",
    "print(f\"  Class 1 (convection): {flag_mean.values * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "å¦‚æœå°æµç™¼ç”Ÿç‡å¾ˆä½ï¼ˆä¾‹å¦‚ < 10%ï¼‰ï¼Œé€™æ˜¯å…¸å‹çš„ imbalanced classification problemã€‚\n",
    "\n",
    "è™•ç†æ–¹æ³•ï¼ˆæœ¬ workshop ä¸æ·±å…¥å¯¦ä½œï¼‰ï¼š\n",
    "1. **Weighted loss**ï¼šçµ¦å°‘æ•¸é¡æ›´é«˜æ¬Šé‡\n",
    "2. **Oversampling**ï¼šå¤šæ¡æ¨£å°æµäº‹ä»¶\n",
    "3. **Focal loss**ï¼šå°ˆæ³¨æ–¼é›£åˆ†é¡çš„æ¨£æœ¬\n",
    "\n",
    "é€™è£¡æˆ‘å€‘å…ˆç”¨ç°¡å–®çš„ cross-entropy lossï¼Œé‡é»æ˜¯æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 é¸å–ç‰¹å®šå€åŸŸèˆ‡æ™‚é–“æ®µ\n",
    "\n",
    "ç‚ºäº†åŠ é€Ÿç¤ºç¯„ï¼Œæˆ‘å€‘é¸å–ï¼š\n",
    "- æ™‚é–“ï¼š2019 å¹´ 6-8 æœˆï¼ˆå¤å­£ï¼Œå°æµæ—ºç››ï¼‰\n",
    "- ç©ºé–“ï¼š20-30Â°N, 110-125Â°Eï¼ˆè¯å—åœ°å€ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¸å–å­é›†\n",
    "ds_subset = ds_daily.sel(\n",
    "    time=slice('2019-06-01', '2019-08-31'),\n",
    "    latitude=slice(20, 30),\n",
    "    longitude=slice(110, 125)\n",
    ")\n",
    "\n",
    "print(\"Subset for training:\")\n",
    "print(ds_subset)\n",
    "print()\n",
    "print(f\"Time steps: {len(ds_subset.time)}\")\n",
    "print(f\"Spatial shape: {len(ds_subset.latitude)} x {len(ds_subset.longitude)}\")\n",
    "print(f\"Total size: {ds_subset.nbytes / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è³‡æ–™åˆ†å‰²ï¼šTrain / Val / Test\n",
    "\n",
    "### æ™‚é–“åºåˆ—åˆ†å‰²çš„é‡è¦æ€§\n",
    "\n",
    "å°æ–¼æ™‚é–“åºåˆ—è³‡æ–™ï¼Œ**ä¸èƒ½éš¨æ©Ÿåˆ†å‰²**ï¼\n",
    "\n",
    "åŸå› ï¼š\n",
    "- ç›¸é„°æ™‚é–“é»é«˜åº¦ç›¸é—œï¼ˆtemporal autocorrelationï¼‰\n",
    "- éš¨æ©Ÿåˆ†å‰²æœƒã€Œæ´©éœ²æœªä¾†è³‡è¨Šã€åˆ°è¨“ç·´é›†\n",
    "- æ¨¡å‹æœƒéåº¦æ“¬åˆçŸ­æœŸè®ŠåŒ–\n",
    "\n",
    "æ­£ç¢ºåšæ³•ï¼š**æŒ‰æ™‚é–“é †åºåˆ†å‰²**\n",
    "- Training: å‰ 70%\n",
    "- Validation: ä¸­é–“ 15%\n",
    "- Test: æœ€å¾Œ 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—åˆ†å‰²é»\n",
    "n_total = len(ds_subset.time)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.15)\n",
    "\n",
    "# æ™‚é–“åºåˆ—åˆ†å‰²\n",
    "train_ds = ds_subset.isel(time=slice(0, n_train))\n",
    "val_ds = ds_subset.isel(time=slice(n_train, n_train + n_val))\n",
    "test_ds = ds_subset.isel(time=slice(n_train + n_val, None))\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training: {len(train_ds.time)} days ({train_ds.time.values[0]} to {train_ds.time.values[-1]})\")\n",
    "print(f\"  Validation: {len(val_ds.time)} days ({val_ds.time.values[0]} to {val_ds.time.values[-1]})\")\n",
    "print(f\"  Test: {len(test_ds.time)} days ({test_ds.time.values[0]} to {test_ds.time.values[-1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. xbatcherï¼šç”¢ç”Ÿæ™‚ç©º Patches\n",
    "\n",
    "### ä»€éº¼æ˜¯æ™‚ç©º Batchï¼Ÿ\n",
    "\n",
    "å‚³çµ± ML çš„ batchï¼š\n",
    "- å¾ N å€‹æ¨£æœ¬ä¸­éš¨æ©Ÿé¸ B å€‹\n",
    "- æ¯å€‹æ¨£æœ¬æ˜¯ç¨ç«‹çš„ feature vector\n",
    "\n",
    "æ™‚ç©º batchï¼š\n",
    "- å¾ 3D/4D array ä¸­åˆ‡å‡ºå°çš„ã€Œpatchesã€\n",
    "- æ¯å€‹ patch åŒ…å«æ™‚é–“å’Œç©ºé–“ç¶­åº¦\n",
    "- ä¿ç•™æ™‚ç©ºçµæ§‹ï¼ˆå° CNN/RNN å¾ˆé‡è¦ï¼‰\n",
    "\n",
    "### xbatcher çš„è¨­è¨ˆ\n",
    "\n",
    "xbatcher æä¾›å…©éšæ®µæµç¨‹ï¼š\n",
    "\n",
    "**Stage 1: BatchGenerator**\n",
    "- å®šç¾©å¦‚ä½•å¾ Xarray Dataset åˆ‡å‡º batches\n",
    "- æŒ‡å®š `input_dims`ï¼ˆç©ºé–“å¤§å°ï¼‰å’Œ `batch_dims`ï¼ˆæ™‚é–“å¤§å°ï¼‰\n",
    "- ä»ç„¶æ˜¯ **lazy** çš„ï¼ˆä¸æœƒå¯¦éš›è®€å–è³‡æ–™ï¼‰\n",
    "\n",
    "**Stage 2: MapDataset**\n",
    "- å°‡ BatchGenerator åŒ…è£æˆ PyTorch Dataset\n",
    "- è™•ç†å¾ Xarray â†’ NumPy â†’ Tensor çš„è½‰æ›\n",
    "- å¯ä»¥æ­é… DataLoader åš shufflingã€multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stage 1: å‰µå»º BatchGenerator\n",
    "\n",
    "æˆ‘å€‘éœ€è¦**åˆ†åˆ¥**ç‚º features å’Œ labels å‰µå»º BatchGeneratorã€‚\n",
    "\n",
    "#### åƒæ•¸èªªæ˜\n",
    "\n",
    "- **input_dims**: ç©ºé–“ç¶­åº¦çš„å¤§å°ï¼ˆä¸æœƒæ²¿è‘—é€™äº›ç¶­åº¦åˆ‡åˆ†ï¼‰\n",
    "  - ä¾‹å¦‚ `{'latitude': 16, 'longitude': 16}` è¡¨ç¤ºæ¯å€‹ patch æ˜¯ 16x16\n",
    "  - å¦‚æœè³‡æ–™æœ‰ 40 å€‹ latitude é»ï¼Œæœƒç”¢ç”Ÿ 40/16 = 2.5 â†’ 3 å€‹ patchesï¼ˆæœ‰ overlapï¼‰\n",
    "\n",
    "- **batch_dims**: æœƒåˆ‡åˆ†çš„ç¶­åº¦ï¼ˆé€šå¸¸æ˜¯æ™‚é–“ï¼‰\n",
    "  - ä¾‹å¦‚ `{'time': 32}` è¡¨ç¤ºæ¯å€‹ batch åŒ…å« 32 å€‹æ™‚é–“æ­¥\n",
    "  - å¦‚æœè³‡æ–™æœ‰ 100 å¤©ï¼Œæœƒç”¢ç”Ÿ 100/32 â‰ˆ 3 å€‹ batches\n",
    "\n",
    "- **preload_batch**: æ˜¯å¦é å…ˆè¼‰å…¥æ•´å€‹ batch åˆ°è¨˜æ†¶é«”\n",
    "  - `False`ï¼ˆæ¨è–¦ï¼‰ï¼šä¿æŒ lazyï¼Œåªåœ¨è¿­ä»£æ™‚è¼‰å…¥\n",
    "  - `True`ï¼šæœƒé å…ˆ .compute()ï¼Œå¯èƒ½è¨˜æ†¶é«”ä¸è¶³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾© feature è®Šæ•¸\n",
    "feature_vars = ['cape', 'cin', 'k_index', 'blh']\n",
    "\n",
    "# Stage 1a: ç‚º features å‰µå»º BatchGenerator\n",
    "X_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},  # 16x16 ç©ºé–“ patches\n",
    "    batch_dims={'time': 32},                       # 32 time steps per batch\n",
    "    preload_batch=False  # ä¿æŒ lazy evaluation\n",
    ")\n",
    "\n",
    "# Stage 1b: ç‚º labels å‰µå»º BatchGenerator\n",
    "y_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds['convection_flag'],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "print(\"BatchGenerators created:\")\n",
    "print(f\"  X_bgen: {len(list(X_bgen))} batches\")\n",
    "print(f\"  y_bgen: {len(list(y_bgen))} batches\")\n",
    "print()\n",
    "print(\"Note: ä¸Šé¢çš„ list() æœƒå¯¦éš›è¿­ä»£ï¼Œåªæ˜¯ç‚ºäº†è¨ˆæ•¸ã€‚\")\n",
    "print(\"      å¯¦éš›ä½¿ç”¨æ™‚ä¸éœ€è¦é€™æ¨£åšã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æª¢è¦–å–®ä¸€ Batch çš„çµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°å‰µå»ºï¼ˆå› ç‚º generator å·²ç¶“è¢«æ¶ˆè€—äº†ï¼‰\n",
    "X_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "# å–å¾—ç¬¬ä¸€å€‹ batch\n",
    "first_batch = next(iter(X_bgen))\n",
    "\n",
    "print(\"First batch (still lazy):\")\n",
    "print(first_batch)\n",
    "print()\n",
    "print(f\"Dimensions: {first_batch.dims}\")\n",
    "print(f\"Shape: {first_batch.dims}\")\n",
    "print(f\"Variables: {list(first_batch.data_vars)}\")\n",
    "print()\n",
    "print(f\"CAPE shape in this batch: {first_batch['cape'].shape}\")\n",
    "print(f\"Type: {type(first_batch['cape'].data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç†è§£ Batch Shape\n",
    "\n",
    "åŸå§‹è³‡æ–™ï¼š`(time, latitude, longitude)`\n",
    "- time: ä¾‹å¦‚ 64 å¤©\n",
    "- latitude: 40 é»\n",
    "- longitude: 60 é»\n",
    "\n",
    "ç¶“é xbatcherï¼š`(time, latitude, longitude)`\n",
    "- time: 32ï¼ˆbatch_dimsï¼‰\n",
    "- latitude: 16ï¼ˆinput_dimsï¼‰\n",
    "- longitude: 16ï¼ˆinput_dimsï¼‰\n",
    "\n",
    "é€™å€‹ shape æœƒè¢«é€å…¥ CNNã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: PyTorch æ•´åˆ\n",
    "\n",
    "### 5.1 ä½¿ç”¨ xbatcher.loaders.torch.MapDataset\n",
    "\n",
    "**é‡è¦**ï¼šé€™æ˜¯ xbatcher å®˜æ–¹æä¾›çš„ PyTorch æ•´åˆæ–¹å¼ã€‚\n",
    "\n",
    "ä¸è¦è‡ªå·±å¯« `Dataset` wrapperï¼xbatcher å·²ç¶“è™•ç†å¥½äº†ï¼š\n",
    "- Xarray â†’ NumPy è½‰æ›\n",
    "- NumPy â†’ Tensor è½‰æ›\n",
    "- ç¶­åº¦é †åºèª¿æ•´ï¼ˆXarray æ˜¯ (time, lat, lon)ï¼ŒPyTorch æ…£ä¾‹æ˜¯ (batch, channel, height, width)ï¼‰\n",
    "- Lazy loading ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°å‰µå»º BatchGeneratorsï¼ˆç¢ºä¿æ²’è¢«æ¶ˆè€—ï¼‰\n",
    "X_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "y_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds['convection_flag'],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "# Stage 2: ä½¿ç”¨å®˜æ–¹çš„ MapDataset\n",
    "train_dataset = xbatcher.loaders.torch.MapDataset(\n",
    "    X_bgen,  # features\n",
    "    y_bgen   # labels\n",
    ")\n",
    "\n",
    "print(f\"PyTorch Dataset created: {len(train_dataset)} batches\")\n",
    "print(f\"Type: {type(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 æª¢è¦– Dataset å›å‚³çš„è³‡æ–™\n",
    "\n",
    "MapDataset å›å‚³ `(X, y)` tupleï¼Œå…¶ä¸­ï¼š\n",
    "- X: Tensor of shape `(batch_dim, n_features, height, width)`\n",
    "- y: Tensor of shape `(batch_dim, height, width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å–å¾—ä¸€å€‹æ¨£æœ¬\n",
    "X_sample, y_sample = train_dataset[0]\n",
    "\n",
    "print(\"Sample from Dataset:\")\n",
    "print(f\"  X type: {type(X_sample)}\")\n",
    "print(f\"  X shape: {X_sample.shape}\")\n",
    "print(f\"  X dtype: {X_sample.dtype}\")\n",
    "print()\n",
    "print(f\"  y type: {type(y_sample)}\")\n",
    "print(f\"  y shape: {y_sample.shape}\")\n",
    "print(f\"  y dtype: {y_sample.dtype}\")\n",
    "print()\n",
    "print(\"Shape interpretation:\")\n",
    "print(f\"  X: (time={X_sample.shape[0]}, features={X_sample.shape[1]}, lat={X_sample.shape[2]}, lon={X_sample.shape[3]})\")\n",
    "print(f\"  y: (time={y_sample.shape[0]}, lat={y_sample.shape[1]}, lon={y_sample.shape[2]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 å‰µå»º DataLoader\n",
    "\n",
    "**é—œéµåƒæ•¸**ï¼š`batch_size=None`\n",
    "\n",
    "ç‚ºä»€éº¼ï¼Ÿ\n",
    "- xbatcher å·²ç¶“å®šç¾©äº† batchï¼ˆé€é `batch_dims`ï¼‰\n",
    "- DataLoader çš„ `batch_size` æ˜¯ç”¨ä¾†ã€ŒæŠŠå¤šå€‹æ¨£æœ¬çµ„æˆä¸€å€‹ batchã€\n",
    "- ä½†æˆ‘å€‘çš„ã€Œä¸€å€‹æ¨£æœ¬ã€å·²ç¶“æ˜¯ä¸€å€‹ batchï¼ˆ32 time stepsï¼‰\n",
    "- å¦‚æœè¨­å®š `batch_size=4`ï¼Œæœƒè®Šæˆ `(4, 32, 4, 16, 16)`ï¼ˆéŒ¯èª¤ï¼ï¼‰\n",
    "\n",
    "**æ­£ç¢ºè¨­å®š**ï¼š\n",
    "```python\n",
    "DataLoader(dataset, batch_size=None, ...)\n",
    "```\n",
    "\n",
    "é€™æ¨£ DataLoader åªè² è²¬ï¼š\n",
    "- Shufflingï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "- Multiprocessingï¼ˆnum_workersï¼‰\n",
    "- ä¸æœƒæ”¹è®Š batch çš„ shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=None,  # ä¸è¦å†å¢åŠ  batch ç¶­åº¦ï¼\n",
    "    shuffle=True,     # æ‰“äº‚ batches é †åºï¼ˆä¸æ˜¯æ‰“äº‚ batch å…§çš„é †åºï¼‰\n",
    "    num_workers=2,    # å¹³è¡Œè¼‰å…¥è³‡æ–™\n",
    "    multiprocessing_context='forkserver'  # é¿å… Dask client pickle å•é¡Œ\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created: {len(train_loader)} batches\")\n",
    "print()\n",
    "print(\"Parameters:\")\n",
    "print(f\"  batch_size: None (xbatcher already defines batch)\")\n",
    "print(f\"  shuffle: True\")\n",
    "print(f\"  num_workers: 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 æ¸¬è©¦ DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿­ä»£å–å¾—ä¸€å€‹ batch\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(\"Batch from DataLoader:\")\n",
    "    print(f\"  X: {X_batch.shape}, dtype: {X_batch.dtype}\")\n",
    "    print(f\"  y: {y_batch.shape}, dtype: {y_batch.dtype}\")\n",
    "    print()\n",
    "    print(f\"  X min/max: {X_batch.min():.2f} / {X_batch.max():.2f}\")\n",
    "    print(f\"  y unique values: {torch.unique(y_batch)}\")\n",
    "    break  # åªçœ‹ç¬¬ä¸€å€‹ batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å®šç¾© CNN æ¨¡å‹\n",
    "\n",
    "æˆ‘å€‘ä½¿ç”¨ä¸€å€‹ç°¡å–®çš„ 3D CNNï¼ˆæ™‚ç©º convolutionï¼‰ï¼š\n",
    "- Input: `(batch, channels, time, height, width)` = (32, 4, 32, 16, 16)\n",
    "- Output: `(batch, time, height, width)` = (32, 32, 16, 16)\n",
    "\n",
    "æ¨¡å‹æ¶æ§‹ï¼š\n",
    "1. Conv3D + ReLU + MaxPool\n",
    "2. Conv3D + ReLU + MaxPool  \n",
    "3. Conv3Dï¼ˆoutput layerï¼Œsigmoid æ¿€æ´»ï¼‰\n",
    "\n",
    "é€™åªæ˜¯ç¤ºç¯„ï¼Œä¸æ˜¯ state-of-the-artã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvectionCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3D Convolutions (time + space)\n",
    "        self.conv1 = nn.Conv3d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 1, kernel_size=3, padding=1)  # output: 1 channel\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # for binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, time, height, width)\n",
    "        # ä½†æˆ‘å€‘çš„è³‡æ–™æ˜¯ (time, channels, height, width)\n",
    "        # éœ€è¦èª¿æ•´ç¶­åº¦é †åº\n",
    "        \n",
    "        # Permute: (time, channels, height, width) -> (1, channels, time, height, width)\n",
    "        # åŠ ä¸Š batch ç¶­åº¦ï¼ˆå› ç‚º batch_size=Noneï¼Œæ²’æœ‰ batch ç¶­åº¦ï¼‰\n",
    "        if x.dim() == 4:\n",
    "            x = x.unsqueeze(0)  # add batch dim\n",
    "        \n",
    "        # Permute: (batch, time, channels, height, width) -> (batch, channels, time, height, width)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        # Convolution layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.sigmoid(self.conv3(x))\n",
    "        \n",
    "        # Output: (batch, 1, time, height, width)\n",
    "        # Squeeze channel dim and permute back\n",
    "        x = x.squeeze(1)  # (batch, time, height, width)\n",
    "        \n",
    "        # Remove batch dim if added\n",
    "        if x.size(0) == 1:\n",
    "            x = x.squeeze(0)  # (time, height, width)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹\n",
    "model = SimpleConvectionCNN(in_channels=4)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# è¨ˆç®—åƒæ•¸æ•¸é‡\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦æ¨¡å‹ Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º dummy input\n",
    "dummy_input = torch.randn(32, 4, 16, 16)  # (time, channels, height, width)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "print()\n",
    "print(\"âœ“ Model forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è¨“ç·´è¿´åœˆ\n",
    "\n",
    "### 7.1 è¨­å®šè¨“ç·´åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®š device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training config\n",
    "n_epochs = 3  # çŸ­æœŸç¤ºç¯„ï¼Œå¯¦å‹™ä¸Šéœ€è¦æ›´å¤š epochs\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {n_epochs}\")\n",
    "print(f\"  Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"  Loss: Binary Cross Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 è¨“ç·´è¿´åœˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'loss': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Move to device\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    history['loss'].append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 ç¹ªè£½ Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, n_epochs+1), history['loss'], marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ¨¡å‹è©•ä¼°èˆ‡è¦–è¦ºåŒ–\n",
    "\n",
    "### 8.1 åœ¨ Test Set ä¸Šé æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‚º test set å‰µå»º DataLoader\n",
    "X_test_bgen = xbatcher.BatchGenerator(\n",
    "    test_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "y_test_bgen = xbatcher.BatchGenerator(\n",
    "    test_ds['convection_flag'],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "test_dataset = xbatcher.loaders.torch.MapDataset(X_test_bgen, y_test_bgen)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=None,\n",
    "    shuffle=False,  # test set ä¸ shuffle\n",
    "    num_workers=2,\n",
    "    multiprocessing_context='forkserver'\n",
    ")\n",
    "\n",
    "print(f\"Test set: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        predictions.append(outputs.cpu())\n",
    "        targets.append(y_batch.cpu())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Concatenate all predictions\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 è¨ˆç®—åˆ†é¡æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# è½‰ç‚º binary predictions (threshold = 0.5)\n",
    "pred_binary = (predictions > 0.5).float()\n",
    "\n",
    "# Flatten for sklearn\n",
    "pred_flat = pred_binary.flatten().numpy()\n",
    "target_flat = targets.flatten().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(target_flat, pred_flat)\n",
    "precision = precision_score(target_flat, pred_flat, zero_division=0)\n",
    "recall = recall_score(target_flat, pred_flat, zero_division=0)\n",
    "f1 = f1_score(target_flat, pred_flat, zero_division=0)\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 è¦–è¦ºåŒ–ï¼šé æ¸¬ vs çœŸå¯¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¸å–ä¸€å€‹æ™‚é–“æ­¥é©Ÿå’Œç©ºé–“ patch ä¾†è¦–è¦ºåŒ–\n",
    "t_idx = 10  # ç¬¬ 10 å€‹æ™‚é–“æ­¥\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# True labels\n",
    "im1 = axes[0].imshow(targets[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "axes[0].set_title('Ground Truth', fontsize=12)\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Predictions (probability)\n",
    "im2 = axes[1].imshow(predictions[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "axes[1].set_title('Predicted Probability', fontsize=12)\n",
    "axes[1].set_xlabel('Longitude')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Binary predictions\n",
    "im3 = axes[2].imshow(pred_binary[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "axes[2].set_title('Binary Prediction (>0.5)', fontsize=12)\n",
    "axes[2].set_xlabel('Longitude')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ä½¿ç”¨ xskillscore é€²è¡Œç©ºé–“é©—è­‰\n",
    "\n",
    "### ç‚ºä»€éº¼éœ€è¦ xskillscoreï¼Ÿ\n",
    "\n",
    "å‚³çµ± ML è©•ä¼°ï¼ˆaccuracy, F1ï¼‰æŠŠæ‰€æœ‰åƒç´ ç•¶ä½œç¨ç«‹æ¨£æœ¬ï¼Œå¿½ç•¥äº†ï¼š\n",
    "- **ç©ºé–“é€£çºŒæ€§**ï¼šç›¸é„°æ ¼é»çš„é æ¸¬æ‡‰è©²å¹³æ»‘\n",
    "- **ç©ºé–“å°ºåº¦**ï¼šå¤§ç¯„åœçš„éŒ¯èª¤ vs å°ç¯„åœçš„éŒ¯èª¤\n",
    "- **ç©ºé–“ç›¸é—œ**ï¼šé æ¸¬çš„ç©ºé–“çµæ§‹æ˜¯å¦åˆç†ï¼Ÿ\n",
    "\n",
    "xskillscore æä¾›ã€Œç©ºé–“æ„ŸçŸ¥ã€çš„é©—è­‰æŒ‡æ¨™ï¼š\n",
    "- **Spatial correlation**ï¼šé æ¸¬å ´èˆ‡çœŸå¯¦å ´çš„ç©ºé–“ç›¸é—œ\n",
    "- **RMSE by region**ï¼šä¸åŒå€åŸŸçš„èª¤å·®\n",
    "- **Fractions Skill Score**ï¼šè€ƒæ…®ç©ºé–“é„°åŸŸçš„é©—è­‰\n",
    "\n",
    "### ç‚ºä»€éº¼ xskillscore é©åˆé€™å€‹ workflowï¼Ÿ\n",
    "\n",
    "xskillscore ç›´æ¥æ¥å— **Xarray DataArray**ï¼š\n",
    "- ä¿ç•™ç¶“ç·¯åº¦åº§æ¨™\n",
    "- å¯ä»¥åšå€åŸŸåŠ æ¬Šï¼ˆarea-weighted metricsï¼‰\n",
    "- å¯ä»¥æ²¿è‘—ä¸åŒç¶­åº¦è¨ˆç®—ï¼ˆæ™‚é–“ã€ç©ºé–“ã€ensembleï¼‰\n",
    "\n",
    "é€™å°±æ˜¯ç‚ºä»€éº¼æˆ‘å€‘ç”¨ Xarray è€Œä¸æ˜¯ pandas/NumPy çš„åŸå› ä¹‹ä¸€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 å°‡é æ¸¬è½‰å› Xarray\n",
    "\n",
    "æˆ‘å€‘éœ€è¦æŠŠ PyTorch Tensor è½‰å› Xarrayï¼Œæ¢å¾©åº§æ¨™è³‡è¨Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨æ„ï¼šé€™è£¡æ˜¯ç°¡åŒ–ç‰ˆï¼Œå¯¦å‹™ä¸Šéœ€è¦æ­£ç¢ºå°æ‡‰æ¯å€‹ patch çš„åº§æ¨™\n",
    "# ç‚ºäº†ç¤ºç¯„ï¼Œæˆ‘å€‘å‡è¨­ predictions å’Œ test_ds çš„ç©ºé–“ç¯„åœç›¸åŒ\n",
    "\n",
    "# å–å¾—ä¸€å€‹ batch çš„åº§æ¨™\n",
    "sample_batch = next(iter(X_test_bgen))\n",
    "time_coords = sample_batch['time'].values\n",
    "lat_coords = sample_batch['latitude'].values\n",
    "lon_coords = sample_batch['longitude'].values\n",
    "\n",
    "# å‰µå»º Xarray DataArray\n",
    "pred_da = xr.DataArray(\n",
    "    predictions[:len(time_coords)].numpy(),  # é™åˆ¶åˆ°å¯¦éš›çš„æ™‚é–“é•·åº¦\n",
    "    dims=['time', 'latitude', 'longitude'],\n",
    "    coords={\n",
    "        'time': time_coords,\n",
    "        'latitude': lat_coords,\n",
    "        'longitude': lon_coords\n",
    "    },\n",
    "    name='convection_probability'\n",
    ")\n",
    "\n",
    "target_da = xr.DataArray(\n",
    "    targets[:len(time_coords)].numpy(),\n",
    "    dims=['time', 'latitude', 'longitude'],\n",
    "    coords={\n",
    "        'time': time_coords,\n",
    "        'latitude': lat_coords,\n",
    "        'longitude': lon_coords\n",
    "    },\n",
    "    name='convection_truth'\n",
    ")\n",
    "\n",
    "print(\"Predictions as Xarray:\")\n",
    "print(pred_da)\n",
    "print()\n",
    "print(\"Targets as Xarray:\")\n",
    "print(target_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 è¨ˆç®—ç©ºé–“ç›¸é—œä¿‚æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹æ™‚é–“æ­¥çš„ç©ºé–“ç›¸é—œ\n",
    "spatial_corr = xs.pearson_r(pred_da, target_da, dim=['latitude', 'longitude'])\n",
    "\n",
    "print(\"Spatial correlation (per time step):\")\n",
    "print(spatial_corr.values)\n",
    "print()\n",
    "print(f\"Mean spatial correlation: {spatial_corr.mean().values:.4f}\")\n",
    "print(f\"Std: {spatial_corr.std().values:.4f}\")\n",
    "\n",
    "# ç¹ªåœ–\n",
    "plt.figure(figsize=(10, 4))\n",
    "spatial_corr.plot(marker='o')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Spatial Correlation over Time', fontsize=13)\n",
    "plt.ylabel('Pearson r', fontsize=12)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 è¨ˆç®—ç©ºé–“ RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®— RMSE\n",
    "rmse = xs.rmse(pred_da, target_da, dim=['time', 'latitude', 'longitude'])\n",
    "\n",
    "print(f\"Overall RMSE: {rmse.values:.4f}\")\n",
    "\n",
    "# ä¹Ÿå¯ä»¥è¨ˆç®—æ¯å€‹æ ¼é»çš„æ™‚é–“ RMSE\n",
    "rmse_spatial = xs.rmse(pred_da, target_da, dim='time')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "rmse_spatial.plot(cmap='YlOrRd', vmin=0)\n",
    "plt.title('RMSE by Location (averaged over time)', fontsize=13)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"ç´…è‰²å€åŸŸï¼šæ¨¡å‹é æ¸¬èª¤å·®è¼ƒå¤§\")\n",
    "print(\"é»ƒè‰²/ç¶ è‰²ï¼šé æ¸¬è¼ƒæº–ç¢º\")\n",
    "print(\"å¯ä»¥å¹«åŠ©è­˜åˆ¥æ¨¡å‹åœ¨å“ªäº›åœ°ç†ä½ç½®è¡¨ç¾è¼ƒå·®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. å®Œæ•´ Workflow å›é¡§\n",
    "\n",
    "è®“æˆ‘å€‘å›é¡§æ•´å€‹å¾ã€Œå¤§å‹ N-D arrayã€åˆ°ã€ŒML æ¨¡å‹ã€çš„æµç¨‹ï¼š\n",
    "\n",
    "```\n",
    "1. è³‡æ–™å„²å­˜\n",
    "   Zarr (on-disk, chunked)\n",
    "   â†“\n",
    "   \n",
    "2. è³‡æ–™è®€å–\n",
    "   intake-xarray + Dask\n",
    "   â†“ (lazy)\n",
    "   \n",
    "3. å‰è™•ç†\n",
    "   - Resample (hourly â†’ daily)\n",
    "   - å»ºç«‹ labels\n",
    "   - æ™‚é–“åºåˆ—åˆ†å‰²\n",
    "   â†“ (still lazy)\n",
    "   \n",
    "4. Batch ç”Ÿæˆ\n",
    "   xbatcher.BatchGenerator\n",
    "   â†“\n",
    "   \n",
    "5. PyTorch æ•´åˆ\n",
    "   xbatcher.loaders.torch.MapDataset\n",
    "   â†“\n",
    "   \n",
    "6. è³‡æ–™è¼‰å…¥\n",
    "   DataLoader (batch_size=None)\n",
    "   â†“ (now eager, on-demand)\n",
    "   \n",
    "7. æ¨¡å‹è¨“ç·´\n",
    "   PyTorch training loop\n",
    "   â†“\n",
    "   \n",
    "8. é æ¸¬èˆ‡è©•ä¼°\n",
    "   - å‚³çµ± metrics (accuracy, F1)\n",
    "   - ç©ºé–“ metrics (xskillscore)\n",
    "   â†“\n",
    "   \n",
    "9. çµæœè¦–è¦ºåŒ–\n",
    "   Xarray + matplotlib\n",
    "```\n",
    "\n",
    "### é—œéµè¨­è¨ˆåŸå‰‡\n",
    "\n",
    "1. **Lazy as long as possible**\n",
    "   - ç›´åˆ° DataLoader è¿­ä»£æ™‚æ‰å¯¦éš›è®€å–è³‡æ–™\n",
    "   - æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨\n",
    "\n",
    "2. **ä¿ç•™å…ƒè³‡æ–™**\n",
    "   - ä½¿ç”¨ Xarray è€Œä¸æ˜¯ NumPy\n",
    "   - åº§æ¨™è³‡è¨Šå°é©—è­‰å’Œè¦–è¦ºåŒ–å¾ˆé‡è¦\n",
    "\n",
    "3. **æ‰¹æ¬¡è™•ç†**\n",
    "   - xbatcher è‡ªå‹•è™•ç†æ™‚ç©ºåˆ‡åˆ†\n",
    "   - ä¸éœ€è¦æ‰‹å‹•ç®¡ç†ç´¢å¼•\n",
    "\n",
    "4. **å¹³è¡ŒåŒ–**\n",
    "   - Dask è™•ç†è³‡æ–™è®€å–çš„å¹³è¡ŒåŒ–\n",
    "   - DataLoader çš„ num_workers è™•ç†å‰è™•ç†å¹³è¡ŒåŒ–\n",
    "   - GPU è™•ç†æ¨¡å‹è¨“ç·´å¹³è¡ŒåŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. å¸¸è¦‹å•é¡Œèˆ‡é™¤éŒ¯\n",
    "\n",
    "### Q1: DataLoader å ±éŒ¯ \"batch_size should be None\"\n",
    "\n",
    "**åŸå› **ï¼šxbatcher å·²ç¶“å®šç¾©äº† batchï¼Œä¸æ‡‰è©²å†ç”¨ DataLoader çš„ batch_sizeã€‚\n",
    "\n",
    "**è§£æ³•**ï¼š\n",
    "```python\n",
    "DataLoader(dataset, batch_size=None, ...)  # æ­£ç¢º\n",
    "DataLoader(dataset, batch_size=4, ...)     # éŒ¯èª¤ï¼\n",
    "```\n",
    "\n",
    "### Q2: Multiprocessing å ±éŒ¯ \"cannot pickle Client\"\n",
    "\n",
    "**åŸå› **ï¼šDask Client ç„¡æ³•è¢« pickleï¼Œä½† DataLoader çš„ multiprocessing éœ€è¦ pickleã€‚\n",
    "\n",
    "**è§£æ³•**ï¼š\n",
    "```python\n",
    "DataLoader(..., multiprocessing_context='forkserver')  # ä½¿ç”¨ forkserver\n",
    "# æˆ–\n",
    "DataLoader(..., num_workers=0)  # ä¸ä½¿ç”¨ multiprocessing\n",
    "```\n",
    "\n",
    "### Q3: è¨˜æ†¶é«”ä¸è¶³ï¼ˆOOMï¼‰\n",
    "\n",
    "**åŸå› **ï¼š\n",
    "- Batch å¤ªå¤§ï¼ˆæ™‚é–“æˆ–ç©ºé–“ç¶­åº¦ï¼‰\n",
    "- preload_batch=True\n",
    "- num_workers å¤ªå¤š\n",
    "\n",
    "**è§£æ³•**ï¼š\n",
    "1. æ¸›å° batch_dims æˆ– input_dims\n",
    "2. ç¢ºä¿ preload_batch=False\n",
    "3. æ¸›å°‘ num_workers\n",
    "4. èª¿æ•´ Dask Client çš„ memory_limit\n",
    "\n",
    "### Q4: è¨“ç·´å¾ˆæ…¢\n",
    "\n",
    "**å¯èƒ½åŸå› **ï¼š\n",
    "- I/O ç“¶é ¸ï¼šå¢åŠ  num_workers\n",
    "- Chunk å¤ªå°ï¼šè€ƒæ…® rechunk\n",
    "- CPU è¨ˆç®—ï¼šæª¢æŸ¥æ˜¯å¦æ­£ç¢ºä½¿ç”¨ GPU\n",
    "\n",
    "**é™¤éŒ¯**ï¼šè§€å¯Ÿ Dask Dashboardï¼Œçœ‹æ™‚é–“èŠ±åœ¨å“ªè£¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. å»¶ä¼¸æ–¹å‘\n",
    "\n",
    "é€™å€‹ workshop å±•ç¤ºäº†åŸºç¤æµç¨‹ï¼Œå¯¦å‹™ä¸Šå¯ä»¥å»¶ä¼¸ï¼š\n",
    "\n",
    "### è³‡æ–™é¢\n",
    "- åŠ å…¥æ›´å¤šè®Šæ•¸ï¼ˆæ¿•åº¦ã€é¢¨å ´ã€æº«åº¦å‰–é¢ï¼‰\n",
    "- å¤šè³‡æ–™ä¾†æºèåˆï¼ˆERA5 + è¡›æ˜Ÿ + åœ°é¢è§€æ¸¬ï¼‰\n",
    "- æ™‚é–“æ»¯å¾Œç‰¹å¾µï¼ˆt-1, t-2 å°æ™‚çš„è³‡æ–™ï¼‰\n",
    "\n",
    "### æ¨¡å‹é¢\n",
    "- æ›´è¤‡é›œçš„æ¶æ§‹ï¼ˆUNet, ResNet, Transformerï¼‰\n",
    "- åºåˆ—æ¨¡å‹ï¼ˆLSTM, GRUï¼‰ç”¨æ–¼æ™‚é–“åºåˆ—\n",
    "- Ensemble æ¨¡å‹\n",
    "\n",
    "### è¨“ç·´é¢\n",
    "- Class imbalance è™•ç†ï¼ˆweighted loss, focal lossï¼‰\n",
    "- Data augmentationï¼ˆspatial flip, rotationï¼‰\n",
    "- Transfer learningï¼ˆé è¨“ç·´æ¨¡å‹ï¼‰\n",
    "\n",
    "### é©—è­‰é¢\n",
    "- æ›´å¤šç©ºé–“æŒ‡æ¨™ï¼ˆFractions Skill Score, SALï¼‰\n",
    "- Case studyï¼ˆåˆ†æç‰¹å®šäº‹ä»¶ï¼‰\n",
    "- å€åŸŸåŒ–è©•ä¼°ï¼ˆå±±å€ vs å¹³åœ°ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ç¸½çµ\n",
    "\n",
    "å®Œæˆé€™å€‹ notebook å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š\n",
    "\n",
    "- [ ] å®šç¾©é©åˆ ML çš„æ°£è±¡ä»»å‹™\n",
    "- [ ] å»ºç«‹æœ‰æ„ç¾©çš„ labels\n",
    "- [ ] ä½¿ç”¨ xbatcher ç”¢ç”Ÿæ™‚ç©º batches\n",
    "- [ ] æ­£ç¢ºæ•´åˆ xbatcher èˆ‡ PyTorch\n",
    "- [ ] è¨­å®š DataLoaderï¼ˆbatch_size=None!ï¼‰\n",
    "- [ ] è¨“ç·´ä¸€å€‹ç°¡å–®çš„ CNN æ¨¡å‹\n",
    "- [ ] ä½¿ç”¨ xskillscore é€²è¡Œç©ºé–“é©—è­‰\n",
    "- [ ] ç†è§£å®Œæ•´çš„ out-of-core ML workflow\n",
    "\n",
    "### æ ¸å¿ƒè¦é»\n",
    "\n",
    "1. **xbatcher çš„å…©éšæ®µè¨­è¨ˆæ˜¯é—œéµ**\n",
    "   - BatchGenerator â†’ MapDataset\n",
    "   - ä¸è¦è‡ªå·±å¯« Dataset wrapper\n",
    "\n",
    "2. **batch_size=None é¿å…ç¶­åº¦éŒ¯èª¤**\n",
    "   - xbatcher å·²ç¶“å®šç¾© batch\n",
    "   - DataLoader åªè² è²¬ shuffling å’Œ multiprocessing\n",
    "\n",
    "3. **ä¿ç•™ç©ºé–“è³‡è¨Šå¾ˆé‡è¦**\n",
    "   - è½‰å› Xarray åšé©—è­‰\n",
    "   - ç©ºé–“ç›¸é—œæ€§æ˜¯æ°£è±¡é å ±çš„æ ¸å¿ƒ\n",
    "\n",
    "4. **Lazy evaluation è²«ç©¿æ•´å€‹æµç¨‹**\n",
    "   - å¾ Zarr è®€å–åˆ° DataLoader éƒ½æ˜¯ lazy\n",
    "   - åªåœ¨éœ€è¦æ™‚æ‰è¼‰å…¥è³‡æ–™\n",
    "\n",
    "é€™å€‹å·¥ä½œæµç¨‹å¯ä»¥æ“´å±•åˆ°ï¼š\n",
    "- æ›´å¤§çš„è³‡æ–™é›†ï¼ˆTB ç´šåˆ¥ï¼‰\n",
    "- æ›´è¤‡é›œçš„æ¨¡å‹ï¼ˆæ·±åº¦å­¸ç¿’ï¼‰\n",
    "- åˆ†æ•£å¼è¨“ç·´ï¼ˆå¤š GPUã€å¤šç¯€é»ï¼‰\n",
    "\n",
    "é‡é»æ˜¯**ç†è§£åŸç†**ï¼Œè€Œä¸æ˜¯è¨˜ä½ APIã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é—œé–‰ Dask Client\n",
    "# client.close()\n",
    "\n",
    "print(\"Workshop completed! ğŸ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
