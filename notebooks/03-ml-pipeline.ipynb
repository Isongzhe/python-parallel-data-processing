{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: 從 Xarray 到 ML Pipeline (50 min)\n",
    "\n",
    "這個 notebook 是整個 workshop 的核心，展示如何將大型 N-D array 資料無縫整合到機器學習流程中。\n",
    "\n",
    "## 本節內容\n",
    "\n",
    "1. **定義 ML 任務**：對流分類（Convection Classification）\n",
    "2. **建立 Labels**：從 CAPE 定義對流事件\n",
    "3. **xbatcher**：產生訓練用的時空 patches\n",
    "4. **PyTorch 整合**：使用 `xbatcher.loaders.torch.MapDataset`\n",
    "5. **模型訓練**：簡單的 CNN 分類器\n",
    "6. **空間驗證**：使用 xskillscore 評估預測品質\n",
    "\n",
    "---\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "- 理解「時空 batch」的概念\n",
    "- 掌握 xbatcher 的兩階段設計（BatchGenerator → MapDataset）\n",
    "- 正確設定 DataLoader 參數（batch_size=None！）\n",
    "- 從 Xarray 到 PyTorch Tensor 的資料流\n",
    "- 保留空間資訊進行驗證（vs 丟棄空間資訊的傳統 ML）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "import intake\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 啟動 Dask Client\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "# 載入 catalog\n",
    "catalog = intake.open_catalog('catalog.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定義 ML 任務：對流分類\n",
    "\n",
    "### 什麼是對流分類？\n",
    "\n",
    "對流（convection）是指空氣垂直運動導致的劇烈天氣現象：\n",
    "- 雷暴（thunderstorms）\n",
    "- 強降雨\n",
    "- 冰雹、龍捲風\n",
    "\n",
    "預報對流非常重要，但傳統數值模式的解析度不足。我們希望用 ML 從大尺度變數預測小尺度對流。\n",
    "\n",
    "### 任務定義\n",
    "\n",
    "**輸入（Features）**：\n",
    "- CAPE（Convective Available Potential Energy）：對流可用位能\n",
    "- CIN（Convective Inhibition）：對流抑制\n",
    "- K-index：不穩定指數\n",
    "- BLH（Boundary Layer Height）：邊界層高度\n",
    "\n",
    "**輸出（Label）**：\n",
    "- 二元分類：是否發生對流（0 or 1）\n",
    "\n",
    "### Label 的定義\n",
    "\n",
    "實務上，對流的定義可以是：\n",
    "- 雷達回波 > 40 dBZ（需要雷達資料）\n",
    "- 降雨強度 > 10 mm/hr（需要雨量資料）\n",
    "- **簡化版**：CAPE > 某個閾值 + CIN < 某個閾值\n",
    "\n",
    "這個 workshop 我們用簡化版，重點是展示流程，不是預報精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 準備資料與建立 Labels\n",
    "\n",
    "### 2.1 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 2019 年資料（或使用前面儲存的優化版）\n",
    "ds = catalog.era5_2019.to_dask()\n",
    "\n",
    "# Resample 到 daily 以減少資料量\n",
    "# 對於 ML，我們通常不需要 hourly 解析度\n",
    "ds_daily = ds.resample(time='1D').mean()\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(ds_daily)\n",
    "print()\n",
    "print(f\"Shape: {ds_daily['cape'].shape}\")\n",
    "print(f\"Total size: {ds_daily.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 建立對流 Label\n",
    "\n",
    "我們定義對流發生的條件：\n",
    "- CAPE > 1000 J/kg（有足夠的不穩定能量）\n",
    "- CIN > -50 J/kg（抑制不會太強）\n",
    "\n",
    "這個閾值是簡化的，實務上需要根據當地氣候調整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 binary label\n",
    "convection_flag = (\n",
    "    (ds_daily['cape'] > 1000) & \n",
    "    (ds_daily['cin'] > -50)\n",
    ").astype(np.float32)  # 轉為 float32 以便與 features 相容\n",
    "\n",
    "# 加入 Dataset\n",
    "ds_daily['convection_flag'] = convection_flag\n",
    "\n",
    "print(\"Convection flag:\")\n",
    "print(ds_daily['convection_flag'])\n",
    "print()\n",
    "\n",
    "# 檢查 class balance\n",
    "flag_mean = convection_flag.mean().compute()\n",
    "print(f\"Convection occurrence rate: {flag_mean.values * 100:.2f}%\")\n",
    "print(f\"  Class 0 (no convection): {(1 - flag_mean.values) * 100:.2f}%\")\n",
    "print(f\"  Class 1 (convection): {flag_mean.values * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "如果對流發生率很低（例如 < 10%），這是典型的 imbalanced classification problem。\n",
    "\n",
    "處理方法（本 workshop 不深入實作）：\n",
    "1. **Weighted loss**：給少數類更高權重\n",
    "2. **Oversampling**：多採樣對流事件\n",
    "3. **Focal loss**：專注於難分類的樣本\n",
    "\n",
    "這裡我們先用簡單的 cross-entropy loss，重點是流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 選取特定區域與時間段\n",
    "\n",
    "為了加速示範，我們選取：\n",
    "- 時間：2019 年 6-8 月（夏季，對流旺盛）\n",
    "- 空間：20-30°N, 110-125°E（華南地區）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選取子集\n",
    "ds_subset = ds_daily.sel(\n",
    "    time=slice('2019-06-01', '2019-08-31'),\n",
    "    latitude=slice(20, 30),\n",
    "    longitude=slice(110, 125)\n",
    ")\n",
    "\n",
    "print(\"Subset for training:\")\n",
    "print(ds_subset)\n",
    "print()\n",
    "print(f\"Time steps: {len(ds_subset.time)}\")\n",
    "print(f\"Spatial shape: {len(ds_subset.latitude)} x {len(ds_subset.longitude)}\")\n",
    "print(f\"Total size: {ds_subset.nbytes / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 資料分割：Train / Val / Test\n",
    "\n",
    "### 時間序列分割的重要性\n",
    "\n",
    "對於時間序列資料，**不能隨機分割**！\n",
    "\n",
    "原因：\n",
    "- 相鄰時間點高度相關（temporal autocorrelation）\n",
    "- 隨機分割會「洩露未來資訊」到訓練集\n",
    "- 模型會過度擬合短期變化\n",
    "\n",
    "正確做法：**按時間順序分割**\n",
    "- Training: 前 70%\n",
    "- Validation: 中間 15%\n",
    "- Test: 最後 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算分割點\n",
    "n_total = len(ds_subset.time)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.15)\n",
    "\n",
    "# 時間序列分割\n",
    "train_ds = ds_subset.isel(time=slice(0, n_train))\n",
    "val_ds = ds_subset.isel(time=slice(n_train, n_train + n_val))\n",
    "test_ds = ds_subset.isel(time=slice(n_train + n_val, None))\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training: {len(train_ds.time)} days ({train_ds.time.values[0]} to {train_ds.time.values[-1]})\")\n",
    "print(f\"  Validation: {len(val_ds.time)} days ({val_ds.time.values[0]} to {val_ds.time.values[-1]})\")\n",
    "print(f\"  Test: {len(test_ds.time)} days ({test_ds.time.values[0]} to {test_ds.time.values[-1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. xbatcher：產生時空 Patches\n",
    "\n",
    "### 什麼是時空 Batch？\n",
    "\n",
    "傳統 ML 的 batch：\n",
    "- 從 N 個樣本中隨機選 B 個\n",
    "- 每個樣本是獨立的 feature vector\n",
    "\n",
    "時空 batch：\n",
    "- 從 3D/4D array 中切出小的「patches」\n",
    "- 每個 patch 包含時間和空間維度\n",
    "- 保留時空結構（對 CNN/RNN 很重要）\n",
    "\n",
    "### xbatcher 的設計\n",
    "\n",
    "xbatcher 提供兩階段流程：\n",
    "\n",
    "**Stage 1: BatchGenerator**\n",
    "- 定義如何從 Xarray Dataset 切出 batches\n",
    "- 指定 `input_dims`（空間大小）和 `batch_dims`（時間大小）\n",
    "- 仍然是 **lazy** 的（不會實際讀取資料）\n",
    "\n",
    "**Stage 2: MapDataset**\n",
    "- 將 BatchGenerator 包裝成 PyTorch Dataset\n",
    "- 處理從 Xarray → NumPy → Tensor 的轉換\n",
    "- 可以搭配 DataLoader 做 shuffling、multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stage 1: 創建 BatchGenerator\n",
    "\n",
    "我們需要**分別**為 features 和 labels 創建 BatchGenerator。\n",
    "\n",
    "#### 參數說明\n",
    "\n",
    "- **input_dims**: 空間維度的大小（不會沿著這些維度切分）\n",
    "  - 例如 `{'latitude': 16, 'longitude': 16}` 表示每個 patch 是 16x16\n",
    "  - 如果資料有 40 個 latitude 點，會產生 40/16 = 2.5 → 3 個 patches（有 overlap）\n",
    "\n",
    "- **batch_dims**: 會切分的維度（通常是時間）\n",
    "  - 例如 `{'time': 32}` 表示每個 batch 包含 32 個時間步\n",
    "  - 如果資料有 100 天，會產生 100/32 ≈ 3 個 batches\n",
    "\n",
    "- **preload_batch**: 是否預先載入整個 batch 到記憶體\n",
    "  - `False`（推薦）：保持 lazy，只在迭代時載入\n",
    "  - `True`：會預先 .compute()，可能記憶體不足"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 feature 變數\n",
    "feature_vars = ['cape', 'cin', 'k_index', 'blh']\n",
    "\n",
    "# Stage 1a: 為 features 創建 BatchGenerator\n",
    "X_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},  # 16x16 空間 patches\n",
    "    batch_dims={'time': 32},                       # 32 time steps per batch\n",
    "    preload_batch=False  # 保持 lazy evaluation\n",
    ")\n",
    "\n",
    "# Stage 1b: 為 labels 創建 BatchGenerator\n",
    "y_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds['convection_flag'],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "print(\"BatchGenerators created:\")\n",
    "print(f\"  X_bgen: {len(list(X_bgen))} batches\")\n",
    "print(f\"  y_bgen: {len(list(y_bgen))} batches\")\n",
    "print()\n",
    "print(\"Note: 上面的 list() 會實際迭代，只是為了計數。\")\n",
    "print(\"      實際使用時不需要這樣做。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 檢視單一 Batch 的結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新創建（因為 generator 已經被消耗了）\n",
    "X_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "# 取得第一個 batch\n",
    "first_batch = next(iter(X_bgen))\n",
    "\n",
    "print(\"First batch (still lazy):\")\n",
    "print(first_batch)\n",
    "print()\n",
    "print(f\"Dimensions: {first_batch.dims}\")\n",
    "print(f\"Shape: {first_batch.dims}\")\n",
    "print(f\"Variables: {list(first_batch.data_vars)}\")\n",
    "print()\n",
    "print(f\"CAPE shape in this batch: {first_batch['cape'].shape}\")\n",
    "print(f\"Type: {type(first_batch['cape'].data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解 Batch Shape\n",
    "\n",
    "原始資料：`(time, latitude, longitude)`\n",
    "- time: 例如 64 天\n",
    "- latitude: 40 點\n",
    "- longitude: 60 點\n",
    "\n",
    "經過 xbatcher：`(time, latitude, longitude)`\n",
    "- time: 32（batch_dims）\n",
    "- latitude: 16（input_dims）\n",
    "- longitude: 16（input_dims）\n",
    "\n",
    "這個 shape 會被送入 CNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: PyTorch 整合\n",
    "\n",
    "### 5.1 使用 xbatcher.loaders.torch.MapDataset\n",
    "\n",
    "**重要**：這是 xbatcher 官方提供的 PyTorch 整合方式。\n",
    "\n",
    "不要自己寫 `Dataset` wrapper！xbatcher 已經處理好了：\n",
    "- Xarray → NumPy 轉換\n",
    "- NumPy → Tensor 轉換\n",
    "- 維度順序調整（Xarray 是 (time, lat, lon)，PyTorch 慣例是 (batch, channel, height, width)）\n",
    "- Lazy loading 管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新創建 BatchGenerators（確保沒被消耗）\n",
    "X_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "y_bgen = xbatcher.BatchGenerator(\n",
    "    train_ds['convection_flag'],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "# Stage 2: 使用官方的 MapDataset\n",
    "train_dataset = xbatcher.loaders.torch.MapDataset(\n",
    "    X_bgen,  # features\n",
    "    y_bgen   # labels\n",
    ")\n",
    "\n",
    "print(f\"PyTorch Dataset created: {len(train_dataset)} batches\")\n",
    "print(f\"Type: {type(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 檢視 Dataset 回傳的資料\n",
    "\n",
    "MapDataset 回傳 `(X, y)` tuple，其中：\n",
    "- X: Tensor of shape `(batch_dim, n_features, height, width)`\n",
    "- y: Tensor of shape `(batch_dim, height, width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得一個樣本\n",
    "X_sample, y_sample = train_dataset[0]\n",
    "\n",
    "print(\"Sample from Dataset:\")\n",
    "print(f\"  X type: {type(X_sample)}\")\n",
    "print(f\"  X shape: {X_sample.shape}\")\n",
    "print(f\"  X dtype: {X_sample.dtype}\")\n",
    "print()\n",
    "print(f\"  y type: {type(y_sample)}\")\n",
    "print(f\"  y shape: {y_sample.shape}\")\n",
    "print(f\"  y dtype: {y_sample.dtype}\")\n",
    "print()\n",
    "print(\"Shape interpretation:\")\n",
    "print(f\"  X: (time={X_sample.shape[0]}, features={X_sample.shape[1]}, lat={X_sample.shape[2]}, lon={X_sample.shape[3]})\")\n",
    "print(f\"  y: (time={y_sample.shape[0]}, lat={y_sample.shape[1]}, lon={y_sample.shape[2]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 創建 DataLoader\n",
    "\n",
    "**關鍵參數**：`batch_size=None`\n",
    "\n",
    "為什麼？\n",
    "- xbatcher 已經定義了 batch（透過 `batch_dims`）\n",
    "- DataLoader 的 `batch_size` 是用來「把多個樣本組成一個 batch」\n",
    "- 但我們的「一個樣本」已經是一個 batch（32 time steps）\n",
    "- 如果設定 `batch_size=4`，會變成 `(4, 32, 4, 16, 16)`（錯誤！）\n",
    "\n",
    "**正確設定**：\n",
    "```python\n",
    "DataLoader(dataset, batch_size=None, ...)\n",
    "```\n",
    "\n",
    "這樣 DataLoader 只負責：\n",
    "- Shuffling（如果需要）\n",
    "- Multiprocessing（num_workers）\n",
    "- 不會改變 batch 的 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=None,  # 不要再增加 batch 維度！\n",
    "    shuffle=True,     # 打亂 batches 順序（不是打亂 batch 內的順序）\n",
    "    num_workers=2,    # 平行載入資料\n",
    "    multiprocessing_context='forkserver'  # 避免 Dask client pickle 問題\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created: {len(train_loader)} batches\")\n",
    "print()\n",
    "print(\"Parameters:\")\n",
    "print(f\"  batch_size: None (xbatcher already defines batch)\")\n",
    "print(f\"  shuffle: True\")\n",
    "print(f\"  num_workers: 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 測試 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代取得一個 batch\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(\"Batch from DataLoader:\")\n",
    "    print(f\"  X: {X_batch.shape}, dtype: {X_batch.dtype}\")\n",
    "    print(f\"  y: {y_batch.shape}, dtype: {y_batch.dtype}\")\n",
    "    print()\n",
    "    print(f\"  X min/max: {X_batch.min():.2f} / {X_batch.max():.2f}\")\n",
    "    print(f\"  y unique values: {torch.unique(y_batch)}\")\n",
    "    break  # 只看第一個 batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定義 CNN 模型\n",
    "\n",
    "我們使用一個簡單的 3D CNN（時空 convolution）：\n",
    "- Input: `(batch, channels, time, height, width)` = (32, 4, 32, 16, 16)\n",
    "- Output: `(batch, time, height, width)` = (32, 32, 16, 16)\n",
    "\n",
    "模型架構：\n",
    "1. Conv3D + ReLU + MaxPool\n",
    "2. Conv3D + ReLU + MaxPool  \n",
    "3. Conv3D（output layer，sigmoid 激活）\n",
    "\n",
    "這只是示範，不是 state-of-the-art。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvectionCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3D Convolutions (time + space)\n",
    "        self.conv1 = nn.Conv3d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 1, kernel_size=3, padding=1)  # output: 1 channel\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # for binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, time, height, width)\n",
    "        # 但我們的資料是 (time, channels, height, width)\n",
    "        # 需要調整維度順序\n",
    "        \n",
    "        # Permute: (time, channels, height, width) -> (1, channels, time, height, width)\n",
    "        # 加上 batch 維度（因為 batch_size=None，沒有 batch 維度）\n",
    "        if x.dim() == 4:\n",
    "            x = x.unsqueeze(0)  # add batch dim\n",
    "        \n",
    "        # Permute: (batch, time, channels, height, width) -> (batch, channels, time, height, width)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        # Convolution layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.sigmoid(self.conv3(x))\n",
    "        \n",
    "        # Output: (batch, 1, time, height, width)\n",
    "        # Squeeze channel dim and permute back\n",
    "        x = x.squeeze(1)  # (batch, time, height, width)\n",
    "        \n",
    "        # Remove batch dim if added\n",
    "        if x.size(0) == 1:\n",
    "            x = x.squeeze(0)  # (time, height, width)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 創建模型\n",
    "model = SimpleConvectionCNN(in_channels=4)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# 計算參數數量\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試模型 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 dummy input\n",
    "dummy_input = torch.randn(32, 4, 16, 16)  # (time, channels, height, width)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "print()\n",
    "print(\"✓ Model forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 訓練迴圈\n",
    "\n",
    "### 7.1 設定訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training config\n",
    "n_epochs = 3  # 短期示範，實務上需要更多 epochs\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {n_epochs}\")\n",
    "print(f\"  Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"  Loss: Binary Cross Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'loss': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Move to device\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    history['loss'].append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 繪製 Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, n_epochs+1), history['loss'], marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 模型評估與視覺化\n",
    "\n",
    "### 8.1 在 Test Set 上預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為 test set 創建 DataLoader\n",
    "X_test_bgen = xbatcher.BatchGenerator(\n",
    "    test_ds[feature_vars],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "y_test_bgen = xbatcher.BatchGenerator(\n",
    "    test_ds['convection_flag'],\n",
    "    input_dims={'latitude': 16, 'longitude': 16},\n",
    "    batch_dims={'time': 32},\n",
    "    preload_batch=False\n",
    ")\n",
    "\n",
    "test_dataset = xbatcher.loaders.torch.MapDataset(X_test_bgen, y_test_bgen)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=None,\n",
    "    shuffle=False,  # test set 不 shuffle\n",
    "    num_workers=2,\n",
    "    multiprocessing_context='forkserver'\n",
    ")\n",
    "\n",
    "print(f\"Test set: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        predictions.append(outputs.cpu())\n",
    "        targets.append(y_batch.cpu())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Concatenate all predictions\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 計算分類指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 轉為 binary predictions (threshold = 0.5)\n",
    "pred_binary = (predictions > 0.5).float()\n",
    "\n",
    "# Flatten for sklearn\n",
    "pred_flat = pred_binary.flatten().numpy()\n",
    "target_flat = targets.flatten().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(target_flat, pred_flat)\n",
    "precision = precision_score(target_flat, pred_flat, zero_division=0)\n",
    "recall = recall_score(target_flat, pred_flat, zero_division=0)\n",
    "f1 = f1_score(target_flat, pred_flat, zero_division=0)\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 視覺化：預測 vs 真實"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選取一個時間步驟和空間 patch 來視覺化\n",
    "t_idx = 10  # 第 10 個時間步\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# True labels\n",
    "im1 = axes[0].imshow(targets[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "axes[0].set_title('Ground Truth', fontsize=12)\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Predictions (probability)\n",
    "im2 = axes[1].imshow(predictions[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "axes[1].set_title('Predicted Probability', fontsize=12)\n",
    "axes[1].set_xlabel('Longitude')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Binary predictions\n",
    "im3 = axes[2].imshow(pred_binary[t_idx], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "axes[2].set_title('Binary Prediction (>0.5)', fontsize=12)\n",
    "axes[2].set_xlabel('Longitude')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 使用 xskillscore 進行空間驗證\n",
    "\n",
    "### 為什麼需要 xskillscore？\n",
    "\n",
    "傳統 ML 評估（accuracy, F1）把所有像素當作獨立樣本，忽略了：\n",
    "- **空間連續性**：相鄰格點的預測應該平滑\n",
    "- **空間尺度**：大範圍的錯誤 vs 小範圍的錯誤\n",
    "- **空間相關**：預測的空間結構是否合理？\n",
    "\n",
    "xskillscore 提供「空間感知」的驗證指標：\n",
    "- **Spatial correlation**：預測場與真實場的空間相關\n",
    "- **RMSE by region**：不同區域的誤差\n",
    "- **Fractions Skill Score**：考慮空間鄰域的驗證\n",
    "\n",
    "### 為什麼 xskillscore 適合這個 workflow？\n",
    "\n",
    "xskillscore 直接接受 **Xarray DataArray**：\n",
    "- 保留經緯度座標\n",
    "- 可以做區域加權（area-weighted metrics）\n",
    "- 可以沿著不同維度計算（時間、空間、ensemble）\n",
    "\n",
    "這就是為什麼我們用 Xarray 而不是 pandas/NumPy 的原因之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 將預測轉回 Xarray\n",
    "\n",
    "我們需要把 PyTorch Tensor 轉回 Xarray，恢復座標資訊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：這裡是簡化版，實務上需要正確對應每個 patch 的座標\n",
    "# 為了示範，我們假設 predictions 和 test_ds 的空間範圍相同\n",
    "\n",
    "# 取得一個 batch 的座標\n",
    "sample_batch = next(iter(X_test_bgen))\n",
    "time_coords = sample_batch['time'].values\n",
    "lat_coords = sample_batch['latitude'].values\n",
    "lon_coords = sample_batch['longitude'].values\n",
    "\n",
    "# 創建 Xarray DataArray\n",
    "pred_da = xr.DataArray(\n",
    "    predictions[:len(time_coords)].numpy(),  # 限制到實際的時間長度\n",
    "    dims=['time', 'latitude', 'longitude'],\n",
    "    coords={\n",
    "        'time': time_coords,\n",
    "        'latitude': lat_coords,\n",
    "        'longitude': lon_coords\n",
    "    },\n",
    "    name='convection_probability'\n",
    ")\n",
    "\n",
    "target_da = xr.DataArray(\n",
    "    targets[:len(time_coords)].numpy(),\n",
    "    dims=['time', 'latitude', 'longitude'],\n",
    "    coords={\n",
    "        'time': time_coords,\n",
    "        'latitude': lat_coords,\n",
    "        'longitude': lon_coords\n",
    "    },\n",
    "    name='convection_truth'\n",
    ")\n",
    "\n",
    "print(\"Predictions as Xarray:\")\n",
    "print(pred_da)\n",
    "print()\n",
    "print(\"Targets as Xarray:\")\n",
    "print(target_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 計算空間相關係數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "\n",
    "# 計算每個時間步的空間相關\n",
    "spatial_corr = xs.pearson_r(pred_da, target_da, dim=['latitude', 'longitude'])\n",
    "\n",
    "print(\"Spatial correlation (per time step):\")\n",
    "print(spatial_corr.values)\n",
    "print()\n",
    "print(f\"Mean spatial correlation: {spatial_corr.mean().values:.4f}\")\n",
    "print(f\"Std: {spatial_corr.std().values:.4f}\")\n",
    "\n",
    "# 繪圖\n",
    "plt.figure(figsize=(10, 4))\n",
    "spatial_corr.plot(marker='o')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Spatial Correlation over Time', fontsize=13)\n",
    "plt.ylabel('Pearson r', fontsize=12)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 計算空間 RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 RMSE\n",
    "rmse = xs.rmse(pred_da, target_da, dim=['time', 'latitude', 'longitude'])\n",
    "\n",
    "print(f\"Overall RMSE: {rmse.values:.4f}\")\n",
    "\n",
    "# 也可以計算每個格點的時間 RMSE\n",
    "rmse_spatial = xs.rmse(pred_da, target_da, dim='time')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "rmse_spatial.plot(cmap='YlOrRd', vmin=0)\n",
    "plt.title('RMSE by Location (averaged over time)', fontsize=13)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"紅色區域：模型預測誤差較大\")\n",
    "print(\"黃色/綠色：預測較準確\")\n",
    "print(\"可以幫助識別模型在哪些地理位置表現較差\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 完整 Workflow 回顧\n",
    "\n",
    "讓我們回顧整個從「大型 N-D array」到「ML 模型」的流程：\n",
    "\n",
    "```\n",
    "1. 資料儲存\n",
    "   Zarr (on-disk, chunked)\n",
    "   ↓\n",
    "   \n",
    "2. 資料讀取\n",
    "   intake-xarray + Dask\n",
    "   ↓ (lazy)\n",
    "   \n",
    "3. 前處理\n",
    "   - Resample (hourly → daily)\n",
    "   - 建立 labels\n",
    "   - 時間序列分割\n",
    "   ↓ (still lazy)\n",
    "   \n",
    "4. Batch 生成\n",
    "   xbatcher.BatchGenerator\n",
    "   ↓\n",
    "   \n",
    "5. PyTorch 整合\n",
    "   xbatcher.loaders.torch.MapDataset\n",
    "   ↓\n",
    "   \n",
    "6. 資料載入\n",
    "   DataLoader (batch_size=None)\n",
    "   ↓ (now eager, on-demand)\n",
    "   \n",
    "7. 模型訓練\n",
    "   PyTorch training loop\n",
    "   ↓\n",
    "   \n",
    "8. 預測與評估\n",
    "   - 傳統 metrics (accuracy, F1)\n",
    "   - 空間 metrics (xskillscore)\n",
    "   ↓\n",
    "   \n",
    "9. 結果視覺化\n",
    "   Xarray + matplotlib\n",
    "```\n",
    "\n",
    "### 關鍵設計原則\n",
    "\n",
    "1. **Lazy as long as possible**\n",
    "   - 直到 DataLoader 迭代時才實際讀取資料\n",
    "   - 減少記憶體佔用\n",
    "\n",
    "2. **保留元資料**\n",
    "   - 使用 Xarray 而不是 NumPy\n",
    "   - 座標資訊對驗證和視覺化很重要\n",
    "\n",
    "3. **批次處理**\n",
    "   - xbatcher 自動處理時空切分\n",
    "   - 不需要手動管理索引\n",
    "\n",
    "4. **平行化**\n",
    "   - Dask 處理資料讀取的平行化\n",
    "   - DataLoader 的 num_workers 處理前處理平行化\n",
    "   - GPU 處理模型訓練平行化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 常見問題與除錯\n",
    "\n",
    "### Q1: DataLoader 報錯 \"batch_size should be None\"\n",
    "\n",
    "**原因**：xbatcher 已經定義了 batch，不應該再用 DataLoader 的 batch_size。\n",
    "\n",
    "**解法**：\n",
    "```python\n",
    "DataLoader(dataset, batch_size=None, ...)  # 正確\n",
    "DataLoader(dataset, batch_size=4, ...)     # 錯誤！\n",
    "```\n",
    "\n",
    "### Q2: Multiprocessing 報錯 \"cannot pickle Client\"\n",
    "\n",
    "**原因**：Dask Client 無法被 pickle，但 DataLoader 的 multiprocessing 需要 pickle。\n",
    "\n",
    "**解法**：\n",
    "```python\n",
    "DataLoader(..., multiprocessing_context='forkserver')  # 使用 forkserver\n",
    "# 或\n",
    "DataLoader(..., num_workers=0)  # 不使用 multiprocessing\n",
    "```\n",
    "\n",
    "### Q3: 記憶體不足（OOM）\n",
    "\n",
    "**原因**：\n",
    "- Batch 太大（時間或空間維度）\n",
    "- preload_batch=True\n",
    "- num_workers 太多\n",
    "\n",
    "**解法**：\n",
    "1. 減小 batch_dims 或 input_dims\n",
    "2. 確保 preload_batch=False\n",
    "3. 減少 num_workers\n",
    "4. 調整 Dask Client 的 memory_limit\n",
    "\n",
    "### Q4: 訓練很慢\n",
    "\n",
    "**可能原因**：\n",
    "- I/O 瓶頸：增加 num_workers\n",
    "- Chunk 太小：考慮 rechunk\n",
    "- CPU 計算：檢查是否正確使用 GPU\n",
    "\n",
    "**除錯**：觀察 Dask Dashboard，看時間花在哪裡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 延伸方向\n",
    "\n",
    "這個 workshop 展示了基礎流程，實務上可以延伸：\n",
    "\n",
    "### 資料面\n",
    "- 加入更多變數（濕度、風場、溫度剖面）\n",
    "- 多資料來源融合（ERA5 + 衛星 + 地面觀測）\n",
    "- 時間滯後特徵（t-1, t-2 小時的資料）\n",
    "\n",
    "### 模型面\n",
    "- 更複雜的架構（UNet, ResNet, Transformer）\n",
    "- 序列模型（LSTM, GRU）用於時間序列\n",
    "- Ensemble 模型\n",
    "\n",
    "### 訓練面\n",
    "- Class imbalance 處理（weighted loss, focal loss）\n",
    "- Data augmentation（spatial flip, rotation）\n",
    "- Transfer learning（預訓練模型）\n",
    "\n",
    "### 驗證面\n",
    "- 更多空間指標（Fractions Skill Score, SAL）\n",
    "- Case study（分析特定事件）\n",
    "- 區域化評估（山區 vs 平地）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 總結\n",
    "\n",
    "完成這個 notebook 後，你應該能夠：\n",
    "\n",
    "- [ ] 定義適合 ML 的氣象任務\n",
    "- [ ] 建立有意義的 labels\n",
    "- [ ] 使用 xbatcher 產生時空 batches\n",
    "- [ ] 正確整合 xbatcher 與 PyTorch\n",
    "- [ ] 設定 DataLoader（batch_size=None!）\n",
    "- [ ] 訓練一個簡單的 CNN 模型\n",
    "- [ ] 使用 xskillscore 進行空間驗證\n",
    "- [ ] 理解完整的 out-of-core ML workflow\n",
    "\n",
    "### 核心要點\n",
    "\n",
    "1. **xbatcher 的兩階段設計是關鍵**\n",
    "   - BatchGenerator → MapDataset\n",
    "   - 不要自己寫 Dataset wrapper\n",
    "\n",
    "2. **batch_size=None 避免維度錯誤**\n",
    "   - xbatcher 已經定義 batch\n",
    "   - DataLoader 只負責 shuffling 和 multiprocessing\n",
    "\n",
    "3. **保留空間資訊很重要**\n",
    "   - 轉回 Xarray 做驗證\n",
    "   - 空間相關性是氣象預報的核心\n",
    "\n",
    "4. **Lazy evaluation 貫穿整個流程**\n",
    "   - 從 Zarr 讀取到 DataLoader 都是 lazy\n",
    "   - 只在需要時才載入資料\n",
    "\n",
    "這個工作流程可以擴展到：\n",
    "- 更大的資料集（TB 級別）\n",
    "- 更複雜的模型（深度學習）\n",
    "- 分散式訓練（多 GPU、多節點）\n",
    "\n",
    "重點是**理解原理**，而不是記住 API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關閉 Dask Client\n",
    "# client.close()\n",
    "\n",
    "print(\"Workshop completed! 🎉\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
