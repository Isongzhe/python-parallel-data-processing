{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: \u6642\u7a7a\u8655\u7406\u8207\u5132\u5b58\u512a\u5316 (30 min)\n",
    "\n",
    "\u9019\u500b notebook \u6703\u4ecb\u7d39\uff1a\n",
    "\n",
    "1. **\u6642\u9593 Resampling**\uff1ahourly \u2192 daily\uff0c\u8a08\u7b97\u7d71\u8a08\u91cf\n",
    "2. **\u591a\u6a94\u6848\u8b80\u53d6**\uff1a\u4f7f\u7528 intake catalog \u4e32\u63a5\u591a\u5e74\u8cc7\u6599\n",
    "3. **\u7a7a\u9593\u7d71\u8a08**\uff1a\u5340\u57df\u5e73\u5747\u3001gradient \u8a08\u7b97\n",
    "4. **Rechunking \u7b56\u7565**\uff1a\u6839\u64da\u5206\u6790\u6a21\u5f0f\u8abf\u6574 chunk\n",
    "5. **\u5132\u5b58\u70ba Zarr**\uff1a\u512a\u5316 metadata \u548c\u58d3\u7e2e\u8a2d\u5b9a\n",
    "\n",
    "---\n",
    "\n",
    "## \u5b78\u7fd2\u76ee\u6a19\n",
    "\n",
    "- \u638c\u63e1 `.resample()` \u548c `.groupby()` \u7684\u6642\u9593\u805a\u5408\n",
    "- \u7406\u89e3\u591a\u6a94\u6848\u8b80\u53d6\u7684\u8a18\u61b6\u9ad4\u7ba1\u7406\n",
    "- \u5b78\u6703\u9078\u64c7\u5408\u9069\u7684 chunking \u7b56\u7565\n",
    "- \u6b63\u78ba\u5132\u5b58\u4e2d\u9593\u7d50\u679c\uff0c\u907f\u514d\u91cd\u8907\u8a08\u7b97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. \u74b0\u5883\u6e96\u5099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "import xarray as xr\n",
    "import intake\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# \u555f\u52d5 Dask Client\n",
    "client = Client(processes=False, n_workers=2, threads_per_worker=2, memory_limit='2GB')\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "# \u8f09\u5165 catalog\n",
    "catalog = intake.open_catalog('catalog.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \u6642\u9593 Resampling\uff1aHourly \u2192 Daily\n",
    "\n",
    "### \u70ba\u4ec0\u9ebc\u9700\u8981 Resampling\uff1f\n",
    "\n",
    "\u539f\u59cb ERA5 \u8cc7\u6599\u662f hourly\uff0c\u4f46\u8a31\u591a\u5206\u6790\u9700\u8981\uff1a\n",
    "- **Daily mean**\uff1a\u65e5\u5e73\u5747\u6eab\u5ea6\u3001\u6fd5\u5ea6\n",
    "- **Daily max**\uff1a\u65e5\u6700\u5927 CAPE\uff08\u5c0d\u6d41\u6f5b\u52e2\uff09\n",
    "- **Daily accumulation**\uff1a\u65e5\u7d2f\u7a4d\u964d\u96e8\n",
    "\n",
    "Resampling \u7684\u512a\u52e2\uff1a\n",
    "1. **\u6e1b\u5c11\u8cc7\u6599\u91cf**\uff1a8760 hours \u2192 365 days (\u6e1b\u5c11 24 \u500d)\n",
    "2. **\u964d\u4f4e\u96dc\u8a0a**\uff1a\u5e73\u6ed1\u77ed\u671f\u6ce2\u52d5\n",
    "3. **\u7b26\u5408\u5206\u6790\u9700\u6c42**\uff1a\u8a31\u591a\u6c23\u5019\u6307\u6a19\u662f daily \u5c3a\u5ea6\n",
    "\n",
    "### Xarray \u7684 `.resample()` \u65b9\u6cd5\n",
    "\n",
    "`.resample()` \u985e\u4f3c pandas\uff0c\u4f46\u662f **lazy** \u7684\uff1a\n",
    "```python\n",
    "ds.resample(time='1D').mean()  # \u4e0d\u6703\u7acb\u5373\u57f7\u884c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u8f09\u5165 2019 \u5e74\u8cc7\u6599\n",
    "ds = catalog.era5_2019.to_dask()\n",
    "\n",
    "print(\"\u539f\u59cb\u8cc7\u6599\uff1a\")\n",
    "print(f\"  Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "print(f\"  Time steps: {len(ds.time)}\")\n",
    "print(f\"  Frequency: hourly\")\n",
    "print(f\"  Size: {ds.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 \u8a08\u7b97 Daily Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily mean: \u6bcf\u5929 24 \u5c0f\u6642\u7684\u5e73\u5747\n",
    "# 'D' = calendar day, '1D' = 1 day\n",
    "ds_daily_mean = ds.resample(time='1D').mean()\n",
    "\n",
    "print(\"Daily mean:\")\n",
    "print(f\"  Time steps: {len(ds_daily_mean.time)}\")\n",
    "print(f\"  Size: {ds_daily_mean.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"  Reduction: {ds.nbytes / ds_daily_mean.nbytes:.1f}x\")\n",
    "print()\n",
    "print(ds_daily_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 \u4e0d\u540c\u8b8a\u6578\u4f7f\u7528\u4e0d\u540c\u7d71\u8a08\u91cf\n",
    "\n",
    "\u5c0d\u65bc\u5c0d\u6d41\u9810\u5831\uff0c\u6211\u5011\u53ef\u80fd\u9700\u8981\uff1a\n",
    "- **CAPE**: daily maximum\uff08\u6700\u5927\u4e0d\u7a69\u5b9a\u5ea6\uff09\n",
    "- **CIN**: daily mean\uff08\u5e73\u5747\u6291\u5236\uff09\n",
    "- **K-index**: daily mean\n",
    "- **BLH**: daily maximum\uff08\u6700\u9ad8\u908a\u754c\u5c64\uff09\n",
    "\n",
    "Xarray \u5141\u8a31\u5c0d\u4e0d\u540c\u8b8a\u6578\u505a\u4e0d\u540c\u64cd\u4f5c\uff1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u5206\u5225\u8a08\u7b97\n",
    "cape_daily_max = ds['cape'].resample(time='1D').max()\n",
    "cin_daily_mean = ds['cin'].resample(time='1D').mean()\n",
    "kindex_daily_mean = ds['k_index'].resample(time='1D').mean()\n",
    "blh_daily_max = ds['blh'].resample(time='1D').max()\n",
    "\n",
    "# \u5408\u4f75\u6210\u65b0\u7684 Dataset\n",
    "ds_daily = xr.Dataset({\n",
    "    'cape_max': cape_daily_max,\n",
    "    'cin_mean': cin_daily_mean,\n",
    "    'kindex_mean': kindex_daily_mean,\n",
    "    'blh_max': blh_daily_max\n",
    "})\n",
    "\n",
    "print(\"Custom daily statistics:\")\n",
    "print(ds_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 \u57f7\u884c\u4e26\u8996\u89ba\u5316\u7d50\u679c\n",
    "\n",
    "\u6211\u5011\u9078\u53d6\u53f0\u7063\u9644\u8fd1\u5340\u57df\uff0c\u8a08\u7b97\u4e26\u7e6a\u5716\u3002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u9078\u53d6\u53f0\u7063\u5340\u57df\u7684 7 \u6708\u8cc7\u6599\n",
    "ds_taiwan_july = ds_daily.sel(\n",
    "    time='2019-07',\n",
    "    latitude=slice(22, 26),\n",
    "    longitude=slice(120, 122)\n",
    ")\n",
    "\n",
    "# \u8a08\u7b97\u6708\u5e73\u5747\n",
    "cape_monthly = ds_taiwan_july['cape_max'].mean(dim='time').compute()\n",
    "\n",
    "# \u7e6a\u5716\n",
    "plt.figure(figsize=(8, 6))\n",
    "cape_monthly.plot(cmap='YlOrRd', vmin=0, vmax=2000)\n",
    "plt.title('Mean of Daily Max CAPE (July 2019, Taiwan)', fontsize=12)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \u591a\u6a94\u6848\u8b80\u53d6\uff1a\u8de8\u5e74\u5ea6\u5206\u6790\n",
    "\n",
    "### \u60c5\u5883\uff1a\u8a08\u7b97 2019-2023 \u5e74\u7684\u6c23\u5019\u614b\n",
    "\n",
    "\u6c23\u5019\u614b\uff08climatology\uff09= \u591a\u5e74\u5e73\u5747\u7684\u5b63\u7bc0\u8b8a\u5316\u3002\u4f8b\u5982\uff1a\n",
    "- 1 \u6708\u7684\u5e73\u5747 CAPE\n",
    "- 7 \u6708\u7684\u5e73\u5747 BLH\n",
    "\n",
    "\u9019\u9700\u8981\u8b80\u53d6\u6240\u6709\u5e74\u4efd\u7684\u8cc7\u6599\u3002\n",
    "\n",
    "### \u65b9\u6cd5 1\uff1a\u9010\u5e74\u8b80\u53d6\u4e26\u4e32\u63a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u8b80\u53d6\u6240\u6709\u5e74\u4efd\n",
    "years = [2019, 2020]  # Demo: \u53ea\u7528\u5169\u5e74\u8cc7\u6599\uff0c\u6e1b\u5c11\u8a08\u7b97\u91cf\n",
    "datasets = []\n",
    "\n",
    "for year in years:\n",
    "    ds_year = catalog[f'era5_{year}'].to_dask()\n",
    "    datasets.append(ds_year)\n",
    "    print(f\"Loaded {year}: {len(ds_year.time)} time steps\")\n",
    "\n",
    "# \u6cbf\u8457\u6642\u9593\u8ef8\u4e32\u63a5\n",
    "ds_all = xr.concat(datasets, dim='time')\n",
    "\n",
    "print(f\"\\nCombined dataset:\")\n",
    "print(f\"  Time range: {ds_all.time.values[0]} to {ds_all.time.values[-1]}\")\n",
    "print(f\"  Total time steps: {len(ds_all.time)}\")\n",
    "print(f\"  Total size: {ds_all.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u91cd\u8981\uff1a\u9019\u88e1\u6c92\u6709\u8f09\u5165\u8cc7\u6599\uff01\n",
    "\n",
    "\u96d6\u7136\u6211\u5011\u300c\u4e32\u63a5\u300d\u4e86 5 \u5e74\u8cc7\u6599\uff08~135 GB\uff09\uff0c\u4f46\u5be6\u969b\u4e0a\uff1a\n",
    "- **\u6c92\u6709\u8b80\u53d6\u4efb\u4f55 Zarr \u6a94\u6848**\n",
    "- **\u6c92\u6709\u4f54\u7528 135 GB \u8a18\u61b6\u9ad4**\n",
    "- \u53ea\u662f\u5efa\u7acb\u4e86\u4e00\u500b**\u865b\u64ec\u7684 Dataset**\uff0c\u8a18\u9304\u4e86\u8cc7\u6599\u7684\u4f4d\u7f6e\n",
    "\n",
    "\u9019\u5c31\u662f lazy evaluation \u7684\u5a01\u529b\u3002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \u8a08\u7b97\u6c23\u5019\u614b\uff1aGroupby \u64cd\u4f5c\n",
    "\n",
    "### \u4ec0\u9ebc\u662f Groupby\uff1f\n",
    "\n",
    "Groupby \u662f\u300csplit-apply-combine\u300d\u6a21\u5f0f\uff1a\n",
    "1. **Split**\uff1a\u6309\u7167\u67d0\u500b\u898f\u5247\u5206\u7d44\uff08\u4f8b\u5982\u6309\u6708\u4efd\uff09\n",
    "2. **Apply**\uff1a\u5c0d\u6bcf\u7d44\u505a\u76f8\u540c\u64cd\u4f5c\uff08\u4f8b\u5982\u8a08\u7b97\u5e73\u5747\uff09\n",
    "3. **Combine**\uff1a\u5408\u4f75\u7d50\u679c\n",
    "\n",
    "\u5728\u6c23\u5019\u5206\u6790\u4e2d\u5e38\u7528\u7684 groupby\uff1a\n",
    "- `.groupby('time.month')`\uff1a\u6309\u6708\u4efd\u5206\u7d44\uff081-12\u6708\uff09\n",
    "- `.groupby('time.season')`\uff1a\u6309\u5b63\u7bc0\u5206\u7d44\uff08DJF, MAM, JJA, SON\uff09\n",
    "- `.groupby('time.dayofyear')`\uff1a\u6309\u4e00\u5e74\u4e2d\u7684\u7b2c\u5e7e\u5929\uff081-365\uff09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u8a08\u7b97\u6bcf\u500b\u6708\u4efd\u7684\u6c23\u5019\u614b\uff085 \u5e74\u5e73\u5747\uff09\n",
    "cape_climatology = ds_all['cape'].groupby('time.month').mean(dim='time')\n",
    "\n",
    "print(\"CAPE Climatology:\")\n",
    "print(cape_climatology)\n",
    "print()\n",
    "print(f\"Dimensions: {cape_climatology.dims}\")\n",
    "print(f\"Shape: {cape_climatology.shape}\")\n",
    "print(f\"Coordinates: {list(cape_climatology.coords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u7406\u89e3\u7d50\u679c\n",
    "\n",
    "\u539f\u672c\u7684\u7dad\u5ea6\uff1a`(time, latitude, longitude)`\n",
    "- time: 43800 (5 years * 8760 hours)\n",
    "\n",
    "Groupby \u5f8c\u7684\u7dad\u5ea6\uff1a`(month, latitude, longitude)`\n",
    "- month: 12\n",
    "\n",
    "\u6bcf\u500b\u6708\u4efd\u7684\u503c = 5 \u5e74\u4e2d\u6240\u6709\u8a72\u6708\u4efd\u8cc7\u6599\u7684\u5e73\u5747\u3002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 \u8a08\u7b97\u4e26\u7e6a\u88fd\u5b63\u7bc0\u8b8a\u5316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u9078\u53d6\u53f0\u7063\u5340\u57df\uff0c\u8a08\u7b97\u6c23\u5019\u614b\n",
    "cape_taiwan = ds_all['cape'].sel(\n",
    "    latitude=slice(22, 26),\n",
    "    longitude=slice(120, 122)\n",
    ")\n",
    "\n",
    "# \u8a08\u7b97\u5340\u57df\u5e73\u5747\u7684\u6708\u6c23\u5019\u614b\n",
    "cape_monthly_clim = cape_taiwan.groupby('time.month').mean(dim=['time', 'latitude', 'longitude'])\n",
    "\n",
    "# \u57f7\u884c\u8a08\u7b97\n",
    "result = cape_monthly_clim.compute()\n",
    "\n",
    "# \u7e6a\u5716\n",
    "plt.figure(figsize=(10, 5))\n",
    "result.plot(marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('CAPE (J/kg)', fontsize=12)\n",
    "plt.title('Taiwan Area-Averaged CAPE Climatology (2019-2023)', fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  Max CAPE month: {result.values.argmax() + 1}\")\n",
    "print(f\"  Min CAPE month: {result.values.argmin() + 1}\")\n",
    "print(f\"  Summer mean (JJA): {result.sel(month=[6,7,8]).mean().values:.0f} J/kg\")\n",
    "print(f\"  Winter mean (DJF): {result.sel(month=[12,1,2]).mean().values:.0f} J/kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \u7a7a\u9593\u7d71\u8a08\uff1a\u8a08\u7b97 Gradient\n",
    "\n",
    "\u5728\u6c23\u8c61\u5206\u6790\u4e2d\uff0c\u8b8a\u6578\u7684\u7a7a\u9593\u68af\u5ea6\uff08gradient\uff09\u5f88\u91cd\u8981\uff1a\n",
    "- **\u6eab\u5ea6\u68af\u5ea6**\uff1a\u92d2\u9762\u4f4d\u7f6e\n",
    "- **CAPE \u68af\u5ea6**\uff1a\u5c0d\u6d41\u908a\u754c\n",
    "\n",
    "Xarray \u63d0\u4f9b `.differentiate()` \u4f86\u8a08\u7b97\u5c0e\u6578\u3002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u9078\u53d6\u4e00\u500b\u6642\u9593\u9ede\n",
    "cape_snapshot = ds_all['cape'].isel(time=0)\n",
    "\n",
    "# \u8a08\u7b97\u7d93\u5411\uff08latitude\uff09\u68af\u5ea6\n",
    "# \u55ae\u4f4d\uff1aCAPE per degree latitude\n",
    "cape_grad_lat = cape_snapshot.differentiate('latitude')\n",
    "\n",
    "# \u8a08\u7b97\u7def\u5411\uff08longitude\uff09\u68af\u5ea6\n",
    "cape_grad_lon = cape_snapshot.differentiate('longitude')\n",
    "\n",
    "# \u8a08\u7b97\u68af\u5ea6\u5f37\u5ea6\uff08magnitude\uff09\n",
    "cape_grad_mag = np.sqrt(cape_grad_lat**2 + cape_grad_lon**2)\n",
    "\n",
    "print(\"Gradient calculation (still lazy):\")\n",
    "print(f\"  Type: {type(cape_grad_mag.data)}\")\n",
    "print(f\"  Shape: {cape_grad_mag.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rechunking\uff1a\u6700\u4f73\u5316 Chunk \u7b56\u7565\n",
    "\n",
    "### \u70ba\u4ec0\u9ebc\u9700\u8981 Rechunk\uff1f\n",
    "\n",
    "\u539f\u59cb Zarr \u7684 chunking \u662f `(744, 121, 161)`\uff08\u6642\u9593\u7d04 1 \u500b\u6708\uff09\u3002\n",
    "\n",
    "\u9019\u9069\u5408\uff1a\n",
    "- \u2705 \u8a08\u7b97\u6642\u9593\u5e73\u5747\u3001\u6642\u9593\u5e8f\u5217\u5206\u6790\n",
    "- \u2705 \u7a7a\u9593\u5834\u7684\u64cd\u4f5c\n",
    "\n",
    "\u4f46**\u4e0d\u9069\u5408**\uff1a\n",
    "- \u274c \u55ae\u9ede\u6216\u5c0f\u5340\u57df\u7684\u9577\u6642\u9593\u5e8f\u5217\n",
    "- \u274c ML training\uff08\u9700\u8981\u6642\u9593\u7dad\u5ea6\u5207\u5c0f\u584a\uff09\n",
    "\n",
    "### Rechunking \u7684\u6210\u672c\n",
    "\n",
    "Rechunking **\u4e0d\u662f\u514d\u8cbb\u7684**\uff1a\n",
    "- \u9700\u8981\u8b80\u53d6\u6240\u6709\u539f\u59cb\u8cc7\u6599\n",
    "- \u91cd\u65b0\u7d44\u7e54\u4e26\u5beb\u5165\u65b0\u7684 chunks\n",
    "- I/O intensive\uff08\u53ef\u80fd\u9700\u8981\u6578\u5c0f\u6642\uff09\n",
    "\n",
    "\u56e0\u6b64\uff1a\n",
    "1. **\u5148\u78ba\u5b9a\u5206\u6790\u6a21\u5f0f**\u518d rechunk\n",
    "2. Rechunk \u7684\u7d50\u679c\u61c9\u8a72**\u5132\u5b58\u4e0b\u4f86**\uff0c\u4e0d\u8981\u6bcf\u6b21\u90fd\u91cd\u65b0\u7b97\n",
    "3. \u4f7f\u7528 intermediate Zarr \u4f5c\u70ba\u4e2d\u9593\u7522\u7269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 \u60c5\u5883\uff1a\u70ba ML Training \u6700\u4f73\u5316\n",
    "\n",
    "\u5c0d\u65bc\u5f8c\u7e8c\u7684 ML pipeline\uff0c\u6211\u5011\u9700\u8981\uff1a\n",
    "- \u5c0f\u7684\u6642\u9593 batch\uff08\u4f8b\u5982 32 time steps\uff09\n",
    "- \u5b8c\u6574\u7684\u7a7a\u9593 patch\uff08\u4f8b\u5982 16x16\uff09\n",
    "\n",
    "\u7406\u60f3\u7684 chunking\uff1a`(32, 16, 16)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u4f7f\u7528 daily mean \u8cc7\u6599\uff08\u5df2\u7d93\u5f9e hourly \u964d\u5230 daily\uff09\n",
    "ds_daily_all = ds_all.resample(time='1D').mean()\n",
    "\n",
    "print(\"Before rechunk:\")\n",
    "print(f\"  Original chunks: {ds_daily_all['cape'].chunks}\")\n",
    "print(f\"  Total size: {ds_daily_all.nbytes / 1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Rechunk \u70ba\u9069\u5408 ML \u7684\u7d50\u69cb\n",
    "ds_rechunked = ds_daily_all.chunk({\n",
    "    'time': 32,        # \u6bcf\u500b batch 32 \u5929\n",
    "    'latitude': 16,    # 16x16 \u7a7a\u9593 patch\n",
    "    'longitude': 16\n",
    "})\n",
    "\n",
    "print(\"After rechunk:\")\n",
    "print(f\"  New chunks: {ds_rechunked['cape'].chunks}\")\n",
    "print(f\"  Number of chunks: {ds_rechunked['cape'].data.npartitions}\")\n",
    "print()\n",
    "\n",
    "# \u8a08\u7b97\u55ae\u4e00 chunk \u7684\u5927\u5c0f\n",
    "chunk_size = 32 * 16 * 16 * 4  # float32 = 4 bytes\n",
    "print(f\"Single chunk size: {chunk_size / 1024:.2f} KB\")\n",
    "print(f\"  Status: {chunk_size / (1024**2):.2f} MB - \u5728\u7406\u60f3\u7bc4\u570d\u5167 (10-100 MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Size \u7684\u6b0a\u8861\n",
    "\n",
    "| Chunk Size | \u512a\u9ede | \u7f3a\u9ede | \u9069\u7528\u60c5\u5883 |\n",
    "|------------|------|------|----------|\n",
    "| < 1 MB | \u6975\u7d30\u7c92\u5ea6\u5e73\u884c\u5316 | Overhead \u904e\u5927 | \u4e0d\u63a8\u85a6 |\n",
    "| 1-10 MB | \u9ad8\u5ea6\u5e73\u884c\u5316 | \u6392\u7a0b\u6210\u672c\u4ecd\u9ad8 | \u5927\u91cf workers |\n",
    "| **10-100 MB** | **\u5e73\u8861** | **- ** | **\u5927\u591a\u6578\u60c5\u6cc1** |\n",
    "| 100-500 MB | \u4f4e overhead | \u5e73\u884c\u5ea6\u53d7\u9650 | \u5c11\u91cf\u8907\u96dc\u8a08\u7b97 |\n",
    "| > 500 MB | \u6700\u4f4e overhead | \u8a18\u61b6\u9ad4\u58d3\u529b\u3001\u7121\u6cd5\u5e73\u884c | \u4e0d\u63a8\u85a6 |\n",
    "\n",
    "\u6211\u5011\u7684 32 KB \u592a\u5c0f\u4e86\uff01\u8b93\u6211\u5011\u8abf\u6574\uff1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u66f4\u5408\u7406\u7684 chunking\n",
    "ds_rechunked = ds_daily_all.chunk({\n",
    "    'time': 32,        # 32 \u5929\n",
    "    'latitude': 60,    # 60 \u500b\u7def\u5ea6\u9ede\n",
    "    'longitude': 80    # 80 \u500b\u7d93\u5ea6\u9ede\n",
    "})\n",
    "\n",
    "chunk_size = 32 * 60 * 80 * 4 * 4  # 4 variables, float32\n",
    "print(f\"Optimized chunk size: {chunk_size / (1024**2):.2f} MB\")\n",
    "print(f\"Number of chunks per variable: {ds_rechunked['cape'].data.npartitions}\")\n",
    "print(f\"Total chunks (4 variables): {ds_rechunked['cape'].data.npartitions * 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \u5132\u5b58\u70ba Zarr\uff1a\u56fa\u5316\u4e2d\u9593\u7d50\u679c\n",
    "\n",
    "### \u70ba\u4ec0\u9ebc\u8981\u5132\u5b58\u4e2d\u9593\u7d50\u679c\uff1f\n",
    "\n",
    "\u5047\u8a2d\u6211\u5011\u7684 workflow \u662f\uff1a\n",
    "1. \u8b80\u53d6 5 \u5e74 hourly \u8cc7\u6599\n",
    "2. Resample \u5230 daily\n",
    "3. Rechunk\n",
    "4. \u7528\u65bc ML training\n",
    "\n",
    "\u5982\u679c\u6bcf\u6b21 training \u90fd\u91cd\u8907 1-3 \u6b65\u9a5f\uff1a\n",
    "- \u6bcf\u6b21\u90fd\u8981\u8b80\u53d6 ~135 GB \u539f\u59cb\u8cc7\u6599\n",
    "- \u91cd\u8907\u8a08\u7b97 resample\uff08\u8017\u8cbb CPU\uff09\n",
    "- \u91cd\u8907 rechunk\uff08\u5927\u91cf I/O\uff09\n",
    "\n",
    "**\u66f4\u597d\u7684\u505a\u6cd5**\uff1a\n",
    "1. \u7b97\u4e00\u6b21 daily + rechunked \u8cc7\u6599\n",
    "2. \u5132\u5b58\u70ba\u65b0\u7684 Zarr\uff08\u53ef\u80fd ~6 GB\uff09\n",
    "3. \u5f8c\u7e8c\u76f4\u63a5\u8b80\u53d6\u9019\u500b\u512a\u5316\u904e\u7684 Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 \u6e96\u5099\u8981\u5132\u5b58\u7684\u8cc7\u6599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u9078\u53d6 2019 \u5e74\uff0cresample \u5230 daily mean\n",
    "ds_to_save = ds.resample(time='1D').mean()\n",
    "\n",
    "# Rechunk \u5230\u9069\u5408 ML \u7684\u7d50\u69cb\n",
    "ds_to_save = ds_to_save.chunk({\n",
    "    'time': 32,\n",
    "    'latitude': 60,\n",
    "    'longitude': 80\n",
    "})\n",
    "\n",
    "print(\"Dataset to save:\")\n",
    "print(ds_to_save)\n",
    "print()\n",
    "print(f\"Original size (hourly): {ds.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"New size (daily): {ds_to_save.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Reduction: {ds.nbytes / ds_to_save.nbytes:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 \u5132\u5b58\u70ba Zarr\n",
    "\n",
    "Zarr \u7684\u5132\u5b58\u53c3\u6578\uff1a\n",
    "\n",
    "1. **mode='w'**: \u8986\u5beb\u6a21\u5f0f\uff08\u5c0f\u5fc3\uff01\u6703\u522a\u9664\u820a\u8cc7\u6599\uff09\n",
    "2. **consolidated=True**: \u5c07 metadata \u96c6\u4e2d\u5132\u5b58\n",
    "   - \u512a\u9ede\uff1a\u6e1b\u5c11\u8b80\u53d6\u6642\u7684 HTTP requests\uff08\u96f2\u7aef\u53cb\u5584\uff09\n",
    "   - \u7f3a\u9ede\uff1a\u4e0d\u652f\u63f4 append\uff08\u5c0d\u6211\u5011\u6c92\u5f71\u97ff\uff09\n",
    "3. **encoding**: \u8a2d\u5b9a\u58d3\u7e2e\u3001chunking\n",
    "   - compressor: \u4f7f\u7528 Blosc \u58d3\u7e2e\uff08CPU-friendly\uff09\n",
    "   - chunks: \u660e\u78ba\u6307\u5b9a chunk shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Blosc\n",
    "\n",
    "# \u8f38\u51fa\u8def\u5f91\n",
    "output_path = 'outputs/era5_2019_daily_optimized.zarr'\n",
    "\n",
    "# \u8a2d\u5b9a encoding\uff08\u58d3\u7e2e\u8207 chunking\uff09\n",
    "compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n",
    "\n",
    "encoding = {\n",
    "    var: {\n",
    "        'compressor': compressor,\n",
    "        'chunks': (32, 60, 80)  # \u660e\u78ba\u6307\u5b9a chunk shape\n",
    "    }\n",
    "    for var in ds_to_save.data_vars\n",
    "}\n",
    "\n",
    "print(\"Saving to Zarr...\")\n",
    "print(f\"  Output: {output_path}\")\n",
    "print(f\"  Compressor: Blosc (zstd, level 3)\")\n",
    "print(f\"  Chunks: (32, 60, 80)\")\n",
    "print()\n",
    "\n",
    "# \u57f7\u884c\u5132\u5b58\n",
    "# compute=True: \u7acb\u5373\u57f7\u884c\uff08\u800c\u4e0d\u662f lazy\uff09\n",
    "# consolidated=True: \u5efa\u7acb .zmetadata\n",
    "ds_to_save.to_zarr(\n",
    "    output_path,\n",
    "    mode='w',\n",
    "    encoding=encoding,\n",
    "    consolidated=True\n",
    ")\n",
    "\n",
    "print(\"\u2713 Saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u5132\u5b58\u904e\u7a0b\u8aaa\u660e\n",
    "\n",
    "\u9019\u500b\u64cd\u4f5c\u6703\uff1a\n",
    "1. **\u8b80\u53d6\u539f\u59cb Zarr**\uff1a\u5f9e `/home/sungche/NAS/dataset/era5/`\n",
    "2. **\u57f7\u884c resample**\uff1ahourly \u2192 daily mean\n",
    "3. **Rechunk**\uff1a\u91cd\u7d44\u70ba\u65b0\u7684 chunk \u7d50\u69cb\n",
    "4. **\u58d3\u7e2e**\uff1a\u4f7f\u7528 Blosc \u58d3\u7e2e\uff08\u7d04 30-50% \u58d3\u7e2e\u7387\uff09\n",
    "5. **\u5beb\u5165\u65b0 Zarr**\uff1a\u5230 `outputs/` \u76ee\u9304\n",
    "6. **\u5efa\u7acb consolidated metadata**\uff1a\u52a0\u901f\u5f8c\u7e8c\u8b80\u53d6\n",
    "\n",
    "\u9019\u500b\u904e\u7a0b\u53ef\u80fd\u9700\u8981 **\u6578\u5206\u9418\u5230\u6578\u5341\u5206\u9418**\uff0c\u53d6\u6c7a\u65bc\u78c1\u789f\u901f\u5ea6\u548c worker \u6578\u91cf\u3002\n",
    "\n",
    "**\u89c0\u5bdf\u5efa\u8b70**\uff1a\u5207\u63db\u5230 Dask Dashboard \u7684 Task Stream\uff0c\u770b\u5230\u5927\u91cf\u5e73\u884c\u7684 read \u2192 compute \u2192 write \u4efb\u52d9\u3002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 \u9a57\u8b49\u5132\u5b58\u7d50\u679c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u91cd\u65b0\u8b80\u53d6\u525b\u5132\u5b58\u7684 Zarr\n",
    "ds_loaded = xr.open_zarr(output_path, consolidated=True)\n",
    "\n",
    "print(\"Loaded dataset:\")\n",
    "print(ds_loaded)\n",
    "print()\n",
    "print(f\"Chunks: {ds_loaded['cape'].chunks}\")\n",
    "print()\n",
    "\n",
    "# \u9a57\u8b49\u8cc7\u6599\u6b63\u78ba\u6027\uff1a\u8a08\u7b97\u4e00\u500b\u7c21\u55ae\u7d71\u8a08\u91cf\n",
    "original_mean = ds_to_save['cape'].mean().compute()\n",
    "loaded_mean = ds_loaded['cape'].mean().compute()\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Original mean: {original_mean.values:.2f}\")\n",
    "print(f\"  Loaded mean: {loaded_mean.values:.2f}\")\n",
    "print(f\"  Difference: {abs(original_mean.values - loaded_mean.values):.6f}\")\n",
    "print(f\"  Status: {'\u2713 Match' if abs(original_mean.values - loaded_mean.values) < 0.01 else '\u2717 Mismatch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 \u6aa2\u67e5\u78c1\u789f\u7a7a\u9593\u4f7f\u7528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# \u4f7f\u7528 du \u6307\u4ee4\u67e5\u770b\u76ee\u9304\u5927\u5c0f\n",
    "result = subprocess.run(\n",
    "    ['du', '-sh', output_path],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(f\"Disk usage: {result.stdout.strip()}\")\n",
    "print()\n",
    "print(\"Comparison:\")\n",
    "print(f\"  Original (hourly, uncompressed): ~27 GB\")\n",
    "print(f\"  Saved (daily, compressed): see above\")\n",
    "print(f\"  Expected: ~1.1 GB (24x smaller + compression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. \u58d3\u7e2e\u9078\u9805\u6bd4\u8f03\n",
    "\n",
    "\u4e0d\u540c\u7684\u58d3\u7e2e\u5668\u6709\u4e0d\u540c\u7684\u6b0a\u8861\uff1a\n",
    "\n",
    "| Compressor | \u58d3\u7e2e\u7387 | \u58d3\u7e2e\u901f\u5ea6 | \u89e3\u58d3\u901f\u5ea6 | \u9069\u7528\u60c5\u5883 |\n",
    "|------------|--------|----------|----------|----------|\n",
    "| None | 1.0x | - | - | \u5feb\u901f\u539f\u578b\u3001SSD |\n",
    "| Blosc (lz4) | 2-3x | \u6975\u5feb | \u6975\u5feb | \u9810\u8a2d\u9078\u64c7 |\n",
    "| **Blosc (zstd)** | **3-5x** | **\u5feb** | **\u5feb** | **\u63a8\u85a6** |\n",
    "| gzip | 4-6x | \u6162 | \u4e2d | \u9577\u671f\u5c01\u5b58\u3001\u7db2\u8def\u50b3\u8f38 |\n",
    "| zlib | 3-5x | \u6162 | \u4e2d | \u76f8\u5bb9\u6027\u512a\u5148 |\n",
    "\n",
    "**\u6211\u5011\u4f7f\u7528 Blosc (zstd, level=3)**\uff1a\n",
    "- \u5e73\u8861\u58d3\u7e2e\u7387\u8207\u901f\u5ea6\n",
    "- CPU overhead \u4f4e\uff08\u91cd\u8981\uff01\u56e0\u70ba\u6211\u5011\u9084\u8981\u505a\u8a08\u7b97\uff09\n",
    "- \u89e3\u58d3\u5feb\u901f\uff08\u8b80\u53d6\u6642\u4e0d\u6703\u6210\u70ba\u74f6\u9838\uff09\n",
    "\n",
    "\u5982\u679c\u9700\u8981\u66f4\u9ad8\u58d3\u7e2e\u7387\uff08\u4f8b\u5982\u8cc7\u6599\u8981\u50b3\u8f38\u5230\u5176\u4ed6\u5730\u65b9\uff09\uff0c\u53ef\u4ee5\u7528\uff1a\n",
    "```python\n",
    "compressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. \u6aa2\u67e5\u9ede\uff1a\u4f60\u61c9\u8a72\u7406\u89e3\u7684\u6982\u5ff5\n",
    "\n",
    "\u5b8c\u6210\u9019\u500b notebook \u5f8c\uff0c\u4f60\u61c9\u8a72\u80fd\u5920\uff1a\n",
    "\n",
    "- [ ] \u4f7f\u7528 `.resample()` \u9032\u884c\u6642\u9593\u805a\u5408\n",
    "- [ ] \u4f7f\u7528 `.groupby()` \u8a08\u7b97\u6c23\u5019\u614b\n",
    "- [ ] \u7528 `xr.concat()` \u4e32\u63a5\u591a\u500b Dataset\n",
    "- [ ] \u7406\u89e3 rechunking \u7684\u6642\u6a5f\u548c\u6210\u672c\n",
    "- [ ] \u6839\u64da\u5206\u6790\u6a21\u5f0f\u9078\u64c7\u5408\u9069\u7684 chunk size\n",
    "- [ ] \u6b63\u78ba\u8a2d\u5b9a\u58d3\u7e2e\u53c3\u6578\u4e26\u5132\u5b58 Zarr\n",
    "- [ ] \u4f7f\u7528 consolidated metadata \u52a0\u901f\u8b80\u53d6\n",
    "\n",
    "### Workflow \u7e3d\u7d50\n",
    "\n",
    "\u8655\u7406\u5927\u578b N-D array \u7684\u6a19\u6e96\u6d41\u7a0b\uff1a\n",
    "\n",
    "```\n",
    "\u539f\u59cb\u8cc7\u6599 (Zarr)\n",
    "    \u2193 (lazy operations)\n",
    "\u9078\u53d6 / \u5207\u7247 / \u805a\u5408\n",
    "    \u2193\n",
    "Rechunk\uff08\u5982\u679c\u9700\u8981\uff09\n",
    "    \u2193 (.to_zarr())\n",
    "\u512a\u5316\u7684\u4e2d\u9593 Zarr\n",
    "    \u2193 (\u7528\u65bc\u5f8c\u7e8c\u5206\u6790)\n",
    "ML / \u8996\u89ba\u5316 / \u7d71\u8a08\n",
    "```\n",
    "\n",
    "\u95dc\u9375\uff1a**\u628a\u8017\u6642\u7684\u524d\u8655\u7406\u505a\u4e00\u6b21\uff0c\u5132\u5b58\u4e0b\u4f86\uff0c\u91cd\u8907\u4f7f\u7528**\u3002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. \u5c0f\u7df4\u7fd2\uff08\u53ef\u9078\uff09\n",
    "\n",
    "1. **\u8a08\u7b97\u5b63\u7bc0\u6c23\u5019\u614b**\uff1a\u4f7f\u7528 `.groupby('time.season')` \u8a08\u7b97 DJF, MAM, JJA, SON \u7684 CAPE \u5e73\u5747\n",
    "2. **\u7570\u5e38\u503c\u5075\u6e2c**\uff1a\u627e\u51fa CAPE > 5000 J/kg \u7684\u6975\u7aef\u4e8b\u4ef6\uff0c\u7d71\u8a08\u767c\u751f\u983b\u7387\n",
    "3. **\u5be6\u9a57 chunking**\uff1a\u6bd4\u8f03\u4e0d\u540c chunk \u7b56\u7565\u5c0d `.mean(dim='time')` \u7684\u5f71\u97ff\n",
    "4. **\u58d3\u7e2e\u7387\u6e2c\u8a66**\uff1a\u7528\u4e0d\u540c\u7684 compressor \u5132\u5b58\u76f8\u540c\u8cc7\u6599\uff0c\u6bd4\u8f03\u78c1\u789f\u7a7a\u9593\u548c\u901f\u5ea6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u4e0b\u4e00\u6b65\n",
    "\n",
    "\u5728 **Notebook 3** \u4e2d\uff0c\u6211\u5011\u6703\u9032\u5165 ML Pipeline\uff1a\n",
    "- \u5b9a\u7fa9\u5c0d\u6d41\u5206\u985e\u4efb\u52d9\uff08\u4ec0\u9ebc\u6642\u5019\u7b97\u300c\u5c0d\u6d41\u300d\uff1f\uff09\n",
    "- \u4f7f\u7528 xbatcher \u7522\u751f\u8a13\u7df4 batches\n",
    "- \u6574\u5408 PyTorch DataLoader\n",
    "- \u8a13\u7df4\u4e00\u500b\u7c21\u55ae\u7684 CNN \u6a21\u578b\n",
    "- \u7528 xskillscore \u9032\u884c\u7a7a\u9593\u9a57\u8b49\n",
    "\n",
    "\u9019\u662f\u6574\u500b workshop \u7684\u9ad8\u6f6e\uff0c\u6703\u5c55\u793a\u5b8c\u6574\u7684\u300c\u5927\u578b\u9663\u5217 \u2192 ML\u300d\u5de5\u4f5c\u6d41\u3002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u5982\u679c\u8981\u95dc\u9589 Client\uff08\u91cb\u653e\u8cc7\u6e90\uff09\n",
    "# client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}