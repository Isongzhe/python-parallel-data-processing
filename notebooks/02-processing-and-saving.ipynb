{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: 時空處理與儲存優化 (30 min)\n",
    "\n",
    "這個 notebook 會介紹：\n",
    "\n",
    "1. **時間 Resampling**：hourly → daily，計算統計量\n",
    "2. **多檔案讀取**：使用 intake catalog 串接多年資料\n",
    "3. **空間統計**：區域平均、gradient 計算\n",
    "4. **Rechunking 策略**：根據分析模式調整 chunk\n",
    "5. **儲存為 Zarr**：優化 metadata 和壓縮設定\n",
    "\n",
    "---\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "- 掌握 `.resample()` 和 `.groupby()` 的時間聚合\n",
    "- 理解多檔案讀取的記憶體管理\n",
    "- 學會選擇合適的 chunking 策略\n",
    "- 正確儲存中間結果，避免重複計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "import xarray as xr\n",
    "import intake\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 啟動 Dask Client\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "# 載入 catalog\n",
    "catalog = intake.open_catalog('catalog.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 時間 Resampling：Hourly → Daily\n",
    "\n",
    "### 為什麼需要 Resampling？\n",
    "\n",
    "原始 ERA5 資料是 hourly，但許多分析需要：\n",
    "- **Daily mean**：日平均溫度、濕度\n",
    "- **Daily max**：日最大 CAPE（對流潛勢）\n",
    "- **Daily accumulation**：日累積降雨\n",
    "\n",
    "Resampling 的優勢：\n",
    "1. **減少資料量**：8760 hours → 365 days (減少 24 倍)\n",
    "2. **降低雜訊**：平滑短期波動\n",
    "3. **符合分析需求**：許多氣候指標是 daily 尺度\n",
    "\n",
    "### Xarray 的 `.resample()` 方法\n",
    "\n",
    "`.resample()` 類似 pandas，但是 **lazy** 的：\n",
    "```python\n",
    "ds.resample(time='1D').mean()  # 不會立即執行\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 2019 年資料\n",
    "ds = catalog.era5_2019.to_dask()\n",
    "\n",
    "print(\"原始資料：\")\n",
    "print(f\"  Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "print(f\"  Time steps: {len(ds.time)}\")\n",
    "print(f\"  Frequency: hourly\")\n",
    "print(f\"  Size: {ds.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 計算 Daily Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily mean: 每天 24 小時的平均\n",
    "# 'D' = calendar day, '1D' = 1 day\n",
    "ds_daily_mean = ds.resample(time='1D').mean()\n",
    "\n",
    "print(\"Daily mean:\")\n",
    "print(f\"  Time steps: {len(ds_daily_mean.time)}\")\n",
    "print(f\"  Size: {ds_daily_mean.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"  Reduction: {ds.nbytes / ds_daily_mean.nbytes:.1f}x\")\n",
    "print()\n",
    "print(ds_daily_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 不同變數使用不同統計量\n",
    "\n",
    "對於對流預報，我們可能需要：\n",
    "- **CAPE**: daily maximum（最大不穩定度）\n",
    "- **CIN**: daily mean（平均抑制）\n",
    "- **K-index**: daily mean\n",
    "- **BLH**: daily maximum（最高邊界層）\n",
    "\n",
    "Xarray 允許對不同變數做不同操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分別計算\n",
    "cape_daily_max = ds['cape'].resample(time='1D').max()\n",
    "cin_daily_mean = ds['cin'].resample(time='1D').mean()\n",
    "kindex_daily_mean = ds['k_index'].resample(time='1D').mean()\n",
    "blh_daily_max = ds['blh'].resample(time='1D').max()\n",
    "\n",
    "# 合併成新的 Dataset\n",
    "ds_daily = xr.Dataset({\n",
    "    'cape_max': cape_daily_max,\n",
    "    'cin_mean': cin_daily_mean,\n",
    "    'kindex_mean': kindex_daily_mean,\n",
    "    'blh_max': blh_daily_max\n",
    "})\n",
    "\n",
    "print(\"Custom daily statistics:\")\n",
    "print(ds_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 執行並視覺化結果\n",
    "\n",
    "我們選取台灣附近區域，計算並繪圖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選取台灣區域的 7 月資料\n",
    "ds_taiwan_july = ds_daily.sel(\n",
    "    time='2019-07',\n",
    "    latitude=slice(22, 26),\n",
    "    longitude=slice(120, 122)\n",
    ")\n",
    "\n",
    "# 計算月平均\n",
    "cape_monthly = ds_taiwan_july['cape_max'].mean(dim='time').compute()\n",
    "\n",
    "# 繪圖\n",
    "plt.figure(figsize=(8, 6))\n",
    "cape_monthly.plot(cmap='YlOrRd', vmin=0, vmax=2000)\n",
    "plt.title('Mean of Daily Max CAPE (July 2019, Taiwan)', fontsize=12)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 多檔案讀取：跨年度分析\n",
    "\n",
    "### 情境：計算 2019-2023 年的氣候態\n",
    "\n",
    "氣候態（climatology）= 多年平均的季節變化。例如：\n",
    "- 1 月的平均 CAPE\n",
    "- 7 月的平均 BLH\n",
    "\n",
    "這需要讀取所有年份的資料。\n",
    "\n",
    "### 方法 1：逐年讀取並串接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取所有年份\n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "datasets = []\n",
    "\n",
    "for year in years:\n",
    "    ds_year = catalog[f'era5_{year}'].to_dask()\n",
    "    datasets.append(ds_year)\n",
    "    print(f\"Loaded {year}: {len(ds_year.time)} time steps\")\n",
    "\n",
    "# 沿著時間軸串接\n",
    "ds_all = xr.concat(datasets, dim='time')\n",
    "\n",
    "print(f\"\\nCombined dataset:\")\n",
    "print(f\"  Time range: {ds_all.time.values[0]} to {ds_all.time.values[-1]}\")\n",
    "print(f\"  Total time steps: {len(ds_all.time)}\")\n",
    "print(f\"  Total size: {ds_all.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要：這裡沒有載入資料！\n",
    "\n",
    "雖然我們「串接」了 5 年資料（~135 GB），但實際上：\n",
    "- **沒有讀取任何 Zarr 檔案**\n",
    "- **沒有佔用 135 GB 記憶體**\n",
    "- 只是建立了一個**虛擬的 Dataset**，記錄了資料的位置\n",
    "\n",
    "這就是 lazy evaluation 的威力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 計算氣候態：Groupby 操作\n",
    "\n",
    "### 什麼是 Groupby？\n",
    "\n",
    "Groupby 是「split-apply-combine」模式：\n",
    "1. **Split**：按照某個規則分組（例如按月份）\n",
    "2. **Apply**：對每組做相同操作（例如計算平均）\n",
    "3. **Combine**：合併結果\n",
    "\n",
    "在氣候分析中常用的 groupby：\n",
    "- `.groupby('time.month')`：按月份分組（1-12月）\n",
    "- `.groupby('time.season')`：按季節分組（DJF, MAM, JJA, SON）\n",
    "- `.groupby('time.dayofyear')`：按一年中的第幾天（1-365）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個月份的氣候態（5 年平均）\n",
    "cape_climatology = ds_all['cape'].groupby('time.month').mean(dim='time')\n",
    "\n",
    "print(\"CAPE Climatology:\")\n",
    "print(cape_climatology)\n",
    "print()\n",
    "print(f\"Dimensions: {cape_climatology.dims}\")\n",
    "print(f\"Shape: {cape_climatology.shape}\")\n",
    "print(f\"Coordinates: {list(cape_climatology.coords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解結果\n",
    "\n",
    "原本的維度：`(time, latitude, longitude)`\n",
    "- time: 43800 (5 years * 8760 hours)\n",
    "\n",
    "Groupby 後的維度：`(month, latitude, longitude)`\n",
    "- month: 12\n",
    "\n",
    "每個月份的值 = 5 年中所有該月份資料的平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 計算並繪製季節變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選取台灣區域，計算氣候態\n",
    "cape_taiwan = ds_all['cape'].sel(\n",
    "    latitude=slice(22, 26),\n",
    "    longitude=slice(120, 122)\n",
    ")\n",
    "\n",
    "# 計算區域平均的月氣候態\n",
    "cape_monthly_clim = cape_taiwan.groupby('time.month').mean(dim=['time', 'latitude', 'longitude'])\n",
    "\n",
    "# 執行計算\n",
    "result = cape_monthly_clim.compute()\n",
    "\n",
    "# 繪圖\n",
    "plt.figure(figsize=(10, 5))\n",
    "result.plot(marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('CAPE (J/kg)', fontsize=12)\n",
    "plt.title('Taiwan Area-Averaged CAPE Climatology (2019-2023)', fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  Max CAPE month: {result.values.argmax() + 1}\")\n",
    "print(f\"  Min CAPE month: {result.values.argmin() + 1}\")\n",
    "print(f\"  Summer mean (JJA): {result.sel(month=[6,7,8]).mean().values:.0f} J/kg\")\n",
    "print(f\"  Winter mean (DJF): {result.sel(month=[12,1,2]).mean().values:.0f} J/kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 空間統計：計算 Gradient\n",
    "\n",
    "在氣象分析中，變數的空間梯度（gradient）很重要：\n",
    "- **溫度梯度**：鋒面位置\n",
    "- **CAPE 梯度**：對流邊界\n",
    "\n",
    "Xarray 提供 `.differentiate()` 來計算導數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選取一個時間點\n",
    "cape_snapshot = ds_all['cape'].isel(time=0)\n",
    "\n",
    "# 計算經向（latitude）梯度\n",
    "# 單位：CAPE per degree latitude\n",
    "cape_grad_lat = cape_snapshot.differentiate('latitude')\n",
    "\n",
    "# 計算緯向（longitude）梯度\n",
    "cape_grad_lon = cape_snapshot.differentiate('longitude')\n",
    "\n",
    "# 計算梯度強度（magnitude）\n",
    "cape_grad_mag = np.sqrt(cape_grad_lat**2 + cape_grad_lon**2)\n",
    "\n",
    "print(\"Gradient calculation (still lazy):\")\n",
    "print(f\"  Type: {type(cape_grad_mag.data)}\")\n",
    "print(f\"  Shape: {cape_grad_mag.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rechunking：最佳化 Chunk 策略\n",
    "\n",
    "### 為什麼需要 Rechunk？\n",
    "\n",
    "原始 Zarr 的 chunking 是 `(744, 121, 161)`（時間約 1 個月）。\n",
    "\n",
    "這適合：\n",
    "- ✅ 計算時間平均、時間序列分析\n",
    "- ✅ 空間場的操作\n",
    "\n",
    "但**不適合**：\n",
    "- ❌ 單點或小區域的長時間序列\n",
    "- ❌ ML training（需要時間維度切小塊）\n",
    "\n",
    "### Rechunking 的成本\n",
    "\n",
    "Rechunking **不是免費的**：\n",
    "- 需要讀取所有原始資料\n",
    "- 重新組織並寫入新的 chunks\n",
    "- I/O intensive（可能需要數小時）\n",
    "\n",
    "因此：\n",
    "1. **先確定分析模式**再 rechunk\n",
    "2. Rechunk 的結果應該**儲存下來**，不要每次都重新算\n",
    "3. 使用 intermediate Zarr 作為中間產物"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 情境：為 ML Training 最佳化\n",
    "\n",
    "對於後續的 ML pipeline，我們需要：\n",
    "- 小的時間 batch（例如 32 time steps）\n",
    "- 完整的空間 patch（例如 16x16）\n",
    "\n",
    "理想的 chunking：`(32, 16, 16)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 daily mean 資料（已經從 hourly 降到 daily）\n",
    "ds_daily_all = ds_all.resample(time='1D').mean()\n",
    "\n",
    "print(\"Before rechunk:\")\n",
    "print(f\"  Original chunks: {ds_daily_all['cape'].chunks}\")\n",
    "print(f\"  Total size: {ds_daily_all.nbytes / 1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Rechunk 為適合 ML 的結構\n",
    "ds_rechunked = ds_daily_all.chunk({\n",
    "    'time': 32,        # 每個 batch 32 天\n",
    "    'latitude': 16,    # 16x16 空間 patch\n",
    "    'longitude': 16\n",
    "})\n",
    "\n",
    "print(\"After rechunk:\")\n",
    "print(f\"  New chunks: {ds_rechunked['cape'].chunks}\")\n",
    "print(f\"  Number of chunks: {ds_rechunked['cape'].data.npartitions}\")\n",
    "print()\n",
    "\n",
    "# 計算單一 chunk 的大小\n",
    "chunk_size = 32 * 16 * 16 * 4  # float32 = 4 bytes\n",
    "print(f\"Single chunk size: {chunk_size / 1024:.2f} KB\")\n",
    "print(f\"  Status: {chunk_size / (1024**2):.2f} MB - 在理想範圍內 (10-100 MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Size 的權衡\n",
    "\n",
    "| Chunk Size | 優點 | 缺點 | 適用情境 |\n",
    "|------------|------|------|----------|\n",
    "| < 1 MB | 極細粒度平行化 | Overhead 過大 | 不推薦 |\n",
    "| 1-10 MB | 高度平行化 | 排程成本仍高 | 大量 workers |\n",
    "| **10-100 MB** | **平衡** | **- ** | **大多數情況** |\n",
    "| 100-500 MB | 低 overhead | 平行度受限 | 少量複雜計算 |\n",
    "| > 500 MB | 最低 overhead | 記憶體壓力、無法平行 | 不推薦 |\n",
    "\n",
    "我們的 32 KB 太小了！讓我們調整："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更合理的 chunking\n",
    "ds_rechunked = ds_daily_all.chunk({\n",
    "    'time': 32,        # 32 天\n",
    "    'latitude': 60,    # 60 個緯度點\n",
    "    'longitude': 80    # 80 個經度點\n",
    "})\n",
    "\n",
    "chunk_size = 32 * 60 * 80 * 4 * 4  # 4 variables, float32\n",
    "print(f\"Optimized chunk size: {chunk_size / (1024**2):.2f} MB\")\n",
    "print(f\"Number of chunks per variable: {ds_rechunked['cape'].data.npartitions}\")\n",
    "print(f\"Total chunks (4 variables): {ds_rechunked['cape'].data.npartitions * 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 儲存為 Zarr：固化中間結果\n",
    "\n",
    "### 為什麼要儲存中間結果？\n",
    "\n",
    "假設我們的 workflow 是：\n",
    "1. 讀取 5 年 hourly 資料\n",
    "2. Resample 到 daily\n",
    "3. Rechunk\n",
    "4. 用於 ML training\n",
    "\n",
    "如果每次 training 都重複 1-3 步驟：\n",
    "- 每次都要讀取 ~135 GB 原始資料\n",
    "- 重複計算 resample（耗費 CPU）\n",
    "- 重複 rechunk（大量 I/O）\n",
    "\n",
    "**更好的做法**：\n",
    "1. 算一次 daily + rechunked 資料\n",
    "2. 儲存為新的 Zarr（可能 ~6 GB）\n",
    "3. 後續直接讀取這個優化過的 Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 準備要儲存的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選取 2019 年，resample 到 daily mean\n",
    "ds_to_save = ds.resample(time='1D').mean()\n",
    "\n",
    "# Rechunk 到適合 ML 的結構\n",
    "ds_to_save = ds_to_save.chunk({\n",
    "    'time': 32,\n",
    "    'latitude': 60,\n",
    "    'longitude': 80\n",
    "})\n",
    "\n",
    "print(\"Dataset to save:\")\n",
    "print(ds_to_save)\n",
    "print()\n",
    "print(f\"Original size (hourly): {ds.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"New size (daily): {ds_to_save.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Reduction: {ds.nbytes / ds_to_save.nbytes:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 儲存為 Zarr\n",
    "\n",
    "Zarr 的儲存參數：\n",
    "\n",
    "1. **mode='w'**: 覆寫模式（小心！會刪除舊資料）\n",
    "2. **consolidated=True**: 將 metadata 集中儲存\n",
    "   - 優點：減少讀取時的 HTTP requests（雲端友善）\n",
    "   - 缺點：不支援 append（對我們沒影響）\n",
    "3. **encoding**: 設定壓縮、chunking\n",
    "   - compressor: 使用 Blosc 壓縮（CPU-friendly）\n",
    "   - chunks: 明確指定 chunk shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import Blosc\n",
    "\n",
    "# 輸出路徑\n",
    "output_path = 'outputs/era5_2019_daily_optimized.zarr'\n",
    "\n",
    "# 設定 encoding（壓縮與 chunking）\n",
    "compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.SHUFFLE)\n",
    "\n",
    "encoding = {\n",
    "    var: {\n",
    "        'compressor': compressor,\n",
    "        'chunks': (32, 60, 80)  # 明確指定 chunk shape\n",
    "    }\n",
    "    for var in ds_to_save.data_vars\n",
    "}\n",
    "\n",
    "print(\"Saving to Zarr...\")\n",
    "print(f\"  Output: {output_path}\")\n",
    "print(f\"  Compressor: Blosc (zstd, level 3)\")\n",
    "print(f\"  Chunks: (32, 60, 80)\")\n",
    "print()\n",
    "\n",
    "# 執行儲存\n",
    "# compute=True: 立即執行（而不是 lazy）\n",
    "# consolidated=True: 建立 .zmetadata\n",
    "ds_to_save.to_zarr(\n",
    "    output_path,\n",
    "    mode='w',\n",
    "    encoding=encoding,\n",
    "    consolidated=True\n",
    ")\n",
    "\n",
    "print(\"✓ Saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 儲存過程說明\n",
    "\n",
    "這個操作會：\n",
    "1. **讀取原始 Zarr**：從 `/home/sungche/NAS/dataset/era5/`\n",
    "2. **執行 resample**：hourly → daily mean\n",
    "3. **Rechunk**：重組為新的 chunk 結構\n",
    "4. **壓縮**：使用 Blosc 壓縮（約 30-50% 壓縮率）\n",
    "5. **寫入新 Zarr**：到 `outputs/` 目錄\n",
    "6. **建立 consolidated metadata**：加速後續讀取\n",
    "\n",
    "這個過程可能需要 **數分鐘到數十分鐘**，取決於磁碟速度和 worker 數量。\n",
    "\n",
    "**觀察建議**：切換到 Dask Dashboard 的 Task Stream，看到大量平行的 read → compute → write 任務。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 驗證儲存結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新讀取剛儲存的 Zarr\n",
    "ds_loaded = xr.open_zarr(output_path, consolidated=True)\n",
    "\n",
    "print(\"Loaded dataset:\")\n",
    "print(ds_loaded)\n",
    "print()\n",
    "print(f\"Chunks: {ds_loaded['cape'].chunks}\")\n",
    "print()\n",
    "\n",
    "# 驗證資料正確性：計算一個簡單統計量\n",
    "original_mean = ds_to_save['cape'].mean().compute()\n",
    "loaded_mean = ds_loaded['cape'].mean().compute()\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Original mean: {original_mean.values:.2f}\")\n",
    "print(f\"  Loaded mean: {loaded_mean.values:.2f}\")\n",
    "print(f\"  Difference: {abs(original_mean.values - loaded_mean.values):.6f}\")\n",
    "print(f\"  Status: {'✓ Match' if abs(original_mean.values - loaded_mean.values) < 0.01 else '✗ Mismatch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 檢查磁碟空間使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# 使用 du 指令查看目錄大小\n",
    "result = subprocess.run(\n",
    "    ['du', '-sh', output_path],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(f\"Disk usage: {result.stdout.strip()}\")\n",
    "print()\n",
    "print(\"Comparison:\")\n",
    "print(f\"  Original (hourly, uncompressed): ~27 GB\")\n",
    "print(f\"  Saved (daily, compressed): see above\")\n",
    "print(f\"  Expected: ~1.1 GB (24x smaller + compression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 壓縮選項比較\n",
    "\n",
    "不同的壓縮器有不同的權衡：\n",
    "\n",
    "| Compressor | 壓縮率 | 壓縮速度 | 解壓速度 | 適用情境 |\n",
    "|------------|--------|----------|----------|----------|\n",
    "| None | 1.0x | - | - | 快速原型、SSD |\n",
    "| Blosc (lz4) | 2-3x | 極快 | 極快 | 預設選擇 |\n",
    "| **Blosc (zstd)** | **3-5x** | **快** | **快** | **推薦** |\n",
    "| gzip | 4-6x | 慢 | 中 | 長期封存、網路傳輸 |\n",
    "| zlib | 3-5x | 慢 | 中 | 相容性優先 |\n",
    "\n",
    "**我們使用 Blosc (zstd, level=3)**：\n",
    "- 平衡壓縮率與速度\n",
    "- CPU overhead 低（重要！因為我們還要做計算）\n",
    "- 解壓快速（讀取時不會成為瓶頸）\n",
    "\n",
    "如果需要更高壓縮率（例如資料要傳輸到其他地方），可以用：\n",
    "```python\n",
    "compressor = Blosc(cname='zstd', clevel=9, shuffle=Blosc.BITSHUFFLE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 檢查點：你應該理解的概念\n",
    "\n",
    "完成這個 notebook 後，你應該能夠：\n",
    "\n",
    "- [ ] 使用 `.resample()` 進行時間聚合\n",
    "- [ ] 使用 `.groupby()` 計算氣候態\n",
    "- [ ] 用 `xr.concat()` 串接多個 Dataset\n",
    "- [ ] 理解 rechunking 的時機和成本\n",
    "- [ ] 根據分析模式選擇合適的 chunk size\n",
    "- [ ] 正確設定壓縮參數並儲存 Zarr\n",
    "- [ ] 使用 consolidated metadata 加速讀取\n",
    "\n",
    "### Workflow 總結\n",
    "\n",
    "處理大型 N-D array 的標準流程：\n",
    "\n",
    "```\n",
    "原始資料 (Zarr)\n",
    "    ↓ (lazy operations)\n",
    "選取 / 切片 / 聚合\n",
    "    ↓\n",
    "Rechunk（如果需要）\n",
    "    ↓ (.to_zarr())\n",
    "優化的中間 Zarr\n",
    "    ↓ (用於後續分析)\n",
    "ML / 視覺化 / 統計\n",
    "```\n",
    "\n",
    "關鍵：**把耗時的前處理做一次，儲存下來，重複使用**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 小練習（可選）\n",
    "\n",
    "1. **計算季節氣候態**：使用 `.groupby('time.season')` 計算 DJF, MAM, JJA, SON 的 CAPE 平均\n",
    "2. **異常值偵測**：找出 CAPE > 5000 J/kg 的極端事件，統計發生頻率\n",
    "3. **實驗 chunking**：比較不同 chunk 策略對 `.mean(dim='time')` 的影響\n",
    "4. **壓縮率測試**：用不同的 compressor 儲存相同資料，比較磁碟空間和速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下一步\n",
    "\n",
    "在 **Notebook 3** 中，我們會進入 ML Pipeline：\n",
    "- 定義對流分類任務（什麼時候算「對流」？）\n",
    "- 使用 xbatcher 產生訓練 batches\n",
    "- 整合 PyTorch DataLoader\n",
    "- 訓練一個簡單的 CNN 模型\n",
    "- 用 xskillscore 進行空間驗證\n",
    "\n",
    "這是整個 workshop 的高潮，會展示完整的「大型陣列 → ML」工作流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要關閉 Client（釋放資源）\n",
    "# client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
