# Dask Array 實戰課程規劃

> ⚠️ **重要更新 (2025-10-29)**：
>
> Module 3.3 的 PyTorch 整合部分已修正。原先錯誤地建議手寫 `Dataset` wrapper，
> 現已更正為使用 xbatcher 官方提供的 `xbatcher.loaders.torch.MapDataset`。
>
> 詳見完整講義的 3.3 章節。

## 課程資訊

**課程名稱**: Dask Array 實戰：從氣象資料到 ML Pipeline
**目標對象**: 已完成「Part 1: Python 平行處理基礎」的學員
**時長**: 3 小時（實作導向）
**環境**: 共用 Server + GPU（多人共用，模型要小）
**日期**: 待定

---

## 核心需求確認

### ✅ 已確認事項

1. **ML 任務選擇**: 對流分類
   - Input: CAPE, CIN, K-index, BLH
   - Output: 是否發生對流（二元分類）
   - 標籤定義: total_precipitation > 5mm/hr
   - 理由: 尺度較大，降雨預測太難；對流分類更適合教學

2. **Zarr 版本**: < 3.0
   - 原因: zarr 3.0 有重大改版（V3 spec）
   - 問題:
     - 改變 metadata 格式
     - 改變 chunk 命名規則
     - xarray/dask 目前主要支援 zarr 2.x
   - 需在課程中詳細解釋

3. **環境設定**:
   - 共用 Server
   - 有 GPU（多人共用，模型要輕量）
   - 需教學: uv → ipynb 完整流程（學員可能忘記）

4. **課程重點**:
   - 實作為主，講解為輔
   - 重點在「資料處理 pipeline」，不是「模型訓練」
   - 模型部分只教「如何接到 PyTorch」，不深入訓練技巧

---

## 資料確認

### ERA5 Zarr 資料
- **路徑**: `/home/sungche/NAS/dataset/era5/`
- **檔案**:
  - `era5_2019_10N40N_100E140E.zarr`
  - `era5_2020_10N40N_100E140E.zarr`
  - `era5_2021_10N40N_100E140E.zarr`
  - `era5_2022_10N40N_100E140E.zarr`
  - `era5_2023_10N40N_100E140E.zarr`

- **空間範圍**: 10N-40N, 100E-140E（東亞-西太平洋）
- **時間範圍**: 2019-2023（5年）

- **變數**:
  - **動力場**: `u_component_of_wind`, `v_component_of_wind`, `geopotential`
  - **熱力場**: `temperature`, `specific_humidity`, `boundary_layer_height`
  - **對流參數**: `convective_available_potential_energy`, `convective_inhibition`, `k_index`
  - **降水**: `total_precipitation`
  - **其他**: `surface_pressure`, `sea_surface_temperature`, `instantaneous_10m_wind_gust`

---

## 課程大綱（3 小時）

### 前置準備（10 min）
**目標**: 快速建立環境並啟動工具

**內容**:
1. uv 專案初始化
2. VSCode 連接 remote Jupyter kernel
3. 啟動 Dask Dashboard

**講解重點**:
- 複習 uv 基本指令（學員可能忘記）
- Dashboard URL 位置
- 如何在 VSCode 看 Dashboard

---

### Module 1: Zarr 與 Xarray 基礎（40 min）

#### 1.1 Zarr 儲存格式（10 min 講解）

**為何需要 Zarr？**

| 特性 | NetCDF/HDF5 | Zarr |
|------|-------------|------|
| 檔案結構 | 單一檔案 | 目錄（每個 chunk 是獨立檔案） |
| 平行讀寫 | 差（metadata lock） | 優（無 lock） |
| 雲端友善 | 差 | 優（S3, GCS） |
| 壓縮 | 有限 | 靈活（多種壓縮器） |

**Zarr 核心優勢**:
- 目錄結構，每個 chunk 獨立
- 支援真正的平行讀寫
- 雲端原生設計

**Zarr 版本注意事項** ⚠️:
```bash
# 為何要用 zarr < 3.0？

# zarr 3.0 重寫了核心架構（V3 spec）
# - 改變了 metadata 格式（.zarray → zarr.json）
# - 改變了 chunk 命名規則
# - 改變了 storage API

# 目前生態系（2024-2025）:
# - xarray: 主要支援 zarr 2.x
# - dask: 主要支援 zarr 2.x
# - zarr-python 3.x 仍在穩定中

# 建議：生產環境用 zarr < 3.0
```

#### 1.2 實作：讀取與探索資料（30 min）

**程式碼**:
```python
import xarray as xr
import dask
from dask.distributed import Client

# 啟動 Dask Client（開啟 Dashboard）
client = Client()
print(client.dashboard_link)

# 1. 讀取單年 zarr
ds = xr.open_zarr('/home/sungche/NAS/dataset/era5/era5_2019_10N40N_100E140E.zarr')

# 2. 查看結構
print(ds)
print(ds.chunks)  # 觀察 chunk 大小

# 3. Lazy evaluation 示範
temp = ds['temperature'].sel(time='2019-06', level=850)
print(type(temp.data))  # dask.array.Array

# 這個操作瞬間完成！因為是 lazy
print("已建立 lazy operation")

# 4. 觸發計算（觀察 Dashboard）
temp_mean = temp.mean(dim='time').compute()
print(f"平均溫度: {temp_mean.values}")

# 5. 繪圖（小計算）
import matplotlib.pyplot as plt
temp.isel(time=0).plot()
plt.title('Temperature at 850hPa (2019-06-01)')
plt.show()
```

**實作重點**:
- 讓學員親手操作 `.sel()`, `.isel()`, `.mean()`
- 強調「瞬間完成」= lazy
- 觀察 Dashboard 的 task graph
- 理解 `.compute()` 才會真正執行

**Dashboard 觀察點**:
- Task Stream: 看到任務執行
- Progress: 完成進度
- Memory: 記憶體使用

---

### Module 2: 時空資料處理（50 min）

#### 2.1 實作：多檔案讀取與時間處理（20 min）

**程式碼**:
```python
import xarray as xr
import glob

# 1. 讀取多年資料
zarr_files = glob.glob('/home/sungche/NAS/dataset/era5/era5_20[12][0-9]*.zarr')
zarr_files.sort()

ds_multi = xr.open_mfdataset(
    zarr_files,
    engine='zarr',
    parallel=True,
    chunks={'time': 24, 'latitude': 50, 'longitude': 50}
)

print(f"Total time range: {ds_multi.time[0].values} to {ds_multi.time[-1].values}")

# 2. 時間切片：夏季（6-9月）
ds_summer = ds_multi.sel(time=ds_multi.time.dt.month.isin([6, 7, 8, 9]))
print(f"Summer data points: {len(ds_summer.time)}")

# 3. 空間切片：台灣區域
ds_tw = ds_summer.sel(
    latitude=slice(25, 22),  # 注意：latitude 是遞減的
    longitude=slice(120, 122)
)

# 4. 時間重採樣：hourly → daily
ds_daily = ds_tw.resample(time='1D').mean()
print(ds_daily)
```

**實作重點**:
- `open_mfdataset` 的 `parallel=True` 參數
- `chunks` 參數的影響（可以實驗不同大小）
- 時間切片的多種方法
- `resample` 的 lazy 特性

#### 2.2 實作：計算與存檔（20 min）

**程式碼**:
```python
import zarr

# 1. 計算氣候平均（climatology）
climatology = ds_tw.groupby('time.month').mean(dim='time')
print("Climatology computed (lazy)")

# 2. 計算距平（anomaly）
anomaly = ds_tw.groupby('time.month') - climatology

# 3. 存成 zarr（重點！）
output_path = './tw_summer_anomaly.zarr'

# 設定壓縮（可選）
encoding = {
    var: {'compressor': zarr.Blosc(cname='zstd', clevel=3, shuffle=2)}
    for var in anomaly.data_vars
}

# 存檔
anomaly.to_zarr(
    output_path,
    mode='w',
    consolidated=True,
    encoding=encoding
)

print(f"Saved to {output_path}")

# 4. 重新讀取驗證速度
ds_reload = xr.open_zarr(output_path)
print("Reload completed (instantaneous!)")
print(ds_reload)
```

**實作重點**:
- `groupby` 的強大功能
- `to_zarr` 參數詳解：
  - `mode='w'`: 覆寫模式
  - `consolidated=True`: 合併 metadata（加速讀取）
  - `encoding`: 壓縮設定
- 重新讀取的速度對比

#### 2.3 Dashboard 觀察（10 min）

**實驗**:
```python
# 實驗：不同 chunk 大小的影響

# 小 chunk（更多任務，更靈活）
ds_small = xr.open_zarr(
    '/home/sungche/NAS/dataset/era5/era5_2019_10N40N_100E140E.zarr',
    chunks={'time': 10, 'latitude': 20, 'longitude': 20}
)

# 大 chunk（更少任務，可能更快）
ds_large = xr.open_zarr(
    '/home/sungche/NAS/dataset/era5/era5_2019_10N40N_100E140E.zarr',
    chunks={'time': 100, 'latitude': 100, 'longitude': 100}
)

# 比較計算時間
import time

start = time.time()
result_small = ds_small['temperature'].mean().compute()
time_small = time.time() - start

start = time.time()
result_large = ds_large['temperature'].mean().compute()
time_large = time.time() - start

print(f"Small chunks: {time_small:.2f}s")
print(f"Large chunks: {time_large:.2f}s")
```

**觀察重點**:
- Task graph 的差異
- 記憶體使用情況
- 哪個更快？為什麼？

---

### Module 3: ML Pipeline 實戰（90 min）

#### 3.1 任務定義與資料準備（15 min）

**科學問題**: 用對流參數預測是否發生對流

**程式碼**:
```python
# 1. 讀取訓練資料（2019-2020）
ds_train = xr.open_mfdataset(
    ['/home/sungche/NAS/dataset/era5/era5_2019*.zarr',
     '/home/sungche/NAS/dataset/era5/era5_2020*.zarr'],
    engine='zarr',
    parallel=True
)

# 2. 選取特徵變數
feature_vars = [
    'convective_available_potential_energy',
    'convective_inhibition',
    'k_index',
    'boundary_layer_height'
]

features = ds_train[feature_vars]

# 3. 建立標籤（降雨 > 5mm/hr 定義為對流）
label = (ds_train['total_precipitation'] > 5).astype(int)
label.name = 'convection_flag'

# 4. 合併成訓練資料集
train_ds = xr.merge([features, label])

# 5. 驗證資料（2021）
ds_valid = xr.open_zarr('/home/sungche/NAS/dataset/era5/era5_2021*.zarr')
valid_ds = xr.merge([
    ds_valid[feature_vars],
    (ds_valid['total_precipitation'] > 5).astype(int).rename('convection_flag')
])

print("Training data:", train_ds)
print("Validation data:", valid_ds)
```

**講解重點**:
- 如何從科學問題定義特徵/標籤
- 為何選這些對流參數？
- 標籤的定義方式（可調整閾值）

---

#### 3.2 xbatcher：從 Xarray 到批次（25 min）⭐ 核心重點

**核心問題**: 如何把 TB 級 Xarray 餵給模型？

**答案**: xbatcher 切成小塊，lazy 讀取

**程式碼**:
```python
import xbatcher

# 1. 建立 BatchGenerator
bgen = xbatcher.BatchGenerator(
    train_ds,
    input_dims={'latitude': 16, 'longitude': 16},  # 空間 patch size
    input_overlap={'latitude': 4, 'longitude': 4},  # overlap 避免邊界效應
    batch_dims={'time': 32}  # 時間批次大小
)

print(f"Total batches: {len(bgen)}")

# 2. 看一個 batch 長什麼樣子
for i, batch in enumerate(bgen):
    print(f"\n--- Batch {i} ---")
    print(batch)
    print(f"CAPE shape: {batch['convective_available_potential_energy'].shape}")
    print(f"Label shape: {batch['convection_flag'].shape}")

    if i == 0:  # 只看第一個 batch
        break

# 3. 轉成 NumPy（這時才會 compute）
X_numpy = batch[feature_vars].to_array(dim='variable').values
y_numpy = batch['convection_flag'].values

print(f"\nX shape: {X_numpy.shape}")  # (variables, time, lat, lon)
print(f"y shape: {y_numpy.shape}")    # (time, lat, lon)
print(f"X dtype: {X_numpy.dtype}")
```

**實作重點**:
- `input_dims`: 空間窗口大小
- `input_overlap`: 為何需要重疊？（避免邊界效應）
- `batch_dims`: 時間批次
- `to_array()`: 把多個變數合併成一個新維度
- 觀察 Dashboard：這時才真正讀取資料

**進階實驗**:
```python
# 實驗不同的 patch size
for patch_size in [8, 16, 32]:
    bgen_test = xbatcher.BatchGenerator(
        train_ds.isel(time=slice(0, 100)),  # 只用一小部分測試
        input_dims={'latitude': patch_size, 'longitude': patch_size},
        batch_dims={'time': 32}
    )
    print(f"Patch size {patch_size}x{patch_size}: {len(bgen_test)} batches")
```

---

#### 3.3 PyTorch DataLoader 整合（15 min）

**目標**: 建立橋接層，讓 PyTorch 能讀 xbatcher

**程式碼**:
```python
from torch.utils.data import Dataset, DataLoader
import torch
import numpy as np

class XarrayDataset(Dataset):
    """
    把 xbatcher 包裝成 PyTorch Dataset
    這是「橋接層」，寫一次就能重複使用
    """

    def __init__(self, ds, feature_vars, label_var, batch_config):
        self.ds = ds
        self.feature_vars = feature_vars
        self.label_var = label_var

        # 建立 BatchGenerator
        self.bgen = xbatcher.BatchGenerator(ds, **batch_config)

        # 預先生成所有 batch（只是索引，不是資料）
        self.batches = list(self.bgen)

    def __len__(self):
        return len(self.batches)

    def __getitem__(self, idx):
        # 取得一個 batch（這時才 compute）
        batch = self.batches[idx]

        # 轉成 NumPy
        X = batch[self.feature_vars].to_array(dim='variable').values
        y = batch[self.label_var].values

        # 處理 NaN（如果有的話）
        X = np.nan_to_num(X, nan=0.0)
        y = np.nan_to_num(y, nan=0)

        # 轉成 Torch Tensor
        return torch.FloatTensor(X), torch.LongTensor(y)

# 建立訓練 Dataset
train_dataset = XarrayDataset(
    train_ds,
    feature_vars=feature_vars,
    label_var='convection_flag',
    batch_config={
        'input_dims': {'latitude': 16, 'longitude': 16},
        'batch_dims': {'time': 32}
    }
)

# 建立 DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=4,      # 一次讀 4 個 xarray batch
    shuffle=True,
    num_workers=2,     # 平行讀取
    pin_memory=True    # 加速 GPU 傳輸
)

# 測試一下
for X, y in train_loader:
    print(f"X: {X.shape}, y: {y.shape}")
    print(f"X device: {X.device}, dtype: {X.dtype}")
    break
```

**講解重點**:
- 這就是「橋接層」的概念
- `num_workers`: 可以並行讀取（觀察 Dashboard）
- `pin_memory`: GPU 相關優化
- 寫一次就能用在所有專案

---

#### 3.4 模型訓練（簡化示範）（10 min）

**重點**: 只示範「流程通了」，不講訓練細節

**程式碼**:
```python
import torch.nn as nn
import torch.optim as optim

# 用一個超級簡單的模型（不是重點）
class SimpleConvNet(nn.Module):
    def __init__(self, in_channels=4, num_classes=2):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # 全局平均池化
        )
        self.classifier = nn.Linear(32, num_classes)

    def forward(self, x):
        # x: (batch, variables, time, lat, lon)
        # 簡化：對時間維度取平均
        x = x.mean(dim=2)  # → (batch, variables, lat, lon)

        x = self.features(x)  # → (batch, 32, 1, 1)
        x = x.view(x.size(0), -1)  # → (batch, 32)
        x = self.classifier(x)  # → (batch, 2)
        return x

# 建立模型
model = SimpleConvNet(in_channels=len(feature_vars), num_classes=2)
model = model.cuda()

# 定義損失函數和優化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 訓練一個 batch（只是示範！）
model.train()
for X, y in train_loader:
    X = X.cuda()
    y = y.cuda()

    # 簡化標籤（取空間平均，轉成單一值）
    y_flat = (y.float().mean(dim=[1, 2, 3]) > 0.5).long()

    # 前向傳播
    outputs = model(X)
    loss = criterion(outputs, y_flat)

    # 反向傳播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Loss: {loss.item():.4f}")
    break  # 只跑一個 batch！

print("\n✅ Pipeline 通了！資料能順利餵給模型")
```

**講解重點**:
- 「這只是證明 pipeline 通了」
- 模型架構不重要（甚至可以用 `torchvision.models.resnet18`）
- 真正的訓練調參是另一堂課的內容
- 重點：**資料能從 Zarr → Xarray → xbatcher → PyTorch**

---

#### 3.5 預測結果 → Xarray（15 min）⭐ 核心重點

**核心問題**: 模型輸出是 NumPy/Tensor，如何轉回帶座標的 Xarray？

**程式碼**:
```python
# 建立驗證 DataLoader（不 shuffle）
valid_dataset = XarrayDataset(
    valid_ds,
    feature_vars=feature_vars,
    label_var='convection_flag',
    batch_config={
        'input_dims': {'latitude': 16, 'longitude': 16},
        'batch_dims': {'time': 32}
    }
)

valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)

# 預測
model.eval()
predictions = []
batch_infos = []

with torch.no_grad():
    for idx, (X, y) in enumerate(valid_loader):
        X = X.cuda()

        # 預測
        outputs = model(X)
        pred = outputs.argmax(dim=1).cpu().numpy()  # (batch,)

        # 重點！取得對應的座標
        batch = valid_dataset.batches[idx]

        # 建立 DataArray（保留座標）
        pred_da = xr.DataArray(
            pred,
            coords={
                'batch_id': [idx]
            },
            dims=['batch_id']
        )

        predictions.append(pred_da)

        if idx >= 10:  # 只預測前 10 個 batch（示範）
            break

# 合併所有預測
pred_full = xr.concat(predictions, dim='batch_id')

print("Predictions with coordinates:")
print(pred_full)
```

**進階版（保留完整空間座標）**:
```python
# 如果需要保留空間資訊，可以這樣做：

model.eval()
predictions_spatial = []

# 修改模型，輸出空間預測
class SpatialConvNet(nn.Module):
    def __init__(self, in_channels=4, num_classes=2):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, num_classes, 1)  # 1x1 conv，保留空間維度
        )

    def forward(self, x):
        x = x.mean(dim=2)  # 時間平均
        return self.conv(x)  # (batch, num_classes, lat, lon)

# ... 預測過程類似，但保留空間維度
```

**講解重點**:
- 這是 Xarray 的核心價值：保留座標資訊
- 氣象/地科資料必須知道「哪裡」的預測
- 與傳統 NumPy/Pandas 的差異

---

#### 3.6 xskillscore 驗證（15 min）

**核心優勢**: 保留空間資訊的驗證

**程式碼**:
```python
import xskillscore as xs

# 假設我們有完整的預測和觀測（簡化示範）
# 實際使用時，需要把所有 batch 的預測拼回完整的空間場

# 1. 準備觀測值（簡化：用部分資料）
obs = valid_ds['convection_flag'].isel(time=slice(0, 100))

# 2. 假設預測也是同樣形狀（實際上需要從模型重建）
# 這裡用一個 dummy prediction 示範
pred = obs + np.random.randn(*obs.shape) * 0.1
pred = pred.clip(0, 1)

# 3. 計算空間分佈的 RMSE
rmse_spatial = xs.rmse(pred, obs, dim='time')
print("Spatial RMSE:")
print(rmse_spatial)

# 4. 計算時間序列的相關係數
corr_temporal = xs.pearson_r(pred, obs, dim=['latitude', 'longitude'])
print("\nTemporal correlation:")
print(corr_temporal)

# 5. 計算整體指標
mae = xs.mae(pred, obs)
mse = xs.mse(pred, obs)
print(f"\nOverall MAE: {mae.values:.4f}")
print(f"Overall MSE: {mse.values:.4f}")

# 6. 視覺化
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 觀測
obs.isel(time=0).plot(ax=axes[0, 0], cmap='RdBu_r', vmin=0, vmax=1)
axes[0, 0].set_title('Observation (t=0)')

# 預測
pred.isel(time=0).plot(ax=axes[0, 1], cmap='RdBu_r', vmin=0, vmax=1)
axes[0, 1].set_title('Prediction (t=0)')

# 空間 RMSE
rmse_spatial.plot(ax=axes[1, 0], cmap='viridis')
axes[1, 0].set_title('RMSE (spatial, over time)')

# 時間相關係數
corr_temporal.plot(ax=axes[1, 1], cmap='RdBu_r', vmin=-1, vmax=1)
axes[1, 1].set_title('Correlation (temporal, over space)')

plt.tight_layout()
plt.savefig('validation_results.png', dpi=150)
plt.show()
```

**其他 xskillscore 功能**:
```python
# 分類問題的指標
from xskillscore import Contingency

# 轉成二元
pred_binary = (pred > 0.5).astype(int)
obs_binary = obs.astype(int)

# 計算混淆矩陣相關指標
contingency = Contingency(obs_binary, pred_binary,
                          observation_category_edges=[0, 0.5, 1],
                          forecast_category_edges=[0, 0.5, 1])

# POD, FAR, CSI 等
# (需要根據 xskillscore 版本調整 API)
```

**講解重點**:
- 為何需要 xskillscore？
  - 傳統 sklearn：只能算整體指標
  - xskillscore：可以算「每個格點」、「每個時間」的指標
- 氣象/地科需要知道「哪裡預測得好/差」
- 可以保留所有 metadata（lat, lon, time）

---

## 時間分配總結

| 模組 | 內容 | 時間 | 類型 |
|------|------|------|------|
| **前置準備** | uv + ipynb + Dashboard | 10 min | 講解+實作 |
| **Module 1.1** | Zarr 原理與版本 | 10 min | 講解 |
| **Module 1.2** | Xarray 實作 | 30 min | 實作為主 |
| **Module 2.1** | 多檔案與時間處理 | 20 min | 實作為主 |
| **Module 2.2** | 計算與儲存 | 20 min | 實作為主 |
| **Module 2.3** | Dashboard 分析 | 10 min | 觀察討論 |
| **Module 3.1** | 任務定義 | 15 min | 講解+實作 |
| **Module 3.2** | xbatcher ⭐ | 25 min | 實作為主 |
| **Module 3.3** | PyTorch 橋接 | 15 min | 實作為主 |
| **Module 3.4** | 訓練示範（簡化） | 10 min | 示範 |
| **Module 3.5** | 結果 → Xarray ⭐ | 15 min | 實作為主 |
| **Module 3.6** | xskillscore 驗證 | 15 min | 實作為主 |
| **Buffer** | Q&A + 自由實作 | 10 min | 彈性 |
| **總計** | | **185 min ≈ 3 小時** | |

---

## 課程重點提醒

### ✅ 要強調的
1. **Zarr < 3.0** 的相容性問題（詳細解釋）
2. **xbatcher** 的批次切割（核心）
3. **Xarray ↔ PyTorch** 的橋接（核心）
4. **預測結果轉回 Xarray**（保留座標）
5. **xskillscore** 的空間驗證能力
6. **Dashboard** 的即時觀察

### ❌ 不深入講解的
1. 模型架構設計
2. 訓練技巧（learning rate, optimizer 選擇）
3. 過擬合、正則化
4. 模型調參
5. 分散式訓練

---

## 課程 Markdown 文件結構

```
# Dask Array 實戰：從氣象資料到 ML Pipeline

## 課程目標
（學習目標、先備知識）

## 0. 環境設定（10 min）
### 0.1 uv 專案初始化
### 0.2 VSCode Jupyter kernel
### 0.3 Dask Dashboard

## 1. Zarr 與 Xarray 基礎（40 min）
### 1.1 為什麼需要 Zarr？
### 1.2 Zarr vs NetCDF 對照表
### 1.3 Zarr < 3.0 的相容性問題 ⚠️
### 1.4 實作：讀取與探索
### 1.5 Dashboard 觀察

## 2. 時空資料處理（50 min）
### 2.1 實作：多檔案讀取
### 2.2 實作：時空切片與重採樣
### 2.3 實作：計算與儲存
### 2.4 Dashboard 效能分析

## 3. ML Pipeline 實戰（90 min）
### 3.1 任務定義：對流分類
### 3.2 xbatcher 批次切割 ⭐
### 3.3 PyTorch DataLoader 橋接
### 3.4 訓練流程示範（簡化）
### 3.5 預測結果 → Xarray ⭐
### 3.6 xskillscore 空間驗證

## 附錄
### A. 常見錯誤排查
### B. 套件版本建議
### C. 參考資料
```

---

## 待辦事項

- [ ] 撰寫完整 Markdown 文件（可貼 Notion）
- [ ] 開發 Jupyter Notebook（完整可執行程式碼）
- [ ] 準備環境設定文件（uv → ipynb 流程）
- [ ] 準備範例資料（如果需要小樣本）
- [ ] 測試所有程式碼在 Server 上可執行
- [ ] 準備 Dashboard 截圖
- [ ] 準備 Zarr vs NetCDF 對照圖

---

## 套件清單（pyproject.toml）

```toml
[project]
name = "python-parallel-data-processing"
version = "0.1.0"
description = "Dask Array workshop for atmospheric data analysis"
requires-python = ">=3.11"
dependencies = [
    "xarray>=2024.10.0",
    "zarr>=2.18.0,<3.0.0",  # 重要！< 3.0
    "dask[complete]>=2024.10.0",
    "numpy>=2.1.0",
    "matplotlib>=3.9.0",
    "xbatcher>=0.3.0",
    "xskillscore>=0.0.26",
    "torch>=2.0.0",
    "torchvision>=0.15.0",
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.0",
    "jupyterlab>=4.0.0",
]
```

---

## 注意事項

1. **資料路徑**: 確認學員對 NAS 路徑有讀取權限
2. **GPU 競爭**: 提醒學員共用 GPU，模型要輕量
3. **記憶體**: 監控記憶體使用，避免 OOM
4. **時間控制**: Module 3 可能會超時，需要嚴格控制
5. **備用計畫**: 如果時間不夠，可以跳過 Module 3.4（訓練示範）

---

**文件版本**: v1.0
**最後更新**: 2025-10-29
**狀態**: 規劃完成，待撰寫詳細內容
